{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65ac24aa",
   "metadata": {
    "papermill": {
     "duration": 0.009486,
     "end_time": "2022-08-30T05:13:25.242461",
     "exception": false,
     "start_time": "2022-08-30T05:13:25.232975",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# change log:\n",
    "| version | comment| cv | lb |\n",
    "|---------|--------|----|----|\n",
    "|version 11|swin transformer + 512 img + w/o stain norm train data + pixel adjust[00026250.model.pth],stain in infer| 0.767 | `0.70` |\n",
    "|version 10| My written swin tf, gave lb-0.61 w/o stain norm train, w/ stain norm infer [00026250.model.pth] | ~0.73 | `0.70` |\n",
    "|version 12/13| swin transformer + 512 img + w/o stain norm train + pixel adjust+ focal loss [00008811/00011880],stain in infer | ~0.772/0.772 | `0.63` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "930e8bb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T05:13:25.260257Z",
     "iopub.status.busy": "2022-08-30T05:13:25.259826Z",
     "iopub.status.idle": "2022-08-30T05:14:25.893756Z",
     "shell.execute_reply": "2022-08-30T05:14:25.892577Z"
    },
    "papermill": {
     "duration": 60.646176,
     "end_time": "2022-08-30T05:14:25.896861",
     "exception": false,
     "start_time": "2022-08-30T05:13:25.250685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/staintools-offline/spams-2.6.5.4-cp37-cp37m-linux_x86_64.whl\r\n",
      "Requirement already satisfied: Pillow>=6.0 in /opt/conda/lib/python3.7/site-packages (from spams==2.6.5.4) (9.1.1)\r\n",
      "Requirement already satisfied: numpy>=1.12 in /opt/conda/lib/python3.7/site-packages (from spams==2.6.5.4) (1.21.6)\r\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.7/site-packages (from spams==2.6.5.4) (1.7.3)\r\n",
      "Installing collected packages: spams\r\n",
      "Successfully installed spams-2.6.5.4\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/staintools-offline/staintools-2.1.2-py3-none-any.whl\r\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from staintools==2.1.2) (3.5.2)\r\n",
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (from staintools==2.1.2) (4.5.4.60)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from staintools==2.1.2) (1.21.6)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->staintools==2.1.2) (2.8.2)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->staintools==2.1.2) (1.4.3)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->staintools==2.1.2) (4.33.3)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->staintools==2.1.2) (0.11.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->staintools==2.1.2) (21.3)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->staintools==2.1.2) (9.1.1)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->staintools==2.1.2) (3.0.9)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->staintools==2.1.2) (4.1.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib->staintools==2.1.2) (1.16.0)\r\n",
      "Installing collected packages: staintools\r\n",
      "Successfully installed staintools-2.1.2\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/staintools-offline/spams-2.6.5.4-cp37-cp37m-linux_x86_64.whl\n",
    "!pip install /kaggle/input/staintools-offline/staintools-2.1.2-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba843f81",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-08-30T05:14:25.916460Z",
     "iopub.status.busy": "2022-08-30T05:14:25.916122Z",
     "iopub.status.idle": "2022-08-30T05:14:29.483608Z",
     "shell.execute_reply": "2022-08-30T05:14:29.482603Z"
    },
    "papermill": {
     "duration": 3.579763,
     "end_time": "2022-08-30T05:14:29.485937",
     "exception": false,
     "start_time": "2022-08-30T05:14:25.906174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.cuda.amp as amp\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import RandomSampler \n",
    "from torch.utils.data import SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "import albumentations as A\n",
    "import gc\n",
    "\n",
    "# from torchmetrics.functional import dice_score\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import tifffile\n",
    "is_amp = True\n",
    "import logging\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import numpy as np\n",
    "from itertools import repeat\n",
    "import collections.abc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84e1aeeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T05:14:29.506280Z",
     "iopub.status.busy": "2022-08-30T05:14:29.505823Z",
     "iopub.status.idle": "2022-08-30T05:14:29.518314Z",
     "shell.execute_reply": "2022-08-30T05:14:29.517482Z"
    },
    "papermill": {
     "duration": 0.025154,
     "end_time": "2022-08-30T05:14:29.520453",
     "exception": false,
     "start_time": "2022-08-30T05:14:29.495299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_dir = '/kaggle/working/'\n",
    "pretrain_dir = '/kaggle/input/swin-tiny-small-22k-pretrained/'\n",
    "\n",
    "TRAIN = '../input/hubmap-2022-256x256/train/'\n",
    "MASKS = '../input/hubmap-2022-256x256/masks/'\n",
    "LABELS = '../input/hubmap-organ-segmentation/train.csv'\n",
    "\n",
    "df = pd.read_csv(\"../input/hubmap-organ-segmentation/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579542c2",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-08-30T05:14:29.540548Z",
     "iopub.status.busy": "2022-08-30T05:14:29.540280Z",
     "iopub.status.idle": "2022-08-30T05:14:29.560887Z",
     "shell.execute_reply": "2022-08-30T05:14:29.560110Z"
    },
    "papermill": {
     "duration": 0.032629,
     "end_time": "2022-08-30T05:14:29.563086",
     "exception": false,
     "start_time": "2022-08-30T05:14:29.530457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def image_to_tensor(image, mode='bgr'): #image mode\n",
    "    if mode=='bgr':\n",
    "        image = image[:,:,::-1]\n",
    "    x = image\n",
    "    x = x.transpose(2,0,1)\n",
    "    x = np.ascontiguousarray(x)\n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    return x\n",
    "\n",
    "\n",
    "def mask_to_tensor(mask):\n",
    "    x = mask\n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    return x\n",
    "\n",
    "\n",
    "tensor_list = ['mask', 'image', 'organ']\n",
    "\n",
    "def null_collate(batch):\n",
    "    d = {}\n",
    "    key = batch[0].keys()\n",
    "    for k in key:\n",
    "        v = [b[k] for b in batch]\n",
    "        if k in tensor_list:\n",
    "            v = torch.stack(v)\n",
    "        d[k] = v\n",
    "\n",
    "    d['mask'] = d['mask'].unsqueeze(1)\n",
    "    d['organ'] = d['organ'].reshape(-1)\n",
    "    return d\n",
    "\n",
    "\n",
    "def _ntuple(n):\n",
    "    def parse(x):\n",
    "        if isinstance(x, collections.abc.Iterable):\n",
    "            return x\n",
    "        return tuple(repeat(x, n))\n",
    "    return parse\n",
    "\n",
    "\n",
    "to_2tuple = _ntuple(2)\n",
    "\n",
    "\n",
    "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
    "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
    "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
    "    def norm_cdf(x):\n",
    "        # Computes standard normal cumulative distribution function\n",
    "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
    "\n",
    "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
    "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
    "                      \"The distribution of values may be incorrect.\",\n",
    "                      stacklevel=2)\n",
    "\n",
    "        \n",
    "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
    "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
    "\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n",
    "    if keep_prob > 0.0 and scale_by_keep:\n",
    "        random_tensor.div_(keep_prob)\n",
    "    return x * random_tensor\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob: float = 0., scale_by_keep: bool = True):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.scale_by_keep = scale_by_keep\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f'drop_prob={round(self.drop_prob,3):0.3f}'\n",
    "    \n",
    "    \n",
    "class RGB(nn.Module):\n",
    "    IMAGE_RGB_MEAN = [0.485, 0.456, 0.406] #[0.5, 0.5, 0.5]\n",
    "    IMAGE_RGB_STD  = [0.229, 0.224, 0.225] #[0.5, 0.5, 0.5]\n",
    "\n",
    "    def __init__(self,):\n",
    "        super(RGB, self).__init__()\n",
    "        self.register_buffer('mean', torch.zeros(1,3,1,1))\n",
    "        self.register_buffer('std', torch.ones(1,3,1,1))\n",
    "        self.mean.data = torch.FloatTensor(self.IMAGE_RGB_MEAN).view(self.mean.shape)\n",
    "        self.std.data = torch.FloatTensor(self.IMAGE_RGB_STD).view(self.std.shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x-self.mean)/self.std\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def message(mode='print'):\n",
    "    asterisk = ' '\n",
    "    if mode==('print'):\n",
    "        loss = batch_loss\n",
    "    if mode==('log'):\n",
    "        loss = train_loss\n",
    "        if (iteration % iter_save == 0): asterisk = '*'\n",
    "\n",
    "    text = \\\n",
    "        ('%0.2e   %08d%s %6.2f | '%(rate, iteration, asterisk, epoch,)).replace('e-0','e-').replace('e+0','e+') + \\\n",
    "        '%4.3f  %4.3f  %4.4f  %4.3f   | '%(*valid_loss,) + \\\n",
    "        '%4.3f  %4.3f   | '%(*loss,) + \\\n",
    "        '%s' % ((time.time() - start_timer))\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dd2d1f6",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-08-30T05:14:29.581600Z",
     "iopub.status.busy": "2022-08-30T05:14:29.581327Z",
     "iopub.status.idle": "2022-08-30T05:14:29.593707Z",
     "shell.execute_reply": "2022-08-30T05:14:29.592778Z"
    },
    "papermill": {
     "duration": 0.023584,
     "end_time": "2022-08-30T05:14:29.595550",
     "exception": false,
     "start_time": "2022-08-30T05:14:29.571966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rle_encode(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = img.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def image_to_tensor(image, mode='bgr'): #image mode\n",
    "    if mode=='bgr':\n",
    "        image = image[:,:,::-1]\n",
    "    x = image\n",
    "    x = x.transpose(2,0,1)\n",
    "    x = np.ascontiguousarray(x)\n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    return x\n",
    "\n",
    "\n",
    "def mask_to_tensor(mask):\n",
    "    x = mask\n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    return x\n",
    "\n",
    "    \n",
    "    \n",
    "class RGB(nn.Module):\n",
    "    IMAGE_RGB_MEAN = [0.485, 0.456, 0.406] #[0.5, 0.5, 0.5]\n",
    "    IMAGE_RGB_STD  = [0.229, 0.224, 0.225] #[0.5, 0.5, 0.5]\n",
    "\n",
    "    def __init__(self,):\n",
    "        super(RGB, self).__init__()\n",
    "        self.register_buffer('mean', torch.zeros(1,3,1,1))\n",
    "        self.register_buffer('std', torch.ones(1,3,1,1))\n",
    "        self.mean.data = torch.FloatTensor(self.IMAGE_RGB_MEAN).view(self.mean.shape)\n",
    "        self.std.data = torch.FloatTensor(self.IMAGE_RGB_STD).view(self.std.shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x-self.mean)/self.std\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def message(mode='print'):\n",
    "    asterisk = ' '\n",
    "    if mode==('print'):\n",
    "        loss = batch_loss\n",
    "    if mode==('log'):\n",
    "        loss = train_loss\n",
    "        if (iteration % iter_save == 0): asterisk = '*'\n",
    "\n",
    "    text = \\\n",
    "        ('%0.2e   %08d%s %6.2f | '%(rate, iteration, asterisk, epoch,)).replace('e-0','e-').replace('e+0','e+') + \\\n",
    "        '%4.3f  %4.3f  %4.4f  %4.3f   | '%(*valid_loss,) + \\\n",
    "        '%4.3f  %4.3f   | '%(*loss,) + \\\n",
    "        '%s' % ((time.time() - start_timer))\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "216ba92f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T05:14:29.614422Z",
     "iopub.status.busy": "2022-08-30T05:14:29.613618Z",
     "iopub.status.idle": "2022-08-30T05:14:29.621192Z",
     "shell.execute_reply": "2022-08-30T05:14:29.620434Z"
    },
    "papermill": {
     "duration": 0.019138,
     "end_time": "2022-08-30T05:14:29.623297",
     "exception": false,
     "start_time": "2022-08-30T05:14:29.604159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def valid_augment5(image, mask, organ):\n",
    "    #image, mask  = do_crop(image, mask, image_size, xy=(None,None))\n",
    "    return image, mask\n",
    "\n",
    "def train_augment5b(image, mask, organ):\n",
    "    image, mask = do_random_flip(image, mask)\n",
    "    image, mask = do_random_rot90(image, mask)\n",
    "\n",
    "    for fn in np.random.choice([\n",
    "        lambda image, mask: (image, mask),\n",
    "        lambda image, mask: do_random_noise(image, mask, mag=0.1),\n",
    "        lambda image, mask: do_random_contast(image, mask, mag=0.40),\n",
    "        lambda image, mask: do_random_hsv(image, mask, mag=[0.40, 0.40, 0])\n",
    "    ], 2): image, mask = fn(image, mask)\n",
    "\n",
    "    for fn in np.random.choice([\n",
    "        lambda image, mask: (image, mask),\n",
    "        lambda image, mask: do_random_rotate_scale(image, mask, angle=45, scale=[0.50, 2.0]),\n",
    "    ], 1): image, mask = fn(image, mask)\n",
    "\n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3028d4d1",
   "metadata": {
    "papermill": {
     "duration": 0.0085,
     "end_time": "2022-08-30T05:14:29.640178",
     "exception": false,
     "start_time": "2022-08-30T05:14:29.631678",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:#c3448b; background:#efe9e9; border:1px dashed #efe50b;\" role=\"tab\" aria-controls=\"augmentations\"><center>Augmentations</center></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cb1533a",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-08-30T05:14:29.659807Z",
     "iopub.status.busy": "2022-08-30T05:14:29.659174Z",
     "iopub.status.idle": "2022-08-30T05:14:29.676380Z",
     "shell.execute_reply": "2022-08-30T05:14:29.675587Z"
    },
    "papermill": {
     "duration": 0.029091,
     "end_time": "2022-08-30T05:14:29.678278",
     "exception": false,
     "start_time": "2022-08-30T05:14:29.649187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def do_random_flip(image, mask):\n",
    "    if np.random.rand()>0.5:\n",
    "        image = cv2.flip(image,0)\n",
    "        mask = cv2.flip(mask,0)\n",
    "    if np.random.rand()>0.5:\n",
    "        image = cv2.flip(image,1)\n",
    "        mask = cv2.flip(mask,1)\n",
    "    if np.random.rand()>0.5:\n",
    "        image = image.transpose(1,0,2)\n",
    "        mask = mask.transpose(1,0)\n",
    "    \n",
    "    image = np.ascontiguousarray(image)\n",
    "    mask = np.ascontiguousarray(mask)\n",
    "    return image, mask\n",
    "\n",
    "def do_random_rot90(image, mask):\n",
    "    r = np.random.choice([\n",
    "        0,\n",
    "        cv2.ROTATE_90_CLOCKWISE,\n",
    "        cv2.ROTATE_90_COUNTERCLOCKWISE,\n",
    "        cv2.ROTATE_180,\n",
    "    ])\n",
    "    if r==0:\n",
    "        return image, mask\n",
    "    else:\n",
    "        image = cv2.rotate(image, r)\n",
    "        mask = cv2.rotate(mask, r)\n",
    "        return image, mask\n",
    "    \n",
    "def do_random_contast(image, mask, mag=0.3):\n",
    "    alpha = 1 + random.uniform(-1,1)*mag\n",
    "    image = image * alpha\n",
    "    image = np.clip(image,0,1)\n",
    "    return image, mask\n",
    "\n",
    "def do_random_hsv(image, mask, mag=[0.15,0.25,0.25]):\n",
    "    image = (image*255).astype(np.uint8)\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    h = hsv[:, :, 0].astype(np.float32)  # hue\n",
    "    s = hsv[:, :, 1].astype(np.float32)  # saturation\n",
    "    v = hsv[:, :, 2].astype(np.float32)  # value\n",
    "    h = (h*(1 + random.uniform(-1,1)*mag[0]))%180\n",
    "    s =  s*(1 + random.uniform(-1,1)*mag[1])\n",
    "    v =  v*(1 + random.uniform(-1,1)*mag[2])\n",
    "\n",
    "    hsv[:, :, 0] = np.clip(h,0,180).astype(np.uint8)\n",
    "    hsv[:, :, 1] = np.clip(s,0,255).astype(np.uint8)\n",
    "    hsv[:, :, 2] = np.clip(v,0,255).astype(np.uint8)\n",
    "    image = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "    image = image.astype(np.float32)/255\n",
    "    return image, mask\n",
    "\n",
    "def do_random_noise(image, mask, mag=0.1):\n",
    "    height, width = image.shape[:2]\n",
    "    noise = np.random.uniform(-1,1, (height, width,1))*mag\n",
    "    image = image + noise\n",
    "    image = np.clip(image,0,1)\n",
    "    return image, mask\n",
    "\n",
    "def do_random_rotate_scale(image, mask, angle=30, scale=[0.8,1.2] ):\n",
    "    angle = np.random.uniform(-angle, angle)\n",
    "    scale = np.random.uniform(*scale) if scale is not None else 1\n",
    "    \n",
    "    height, width = image.shape[:2]\n",
    "    center = (height // 2, width // 2)\n",
    "    \n",
    "    transform = cv2.getRotationMatrix2D(center, angle, scale)\n",
    "    image = cv2.warpAffine( image, transform, (width, height), flags=cv2.INTER_LINEAR,\n",
    "                            borderMode=cv2.BORDER_CONSTANT, borderValue=(0,0,0))\n",
    "    mask  = cv2.warpAffine( mask, transform, (width, height), flags=cv2.INTER_LINEAR,\n",
    "                            borderMode=cv2.BORDER_CONSTANT, borderValue=0)\n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43964703",
   "metadata": {
    "papermill": {
     "duration": 0.008448,
     "end_time": "2022-08-30T05:14:29.695310",
     "exception": false,
     "start_time": "2022-08-30T05:14:29.686862",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:#c3448b; background:#efe9e9; border:1px dashed #efe50b;\" role=\"tab\" aria-controls=\"dataset\"><center>Dataset</center></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa2df640",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-08-30T05:14:29.714307Z",
     "iopub.status.busy": "2022-08-30T05:14:29.713624Z",
     "iopub.status.idle": "2022-08-30T05:14:29.726112Z",
     "shell.execute_reply": "2022-08-30T05:14:29.725307Z"
    },
    "papermill": {
     "duration": 0.023958,
     "end_time": "2022-08-30T05:14:29.728071",
     "exception": false,
     "start_time": "2022-08-30T05:14:29.704113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_size = 512\n",
    "\n",
    "class HubmapDataset(Dataset):\n",
    "    def __init__(self, df, augment=None):\n",
    "\n",
    "        self.df = df\n",
    "        self.augment = augment\n",
    "        self.length = len(self.df)\n",
    "        self.organ_to_label = {'kidney' : 0,\n",
    "                               'prostate' : 1,\n",
    "                               'largeintestine' : 2,\n",
    "                               'spleen' : 3,\n",
    "                               'lung' : 4}\n",
    "\n",
    "    def __str__(self):\n",
    "        string = ''\n",
    "        string += '\\tlen = %d\\n' % len(self)\n",
    "\n",
    "        d = self.df.organ.value_counts().to_dict()\n",
    "        for k in ['kidney', 'prostate', 'largeintestine', 'spleen', 'lung']:\n",
    "            string +=  '%24s %3d (%0.3f) \\n'%(k, d.get(k,0), d.get(k,0)/len(self.df))\n",
    "        return string\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        d = self.df.iloc[index]\n",
    "        img_height = self.df.loc[index, 'img_height']\n",
    "        img_width = self.df.loc[index, 'img_width']\n",
    "        organ = self.organ_to_label[d.organ]\n",
    "\n",
    "        image = cv2.cvtColor(tifffile.imread(os.path.join(TRAIN, f'{d.id}.tiff')), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        rle_mask = self.df.loc[index, 'rle']\n",
    "        mask = rle_decode(rle_mask, (img_height, img_width))\n",
    "        #mask = cv2.cvtColor(mask, cv2.IMREAD_GRAYSCALE)\n",
    "        #mask = cv2.imread(os.path.join(MASKS,fname),cv2.IMREAD_GRAYSCALE)\n",
    "        mask = np.expand_dims(mask, axis = 2)\n",
    "        #print(mask.shape)\n",
    "        \n",
    "        image = image.astype(np.float32)/255\n",
    "        #mask  = mask.astype(np.float32)/255\n",
    "        mask = mask.astype(np.float32)\n",
    "\n",
    "        s = d.pixel_size/0.4 * (image_size/3000)\n",
    "        image = cv2.resize(image,dsize=(image_size,image_size),interpolation=cv2.INTER_LINEAR)\n",
    "        mask  = cv2.resize(mask, dsize=(image_size,image_size),interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        if self.augment is not None:\n",
    "            image, mask = self.augment(image, mask, organ)\n",
    "\n",
    "\n",
    "        r ={}\n",
    "        r['index']= index\n",
    "        r['organ'] = torch.tensor([organ], dtype=torch.long)\n",
    "        r['image'] = image_to_tensor(image)\n",
    "        r['mask' ] = mask_to_tensor(mask)\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5efc5119",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-08-30T05:14:29.747232Z",
     "iopub.status.busy": "2022-08-30T05:14:29.746540Z",
     "iopub.status.idle": "2022-08-30T05:14:29.757179Z",
     "shell.execute_reply": "2022-08-30T05:14:29.756370Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.022507,
     "end_time": "2022-08-30T05:14:29.759166",
     "exception": false,
     "start_time": "2022-08-30T05:14:29.736659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class FPN(nn.Module):\n",
    "#     def __init__(self, input_channels:list, output_channels:list):\n",
    "#         super().__init__()\n",
    "#         self.convs = nn.ModuleList(\n",
    "#             [nn.Sequential(nn.Conv2d(in_ch, out_ch*2, kernel_size=3, padding=1),\n",
    "#              nn.ReLU(inplace=True), nn.BatchNorm2d(out_ch*2),\n",
    "#              nn.Conv2d(out_ch*2, out_ch, kernel_size=3, padding=1))\n",
    "#             for in_ch, out_ch in zip(input_channels, output_channels)])\n",
    "        \n",
    "#     def forward(self, xs:list, last_layer):\n",
    "#         hcs = [F.interpolate(c(x),scale_factor=2**(len(self.convs)-i),mode='bilinear') \n",
    "#                for i,(c,x) in enumerate(zip(self.convs, xs))]\n",
    "#         hcs.append(last_layer)\n",
    "#         return torch.cat(hcs, dim=1)\n",
    "\n",
    "# class UnetBlock(nn.Module):\n",
    "#     def __init__(self, up_in_c:int, x_in_c:int, nf:int=None, blur:bool=False,\n",
    "#                  self_attention:bool=False, **kwargs):\n",
    "#         super().__init__()\n",
    "#         self.shuf = PixelShuffle_ICNR(up_in_c, up_in_c//2, blur=blur, **kwargs)\n",
    "#         self.bn = nn.BatchNorm2d(x_in_c)\n",
    "#         ni = up_in_c//2 + x_in_c\n",
    "#         nf = nf if nf is not None else max(up_in_c//2,32)\n",
    "#         self.conv1 = ConvLayer(ni, nf, norm_type=None, **kwargs)\n",
    "#         self.conv2 = ConvLayer(nf, nf, norm_type=None,\n",
    "#             xtra=SelfAttention(nf) if self_attention else None, **kwargs)\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "#     def forward(self, up_in:Tensor, left_in:Tensor) -> Tensor:\n",
    "#         s = left_in\n",
    "#         up_out = self.shuf(up_in)\n",
    "#         cat_x = self.relu(torch.cat([up_out, self.bn(s)], dim=1))\n",
    "#         return self.conv2(self.conv1(cat_x))\n",
    "        \n",
    "# class _ASPPModule(nn.Module):\n",
    "#     def __init__(self, inplanes, planes, kernel_size, padding, dilation, groups=1):\n",
    "#         super().__init__()\n",
    "#         self.atrous_conv = nn.Conv2d(inplanes, planes, kernel_size=kernel_size,\n",
    "#                 stride=1, padding=padding, dilation=dilation, bias=False, groups=groups)\n",
    "#         self.bn = nn.BatchNorm2d(planes)\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#         self._init_weight()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.atrous_conv(x)\n",
    "#         x = self.bn(x)\n",
    "\n",
    "#         return self.relu(x)\n",
    "\n",
    "#     def _init_weight(self):\n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m, nn.Conv2d):\n",
    "#                 torch.nn.init.kaiming_normal_(m.weight)\n",
    "#             elif isinstance(m, nn.BatchNorm2d):\n",
    "#                 m.weight.data.fill_(1)\n",
    "#                 m.bias.data.zero_()\n",
    "\n",
    "# class ASPP(nn.Module):\n",
    "#     def __init__(self, inplanes=512, mid_c=256, dilations=[6, 12, 18, 24], out_c=None):\n",
    "#         super().__init__()\n",
    "#         self.aspps = [_ASPPModule(inplanes, mid_c, 1, padding=0, dilation=1)] + \\\n",
    "#             [_ASPPModule(inplanes, mid_c, 3, padding=d, dilation=d,groups=4) for d in dilations]\n",
    "#         self.aspps = nn.ModuleList(self.aspps)\n",
    "#         self.global_pool = nn.Sequential(nn.AdaptiveMaxPool2d((1, 1)),\n",
    "#                         nn.Conv2d(inplanes, mid_c, 1, stride=1, bias=False),\n",
    "#                         nn.BatchNorm2d(mid_c), nn.ReLU())\n",
    "#         out_c = out_c if out_c is not None else mid_c\n",
    "#         self.out_conv = nn.Sequential(nn.Conv2d(mid_c*(2+len(dilations)), out_c, 1, bias=False),\n",
    "#                                     nn.BatchNorm2d(out_c), nn.ReLU(inplace=True))\n",
    "#         self.conv1 = nn.Conv2d(mid_c*(2+len(dilations)), out_c, 1, bias=False)\n",
    "#         self._init_weight()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x0 = self.global_pool(x)\n",
    "#         xs = [aspp(x) for aspp in self.aspps]\n",
    "#         x0 = F.interpolate(x0, size=xs[0].size()[2:], mode='bilinear', align_corners=True)\n",
    "#         x = torch.cat([x0] + xs, dim=1)\n",
    "#         return self.out_conv(x)\n",
    "    \n",
    "#     def _init_weight(self):\n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m, nn.Conv2d):\n",
    "#                 torch.nn.init.kaiming_normal_(m.weight)\n",
    "#             elif isinstance(m, nn.BatchNorm2d):\n",
    "#                 m.weight.data.fill_(1)\n",
    "#                 m.bias.data.zero_()\n",
    "\n",
    "\n",
    "# class config:\n",
    "#     pretrained_root = '../input/efficientnet-pytorch/'\n",
    "#     efficient_net_encoders = {\n",
    "#         \"efficientnet-b0\": {\n",
    "#             \"out_channels\": (3, 32, 24, 40, 112, 320),\n",
    "#             \"stage_idxs\": (3, 5, 9, 16),\n",
    "#             \"weight_path\": pretrained_root + \"efficientnet-b0-08094119.pth\"\n",
    "#         },\n",
    "#         \"efficientnet-b1\": {\n",
    "#             \"out_channels\": (3, 32, 24, 40, 112, 320),\n",
    "#             \"stage_idxs\": (5, 8, 16, 23),\n",
    "#             \"weight_path\": pretrained_root + \"efficientnet-b1-dbc7070a.pth\"\n",
    "#         },\n",
    "#         \"efficientnet-b2\": {\n",
    "#             \"out_channels\": (3, 32, 24, 48, 120, 352),\n",
    "#             \"stage_idxs\": (5, 8, 16, 23),\n",
    "#             \"weight_path\": pretrained_root + \"efficientnet-b2-27687264.pth\"\n",
    "#         },\n",
    "#         \"efficientnet-b3\": {\n",
    "#             \"out_channels\": (3, 40, 32, 48, 136, 384),\n",
    "#             \"stage_idxs\": (5, 8, 18, 26),\n",
    "#             \"weight_path\": pretrained_root + \"efficientnet-b3-c8376fa2.pth\"\n",
    "#         },\n",
    "#         \"efficientnet-b4\": {\n",
    "#             \"out_channels\": (3, 48, 32, 56, 160, 448),\n",
    "#             \"stage_idxs\": (6, 10, 22, 32),\n",
    "#             \"weight_path\": pretrained_root + \"efficientnet-b4-e116e8b3.pth\"\n",
    "#         },\n",
    "#         \"efficientnet-b5\": {\n",
    "#             \"out_channels\": (3, 48, 40, 64, 176, 512),\n",
    "#             \"stage_idxs\": (8, 13, 27, 39),\n",
    "#             \"weight_path\": pretrained_root + \"efficientnet-b5-586e6cc6.pth\"\n",
    "#         },\n",
    "#         \"efficientnet-b6\": {\n",
    "#             \"out_channels\": (3, 56, 40, 72, 200, 576),\n",
    "#             \"stage_idxs\": (9, 15, 31, 45),\n",
    "#             \"weight_path\": pretrained_root + \"efficientnet-b6-c76e70fd.pth\"\n",
    "#         },\n",
    "#         \"efficientnet-b7\": {\n",
    "#             \"out_channels\": (3, 64, 48, 80, 224, 640),\n",
    "#             \"stage_idxs\": (11, 18, 38, 55),\n",
    "#             \"weight_path\": pretrained_root + \"efficientnet-b7-dcc49843.pth\"\n",
    "#         }\n",
    "#     }\n",
    "#     model = 'efficientnet-b7'\n",
    "    \n",
    "# class EfficientNetEncoder(EfficientNet):\n",
    "#     def __init__(self, stage_idxs, out_channels, model_name, depth=5):\n",
    "\n",
    "#         blocks_args, global_params = get_model_params(model_name, override_params=None)\n",
    "#         super().__init__(blocks_args, global_params)\n",
    "        \n",
    "#         cfg = config.efficient_net_encoders[model_name]\n",
    "\n",
    "#         self._stage_idxs = stage_idxs\n",
    "#         self._out_channels = out_channels\n",
    "#         self._depth = depth\n",
    "#         self._in_channels = 3\n",
    "\n",
    "#         del self._fc\n",
    "#         self.load_state_dict(torch.load(cfg['weight_path']))\n",
    "\n",
    "#     def get_stages(self):\n",
    "#         return [\n",
    "#             nn.Identity(),\n",
    "#             nn.Sequential(self._conv_stem, self._bn0, self._swish),\n",
    "#             self._blocks[:self._stage_idxs[0]],\n",
    "#             self._blocks[self._stage_idxs[0]:self._stage_idxs[1]],\n",
    "#             self._blocks[self._stage_idxs[1]:self._stage_idxs[2]],\n",
    "#             self._blocks[self._stage_idxs[2]:],\n",
    "#         ]\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         stages = self.get_stages()\n",
    "\n",
    "#         block_number = 0.\n",
    "#         drop_connect_rate = self._global_params.drop_connect_rate\n",
    "\n",
    "#         features = []\n",
    "#         for i in range(self._depth + 1):\n",
    "\n",
    "#             # Identity and Sequential stages\n",
    "#             if i < 2:\n",
    "#                 x = stages[i](x)\n",
    "\n",
    "#             # Block stages need drop_connect rate\n",
    "#             else:\n",
    "#                 for module in stages[i]:\n",
    "#                     drop_connect = drop_connect_rate * block_number / len(self._blocks)\n",
    "#                     block_number += 1.\n",
    "#                     x = module(x, drop_connect)\n",
    "\n",
    "#             features.append(x)\n",
    "\n",
    "#         return features\n",
    "\n",
    "#     def load_state_dict(self, state_dict, **kwargs):\n",
    "#         state_dict.pop(\"_fc.bias\")\n",
    "#         state_dict.pop(\"_fc.weight\")\n",
    "#         super().load_state_dict(state_dict, **kwargs)  \n",
    "        \n",
    "\n",
    "# class EffUnet(nn.Module):\n",
    "#     def __init__(self, model_name, stride=1):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         cfg = config.efficient_net_encoders[model_name]\n",
    "#         stage_idxs = cfg['stage_idxs']\n",
    "#         out_channels = cfg['out_channels']\n",
    "        \n",
    "#         self.encoder = EfficientNetEncoder(stage_idxs, out_channels, model_name)\n",
    "\n",
    "#         #aspp with customized dilatations\n",
    "#         self.aspp = ASPP(out_channels[-1], 256, out_c=384, \n",
    "#                          dilations=[stride*1, stride*2, stride*3, stride*4])\n",
    "#         self.drop_aspp = nn.Dropout2d(0.5)\n",
    "#         #decoder\n",
    "#         self.dec4 = UnetBlock(384, out_channels[-2], 256)\n",
    "#         self.dec3 = UnetBlock(256, out_channels[-3], 128)\n",
    "#         self.dec2 = UnetBlock(128, out_channels[-4], 64)\n",
    "#         self.dec1 = UnetBlock(64, out_channels[-5], 32)\n",
    "#         self.fpn = FPN([384, 256, 128, 64], [16]*4)\n",
    "#         self.drop = nn.Dropout2d(0.1)\n",
    "#         self.final_conv = ConvLayer(32+16*4, 1, ks=1, norm_type=None, act_cls=None)\n",
    "        \n",
    "#         self.rgb = RGB()\n",
    "        \n",
    "#     def forward(self, batch):\n",
    "#         x = batch['image']\n",
    "#         B, C, H, W = x.shape\n",
    "#         x = self.rgb(x)\n",
    "#         enc0, enc1, enc2, enc3, enc4 = self.encoder(x)[-5:]\n",
    "#         enc5 = self.aspp(enc4)\n",
    "#         dec3 = self.dec4(self.drop_aspp(enc5), enc3)\n",
    "#         dec2 = self.dec3(dec3,enc2)\n",
    "#         dec1 = self.dec2(dec2,enc1)\n",
    "#         dec0 = self.dec1(dec1,enc0)\n",
    "#         x = self.fpn([enc5, dec3, dec2, dec1], dec0)\n",
    "#         x = self.final_conv(self.drop(x))\n",
    "#         x = F.interpolate(x, size = 512, mode = 'bilinear')\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5349882c",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-08-30T05:14:29.778237Z",
     "iopub.status.busy": "2022-08-30T05:14:29.777959Z",
     "iopub.status.idle": "2022-08-30T05:14:29.868628Z",
     "shell.execute_reply": "2022-08-30T05:14:29.867805Z"
    },
    "papermill": {
     "duration": 0.102911,
     "end_time": "2022-08-30T05:14:29.870682",
     "exception": false,
     "start_time": "2022-08-30T05:14:29.767771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    r\"\"\" Image to Patch Embedding\n",
    "\n",
    "    Args:\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 patch_size=4,\n",
    "                 in_chans=3,\n",
    "                 embed_dim=96,\n",
    "                 norm_layer=None\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        # padding\n",
    "        if W % self.patch_size[1] != 0:\n",
    "            x = F.pad(x, (0, self.patch_size[1] - W % self.patch_size[1]))\n",
    "        if H % self.patch_size[0] != 0:\n",
    "            x = F.pad(x, (0, 0, 0, self.patch_size[0] - H % self.patch_size[0]))\n",
    "\n",
    "\n",
    "        x = self.proj(x)  # B C Wh Ww\n",
    "        if self.norm is not None:\n",
    "            Wh, Ww = x.size(2), x.size(3)\n",
    "            x = x.flatten(2).transpose(1, 2)\n",
    "            x = self.norm(x)\n",
    "            x = x.transpose(1, 2).view(-1, self.embed_dim, Wh, Ww)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class PatchMerging(nn.Module):\n",
    "    r\"\"\" Patch Merging Layer.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input feature, tensor size (B, H*W, C).\n",
    "            H, W: Spatial resolution of the input feature.\n",
    "        \"\"\"\n",
    "\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "        # padding\n",
    "        pad_input = (H % 2 == 1) or (W % 2 == 1)\n",
    "        if pad_input:\n",
    "            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B, H/2, W/2, 4*C\n",
    "        x = x.view(B, -1, 4 * C)  # B, H/2*W/2, 4*C\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class BasicLayer(nn.Module):\n",
    "    \"\"\" A basic Swin Transformer layer for one stage.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Local window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "        fused_window_process (bool, optional): If True, use one kernel to fused window shift & window partition for acceleration, similar for the reversed part. Default: False\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        dim,\n",
    "        depth,\n",
    "        num_heads,\n",
    "        window_size,\n",
    "        mlp_ratio=4.,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop=0.,\n",
    "        attn_drop=0.,\n",
    "        drop_path=0.,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        downsample=None,\n",
    "        #use_checkpoint=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = window_size // 2\n",
    "        self.depth = depth\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SwinTransformerBlock(\n",
    "                dim=dim,\n",
    "                num_heads=num_heads,\n",
    "                window_size=window_size,\n",
    "                shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale,\n",
    "                drop=drop,\n",
    "                attn_drop=attn_drop,\n",
    "                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                norm_layer=norm_layer,\n",
    "            )\n",
    "            for i in range(depth)\n",
    "        ])\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input feature, tensor size (B, H*W, C).\n",
    "            H, W: Spatial resolution of the input feature.\n",
    "        \"\"\"\n",
    "\n",
    "        # calculate attention mask for SW-MSA ----\n",
    "        Hp = int(np.ceil(H / self.window_size)) * self.window_size\n",
    "        Wp = int(np.ceil(W / self.window_size)) * self.window_size\n",
    "        img_mask = torch.zeros((1, Hp, Wp, 1), device=x.device)  # 1 Hp Wp 1\n",
    "        h_slices = (slice(0, -self.window_size),\n",
    "                    slice(-self.window_size, -self.shift_size),\n",
    "                    slice(-self.shift_size, None))\n",
    "        w_slices = (slice(0, -self.window_size),\n",
    "                    slice(-self.window_size, -self.shift_size),\n",
    "                    slice(-self.shift_size, None))\n",
    "        cnt = 0\n",
    "        for h in h_slices:\n",
    "            for w in w_slices:\n",
    "                img_mask[:, h, w, :] = cnt\n",
    "                cnt += 1\n",
    "\n",
    "        mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "        #------\n",
    "\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, H, W, attn_mask)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            x_down = self.downsample(x, H, W)\n",
    "            Wh, Ww = (H + 1) // 2, (W + 1) // 2\n",
    "            return x, H, W, x_down, Wh, Ww\n",
    "        else:\n",
    "            return x, H, W, x, H, W\n",
    "        \n",
    "        \n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    r\"\"\" Swin Transformer Block.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resulotion.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        shift_size (int): Shift size for SW-MSA.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "        fused_window_process (bool, optional): If True, use one kernel to fused window shift & window partition for acceleration, similar for the reversed part. Default: False\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        dim,\n",
    "        num_heads,\n",
    "        window_size=7,\n",
    "        shift_size=0,\n",
    "        mlp_ratio=4.,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop=0.,\n",
    "        attn_drop=0.,\n",
    "        drop_path=0.,\n",
    "        act_layer=nn.GELU,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim,\n",
    "            window_size=to_2tuple(self.window_size),\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop,\n",
    "        )\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "\n",
    "    def forward(self, x, H, W, mask_matrix):\n",
    "\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # pad feature maps to multiples of window size\n",
    "        pad_l = pad_t = 0\n",
    "        pad_r = (self.window_size - W % self.window_size) % self.window_size\n",
    "        pad_b = (self.window_size - H % self.window_size) % self.window_size\n",
    "        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n",
    "        _, Hp, Wp, _ = x.shape\n",
    "\n",
    "\n",
    "        # cyclic shift ---\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "            attn_mask = mask_matrix\n",
    "        else:\n",
    "            shifted_x = x\n",
    "            attn_mask = None\n",
    "\n",
    "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n",
    "        attn_windows = self.attn(x_windows, mask=attn_mask)  # nW*B, window_size*window_size, C\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "\n",
    "\n",
    "        # reverse cyclic shift ---\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, Hp, Wp)  # B H' W' C\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "\n",
    "        if pad_r > 0 or pad_b > 0:\n",
    "            x = x[:, :H, :W, :].contiguous()\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, num_heads={self.num_heads}, \" \\\n",
    "               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n",
    "\n",
    "    \n",
    "class SwinTransformerV1(nn.Module):\n",
    "    def __init__(self,\n",
    "        pretrain_img_size=224,\n",
    "        patch_size=4,\n",
    "        in_chans=3,\n",
    "        embed_dim=96,\n",
    "        depths=[2, 2, 6, 2],\n",
    "        num_heads=[3, 6, 12, 24],\n",
    "        window_size=7,\n",
    "        mlp_ratio=4.,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.,\n",
    "        attn_drop_rate=0.,\n",
    "        drop_path_rate=0.1,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        patch_norm=True,\n",
    "        out_norm = nn.Identity, #use nn.Identity, nn.BatchNorm2d, LayerNorm2d\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pretrain_img_size = pretrain_img_size\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if patch_norm else None\n",
    "        )\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = np.linspace(0, drop_path_rate, sum(depths)).tolist() # stochastic depth decay rule\n",
    "\n",
    "        # build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(self.num_layers):\n",
    "            layer = BasicLayer(\n",
    "                dim=int(embed_dim * 2 ** i),\n",
    "                depth=depths[i],\n",
    "                num_heads=num_heads[i],\n",
    "                window_size=window_size,\n",
    "                mlp_ratio=self.mlp_ratio,\n",
    "                qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[sum(depths[:i]):sum(depths[:i + 1])],\n",
    "                norm_layer=norm_layer,\n",
    "                downsample=PatchMerging if (i < self.num_layers - 1) else None,\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        #---\n",
    "        # add a norm layer for each output\n",
    "        self.out_norm = nn.ModuleList(\n",
    "            [ out_norm(int(embed_dim * 2 ** i)) for i in range(self.num_layers)]\n",
    "        )\n",
    "\n",
    "        #---\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        Wh, Ww = x.size(2), x.size(3)\n",
    "\n",
    "        #positional encode?\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        outs = []\n",
    "        for i in range(self.num_layers):\n",
    "            x_out, H, W, x, Wh, Ww = self.layers[i](x, Wh, Ww)\n",
    "            out = x_out.view(-1, H, W, int(self.embed_dim * 2 ** i)).permute(0, 3, 1, 2).contiguous()\n",
    "            out = self.out_norm[i](out)\n",
    "            outs.append(out)\n",
    "\n",
    "        return outs\n",
    "class WindowAttention(nn.Module):\n",
    "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** (-0.5)\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        relative_position_bias = \\\n",
    "            self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], self.num_heads)\n",
    "            # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n",
    "    \n",
    "    \n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (int): window size\n",
    "\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (int): Window size\n",
    "        H (int): Height of image\n",
    "        W (int): Width of image\n",
    "\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "\n",
    "def conv3x3_bn_relu(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution + BN + relu\"\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "        nn.BatchNorm2d(out_planes),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "\n",
    "class UPerDecoder(nn.Module):\n",
    "    def __init__(self,\n",
    "        in_dim=[256, 512, 1024, 2048],\n",
    "        ppm_pool_scale=[1, 2, 3, 6],\n",
    "        ppm_dim=512,\n",
    "        fpn_out_dim=256\n",
    "    ):\n",
    "        super(UPerDecoder, self).__init__()\n",
    "\n",
    "        # PPM ----\n",
    "        dim = in_dim[-1]\n",
    "        ppm_pooling = []\n",
    "        ppm_conv = []\n",
    "\n",
    "        for scale in ppm_pool_scale:\n",
    "            ppm_pooling.append(\n",
    "                nn.AdaptiveAvgPool2d(scale)\n",
    "            )\n",
    "            ppm_conv.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(dim, ppm_dim, kernel_size=1, bias=False),\n",
    "                    nn.BatchNorm2d(ppm_dim),\n",
    "                    nn.ReLU(inplace=True)\n",
    "                )\n",
    "            )\n",
    "        self.ppm_pooling   = nn.ModuleList(ppm_pooling)\n",
    "        self.ppm_conv      = nn.ModuleList(ppm_conv)\n",
    "        self.ppm_out = conv3x3_bn_relu(dim + len(ppm_pool_scale)*ppm_dim, fpn_out_dim, 1)\n",
    "\n",
    "        # FPN ----\n",
    "        fpn_in = []\n",
    "        for i in range(0, len(in_dim)-1):  # skip the top layer\n",
    "            fpn_in.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_dim[i], fpn_out_dim, kernel_size=1, bias=False),\n",
    "                    nn.BatchNorm2d(fpn_out_dim),\n",
    "                    nn.ReLU(inplace=True)\n",
    "                )\n",
    "            )\n",
    "        self.fpn_in = nn.ModuleList(fpn_in)\n",
    "\n",
    "        fpn_out = []\n",
    "        for i in range(len(in_dim) - 1):  # skip the top layer\n",
    "            fpn_out.append(\n",
    "                conv3x3_bn_relu(fpn_out_dim, fpn_out_dim, 1),\n",
    "            )\n",
    "        self.fpn_out = nn.ModuleList(fpn_out)\n",
    "\n",
    "        self.fpn_fuse = nn.Sequential(\n",
    "            conv3x3_bn_relu(len(in_dim) * fpn_out_dim, fpn_out_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, feature):\n",
    "        f = feature[-1]\n",
    "        pool_shape = f.shape[2:]\n",
    "\n",
    "        ppm_out = [f]\n",
    "        for pool, conv in zip(self.ppm_pooling, self.ppm_conv):\n",
    "            p = pool(f)\n",
    "            p = F.interpolate(p, size=pool_shape, mode='bilinear', align_corners=False)\n",
    "            p = conv(p)\n",
    "            ppm_out.append(p)\n",
    "        ppm_out = torch.cat(ppm_out, 1)\n",
    "        down = self.ppm_out(ppm_out)\n",
    "\n",
    "        fpn_out = [down]\n",
    "        for i in reversed(range(len(feature) - 1)):\n",
    "            lateral = feature[i]\n",
    "            lateral = self.fpn_in[i](lateral) # lateral branch\n",
    "            down = F.interpolate(down, size=lateral.shape[2:], mode='bilinear', align_corners=False) # top-down branch\n",
    "            down = down + lateral\n",
    "            fpn_out.append(self.fpn_out[i](down))\n",
    "\n",
    "        fpn_out.reverse() # [P2 - P5]\n",
    "        fusion_shape = fpn_out[0].shape[2:]\n",
    "        fusion = [fpn_out[0]]\n",
    "        for i in range(1, len(fpn_out)):\n",
    "            fusion.append(\n",
    "                F.interpolate( fpn_out[i], fusion_shape, mode='bilinear', align_corners=False)\n",
    "            )\n",
    "        x = self.fpn_fuse( torch.cat(fusion, 1))\n",
    "\n",
    "        return x, fusion\n",
    "    \n",
    "    \n",
    "    \n",
    "class LayerNorm2d(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.bias = nn.Parameter(torch.zeros(dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.mean(1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.eps)\n",
    "        x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "def criterion_aux_loss(logit, mask):\n",
    "    mask = F.interpolate(mask,size=logit.shape[-2:], mode='nearest')\n",
    "    loss = F.binary_cross_entropy_with_logits(logit,mask)\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def load_pretrain( self,):\n",
    "\n",
    "        checkpoint = cfg[self.arch]['checkpoint']\n",
    "        print('loading %s ...'%checkpoint)\n",
    "        checkpoint = torch.load(checkpoint, map_location=lambda storage, loc: storage)['model']\n",
    "        if 0:\n",
    "            skip = ['relative_coords_table','relative_position_index']\n",
    "            filtered={}\n",
    "            for k,v in checkpoint.items():\n",
    "                if any([s in k for s in skip ]): continue\n",
    "                filtered[k]=v\n",
    "            checkpoint = filtered\n",
    "        print(self.encoder.load_state_dict(checkpoint,strict=False))  #True\n",
    "\n",
    "\n",
    "    def __init__( self,):\n",
    "        super(Net, self).__init__()\n",
    "        self.output_type = ['inference']\n",
    "\n",
    "        self.rgb = RGB()\n",
    "        self.arch = 'swin_tiny_patch4_window7_224'\n",
    "\n",
    "        self.encoder = SwinTransformerV1(\n",
    "            ** {**cfg['basic']['swin'], **cfg[self.arch]['swin'],\n",
    "                **{'out_norm' : LayerNorm2d} }\n",
    "        )\n",
    "        encoder_dim =cfg[self.arch]['upernet']['in_channels']\n",
    "        #[96, 192, 384, 768]\n",
    "\n",
    "        self.decoder = UPerDecoder(\n",
    "            in_dim=encoder_dim,\n",
    "            ppm_pool_scale=[1, 2, 3, 6],\n",
    "            ppm_dim=512,\n",
    "            fpn_out_dim=256\n",
    "        )\n",
    "\n",
    "        self.logit = nn.Sequential(\n",
    "            nn.Conv2d(256, 1, kernel_size=1)\n",
    "        )\n",
    "        self.aux = nn.ModuleList([\n",
    "            nn.Conv2d(256, 1, kernel_size=1, padding=0) for i in range(4)\n",
    "        ])\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x = batch['image']\n",
    "        B,C,H,W = x.shape\n",
    "        x = self.rgb(x)\n",
    "        encoder = self.encoder(x)\n",
    "        last, decoder = self.decoder(encoder)\n",
    "        logit = self.logit(last)\n",
    "        logit = F.interpolate(logit, size=None, scale_factor=4, mode='bilinear', align_corners=False)\n",
    "\n",
    "        output = {}\n",
    "        if 'loss' in self.output_type:\n",
    "            output['bce_loss'] = F.binary_cross_entropy_with_logits(logit,batch['mask'])\n",
    "            for i in range(4):\n",
    "                output['aux%d_loss'%i] = criterion_aux_loss(self.aux[i](decoder[i]),batch['mask'])\n",
    "\n",
    "        if 'inference' in self.output_type:\n",
    "            output['probability'] = torch.sigmoid(logit)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9763cc",
   "metadata": {
    "papermill": {
     "duration": 0.008673,
     "end_time": "2022-08-30T05:14:29.888320",
     "exception": false,
     "start_time": "2022-08-30T05:14:29.879647",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:#c3448b; background:#efe9e9; border:1px dashed #efe50b;\" role=\"tab\" aria-controls=\"folds\"><center>Folds</center></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95b2f1e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T05:14:29.907221Z",
     "iopub.status.busy": "2022-08-30T05:14:29.906951Z",
     "iopub.status.idle": "2022-08-30T05:14:29.918570Z",
     "shell.execute_reply": "2022-08-30T05:14:29.917554Z"
    },
    "papermill": {
     "duration": 0.023581,
     "end_time": "2022-08-30T05:14:29.920563",
     "exception": false,
     "start_time": "2022-08-30T05:14:29.896982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    \"\"\" Multilayer perceptron.\"\"\"\n",
    "\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "cfg = dict(\n",
    "\n",
    "        #configs/_base_/models/upernet_swin.py\n",
    "        basic = dict(\n",
    "            swin=dict(\n",
    "                embed_dim=96,\n",
    "                depths=[2, 2, 6, 2],\n",
    "                num_heads=[3, 6, 12, 24],\n",
    "                window_size=7,\n",
    "                mlp_ratio=4.,\n",
    "                qkv_bias=True,\n",
    "                qk_scale=None,\n",
    "                drop_rate=0.,\n",
    "                attn_drop_rate=0.,\n",
    "                drop_path_rate=0.3,\n",
    "                ape=False,\n",
    "                patch_norm=True,\n",
    "                out_indices=(0, 1, 2, 3),\n",
    "                use_checkpoint=False\n",
    "            ),\n",
    "\n",
    "        ),\n",
    "\n",
    "        #configs/swin/upernet_swin_tiny_patch4_window7_512x512_160k_ade20k.py\n",
    "        swin_tiny_patch4_window7_224=dict(\n",
    "            checkpoint = pretrain_dir+'/swin_tiny_patch4_window7_224_22k.pth',\n",
    "\n",
    "            swin = dict(\n",
    "                embed_dim=96,\n",
    "                depths=[2, 2, 6, 2],\n",
    "                num_heads=[3, 6, 12, 24],\n",
    "                window_size=7,\n",
    "                ape=False,\n",
    "                drop_path_rate=0.3,\n",
    "                patch_norm=True,\n",
    "                use_checkpoint=False,\n",
    "            ),\n",
    "            upernet=dict(\n",
    "                in_channels=[96, 192, 384, 768],\n",
    "            ),\n",
    "        ),\n",
    "\n",
    "        #/configs/swin/upernet_swin_small_patch4_window7_512x512_160k_ade20k.py\n",
    "        swin_small_patch4_window7_224_22k=dict(\n",
    "            checkpoint = pretrain_dir+'/swin_small_patch4_window7_224_22k.pth',\n",
    "\n",
    "            swin = dict(\n",
    "                embed_dim=96,\n",
    "                depths=[2, 2, 18, 2],\n",
    "                num_heads=[3, 6, 12, 24],\n",
    "                window_size=7,\n",
    "                ape=False,\n",
    "                drop_path_rate=0.3,\n",
    "                patch_norm=True,\n",
    "                use_checkpoint=False\n",
    "            ),\n",
    "            upernet=dict(\n",
    "                in_channels=[96, 192, 384, 768],\n",
    "            ),\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4a8811d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T05:14:29.939674Z",
     "iopub.status.busy": "2022-08-30T05:14:29.938973Z",
     "iopub.status.idle": "2022-08-30T05:14:29.948415Z",
     "shell.execute_reply": "2022-08-30T05:14:29.947597Z"
    },
    "papermill": {
     "duration": 0.021058,
     "end_time": "2022-08-30T05:14:29.950357",
     "exception": false,
     "start_time": "2022-08-30T05:14:29.929299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_fold(fold=3):\n",
    "    df = pd.read_csv('../input/hubmap-organ-segmentation/train.csv')\n",
    "\n",
    "    num_fold = 5\n",
    "    skf = KFold(n_splits=num_fold, shuffle=True,random_state=42)\n",
    "\n",
    "    df.loc[:,'fold']=-1\n",
    "    for f,(t_idx, v_idx) in enumerate(skf.split(X=df['id'], y=df['organ'])):\n",
    "        df.iloc[v_idx,-1]=f\n",
    "\n",
    "    #check\n",
    "    if 0:\n",
    "        for f in range(num_fold):\n",
    "            train_df=df[df.fold!=f].reset_index(drop=True)\n",
    "            valid_df=df[df.fold==f].reset_index(drop=True)\n",
    "\n",
    "            print('fold %d'%f)\n",
    "            t = train_df.organ.value_counts().to_dict()\n",
    "            v = valid_df.organ.value_counts().to_dict()\n",
    "            for k in ['kidney', 'prostate', 'largeintestine', 'spleen', 'lung']:\n",
    "                print('%32s %3d (%0.3f)  %3d (%0.3f)'%(k,t.get(k,0),t.get(k,0)/len(train_df),v.get(k,0),v.get(k,0)/len(valid_df)))\n",
    "\n",
    "            print('')\n",
    "            zz=0\n",
    "\n",
    "    train_df=df[df.fold!=fold].reset_index(drop=True)\n",
    "    valid_df=df[df.fold==fold].reset_index(drop=True)\n",
    "    return train_df,valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5bf0170",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T05:14:29.969187Z",
     "iopub.status.busy": "2022-08-30T05:14:29.968676Z",
     "iopub.status.idle": "2022-08-30T05:14:29.974731Z",
     "shell.execute_reply": "2022-08-30T05:14:29.973861Z"
    },
    "papermill": {
     "duration": 0.017575,
     "end_time": "2022-08-30T05:14:29.976743",
     "exception": false,
     "start_time": "2022-08-30T05:14:29.959168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_dice_score(probability, mask, smooth = 1):\n",
    "    N = len(probability)\n",
    "    p = probability.reshape(N,-1)\n",
    "    t = mask.reshape(N,-1)\n",
    "\n",
    "    p = p>0.5\n",
    "    t = t>0.5\n",
    "    uion = p.sum(-1) + t.sum(-1)\n",
    "    overlap = (p*t).sum(-1)\n",
    "    dice = 2*overlap/(uion+0.0001)\n",
    "    return dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bfa4c79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T05:14:29.995785Z",
     "iopub.status.busy": "2022-08-30T05:14:29.995532Z",
     "iopub.status.idle": "2022-08-30T05:14:29.999705Z",
     "shell.execute_reply": "2022-08-30T05:14:29.998762Z"
    },
    "papermill": {
     "duration": 0.01575,
     "end_time": "2022-08-30T05:14:30.001663",
     "exception": false,
     "start_time": "2022-08-30T05:14:29.985913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_learning_rate(optimizer):\n",
    "    return optimizer.param_groups[0]['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56d0bfcc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T05:14:30.020440Z",
     "iopub.status.busy": "2022-08-30T05:14:30.019860Z",
     "iopub.status.idle": "2022-08-30T05:14:30.025037Z",
     "shell.execute_reply": "2022-08-30T05:14:30.024126Z"
    },
    "papermill": {
     "duration": 0.016784,
     "end_time": "2022-08-30T05:14:30.027196",
     "exception": false,
     "start_time": "2022-08-30T05:14:30.010412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# organ_threshold = {\n",
    "#     'kidney': 0.50,\n",
    "#     'prostate': 0.50,\n",
    "#     'largeintestine': 0.50,\n",
    "#     'spleen': 0.50,\n",
    "#     'lung': 0.20,\n",
    "# }   \n",
    "\n",
    "organ_threshold = {\n",
    "    'Hubmap': {\n",
    "        'kidney'        : 0.40,\n",
    "        'prostate'      : 0.40,\n",
    "        'largeintestine': 0.40,\n",
    "        'spleen'        : 0.40,\n",
    "        'lung'          : 0.10,\n",
    "    },\n",
    "    'HPA': {\n",
    "        'kidney'        : 0.50,\n",
    "        'prostate'      : 0.50,\n",
    "        'largeintestine': 0.50,\n",
    "        'spleen'        : 0.50,\n",
    "        'lung'          : 0.10,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a0353f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T05:14:30.046273Z",
     "iopub.status.busy": "2022-08-30T05:14:30.045760Z",
     "iopub.status.idle": "2022-08-30T05:14:30.053123Z",
     "shell.execute_reply": "2022-08-30T05:14:30.052238Z"
    },
    "papermill": {
     "duration": 0.019336,
     "end_time": "2022-08-30T05:14:30.055152",
     "exception": false,
     "start_time": "2022-08-30T05:14:30.035816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stochastic Weight Averaging\n",
    "# https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/\n",
    "def do_swa(checkpoint):\n",
    "    skip = ['relative_position_index', 'num_batches_tracked']\n",
    "    \n",
    "    K = len(checkpoint)\n",
    "    swa = None\n",
    "    \n",
    "    for k in range(K):\n",
    "        state_dict = torch.load(checkpoint[k], map_location=lambda storage, loc: storage)['state_dict']\n",
    "        if swa is None:\n",
    "            swa = state_dict\n",
    "        else:\n",
    "            for k, v in state_dict.items():\n",
    "                #print(k)\n",
    "                if any(s in k for s in skip): continue\n",
    "                swa[k] += v\n",
    "    \n",
    "    for k, v in swa.items():\n",
    "        if any(s in k for s in skip): continue\n",
    "        swa[k] /= K\n",
    "    \n",
    "    return swa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ea72e0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T05:14:30.073788Z",
     "iopub.status.busy": "2022-08-30T05:14:30.073535Z",
     "iopub.status.idle": "2022-08-30T05:14:30.079455Z",
     "shell.execute_reply": "2022-08-30T05:14:30.078515Z"
    },
    "papermill": {
     "duration": 0.017739,
     "end_time": "2022-08-30T05:14:30.081456",
     "exception": false,
     "start_time": "2022-08-30T05:14:30.063717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rle_decode(mask_rle, shape, color=1):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.float32)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo : hi] = color\n",
    "    return img.reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c380a879",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T05:14:30.099849Z",
     "iopub.status.busy": "2022-08-30T05:14:30.099595Z",
     "iopub.status.idle": "2022-08-30T05:14:30.103982Z",
     "shell.execute_reply": "2022-08-30T05:14:30.102861Z"
    },
    "papermill": {
     "duration": 0.016032,
     "end_time": "2022-08-30T05:14:30.106044",
     "exception": false,
     "start_time": "2022-08-30T05:14:30.090012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#root_dir = '../input/swin-10k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef8d718b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T05:14:30.125180Z",
     "iopub.status.busy": "2022-08-30T05:14:30.124355Z",
     "iopub.status.idle": "2022-08-30T05:14:30.128730Z",
     "shell.execute_reply": "2022-08-30T05:14:30.127902Z"
    },
    "papermill": {
     "duration": 0.015828,
     "end_time": "2022-08-30T05:14:30.130708",
     "exception": false,
     "start_time": "2022-08-30T05:14:30.114880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79f7a43d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T05:14:30.149987Z",
     "iopub.status.busy": "2022-08-30T05:14:30.149318Z",
     "iopub.status.idle": "2022-08-30T05:14:30.156016Z",
     "shell.execute_reply": "2022-08-30T05:14:30.155184Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.018555,
     "end_time": "2022-08-30T05:14:30.157992",
     "exception": false,
     "start_time": "2022-08-30T05:14:30.139437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #out_dir = root_dir + '/result/upernet-swin-v1-tiny-aux5-768'\n",
    "# all_df = []\n",
    "# valid = {\n",
    "#     3: [\n",
    "#         '../input/kaggle-hubmap-model-weights/00008514.model.pth',\n",
    "#         '../input/kaggle-hubmap-model-weights/00008712.model.pth',\n",
    "#         '../input/kaggle-hubmap-model-weights/00008910.model.pth',\n",
    "#         ],\n",
    "# }\n",
    "\n",
    "# all_score_df = []\n",
    "# for f, checkpoint in valid.items():\n",
    "#     if len(checkpoint)==0: \n",
    "#         continue\n",
    "\n",
    "#     #project_name = out_dir.split('/')[-1]\n",
    "#     #fold_dir = out_dir + '/fold-%d' % f\n",
    "\n",
    "#     #checkpoint = [fold_dir + '/checkpoint' + c for c in checkpoint]\n",
    "#     checkpoint = [c for c in checkpoint]\n",
    "#     swa = do_swa(checkpoint)\n",
    "#     torch.save({\n",
    "#         'state_dict': swa,\n",
    "#         'swa': [c.split('/')[-1] for c in checkpoint],\n",
    "#     }, \n",
    "#         f'./fold-{f}-swa.pth')\n",
    "#     net = EffUnet(config.model).cuda()\n",
    "# #     net = Net().cuda()\n",
    "#     state_dict = swa\n",
    "#     net.load_state_dict(state_dict, strict=False)  # True\n",
    "\n",
    "#     net = net.eval()\n",
    "#     net.output_type = ['inference']\n",
    "\n",
    "#     result = {\n",
    "#         'id': [],\n",
    "#         'probability': [],\n",
    "#         'rle': [],\n",
    "#     }\n",
    "    \n",
    "# #     df = pd.read_csv(\"../input/hubmap-organ-segmentation/train.csv\").iloc[0:10]\n",
    "#     start_timer = time.time()\n",
    "#     for t, d in tqdm(df.iterrows()):\n",
    "        \n",
    "#         fname = d['id']\n",
    "#         image = tifffile.imread(f'../input/hubmap-organ-segmentation/test_images/{fname}.tiff')\n",
    "#         image = image.astype(np.float32)/255\n",
    "        \n",
    "#         H, W, _ = image.shape\n",
    "\n",
    "#         s = d.pixel_size / 0.4 * (image_size / 3000)\n",
    "#         image = cv2.resize(image, dsize=(image_size,image_size), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "#         image = image_to_tensor(image)\n",
    "#         image = image.cuda()\n",
    "#         batch = {\n",
    "#             'image':\n",
    "#                 torch.stack([\n",
    "#                     image,\n",
    "#                     torch.flip(image, [1]),\n",
    "#                     torch.flip(image, [2]),\n",
    "#                 ]),  # simple TTA\n",
    "#         }\n",
    "        \n",
    "#         probability = 0\n",
    "#         with torch.no_grad():\n",
    "#             with amp.autocast(enabled=is_amp):\n",
    "#                 output = net(batch) \n",
    "\n",
    "#                 probability += F.interpolate(\n",
    "#                     output, size=(H, W), mode='bilinear', align_corners=False, antialias=True)\n",
    "\n",
    "#         # undo TTA\n",
    "#         probability[1] = torch.flip(probability[1], [1])\n",
    "#         probability[2] = torch.flip(probability[2], [2])\n",
    "#         probability = probability.float().data.cpu().numpy().mean(0)[0]\n",
    "#         #p = probability > organ_threshold[d.organ] \n",
    "#         p = probability > organ_threshold[d.data_source][d.organ] \n",
    "#         p = p.T\n",
    "#         rle = rle_encode(p)\n",
    "#         #rle = rle\n",
    "\n",
    "#         result['rle'].append(rle)\n",
    "#         result['probability'].append(p)\n",
    "#         result['id'].append(fname)\n",
    "#         print('\\r', t, end='', flush=True)\n",
    "#     print('')\n",
    "#     submit_df = pd.DataFrame({'id': result['id'], 'rle': result['rle']})\n",
    "#     print(submit_df)\n",
    "#     all_df.append(submit_df)\n",
    "#     print('submit_df ok!')\n",
    "#     print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08440839",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T05:14:30.176948Z",
     "iopub.status.busy": "2022-08-30T05:14:30.176694Z",
     "iopub.status.idle": "2022-08-30T05:14:30.180676Z",
     "shell.execute_reply": "2022-08-30T05:14:30.179800Z"
    },
    "papermill": {
     "duration": 0.015681,
     "end_time": "2022-08-30T05:14:30.182817",
     "exception": false,
     "start_time": "2022-08-30T05:14:30.167136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# submit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bc338ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T05:14:30.201766Z",
     "iopub.status.busy": "2022-08-30T05:14:30.200962Z",
     "iopub.status.idle": "2022-08-30T05:14:30.205565Z",
     "shell.execute_reply": "2022-08-30T05:14:30.204724Z"
    },
    "papermill": {
     "duration": 0.016033,
     "end_time": "2022-08-30T05:14:30.207474",
     "exception": false,
     "start_time": "2022-08-30T05:14:30.191441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plt.imshow(rle_decode(submit_df[\"rle\"].iloc[0], (2023,2023)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48f721cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T05:14:30.226296Z",
     "iopub.status.busy": "2022-08-30T05:14:30.225728Z",
     "iopub.status.idle": "2022-08-30T05:14:30.230477Z",
     "shell.execute_reply": "2022-08-30T05:14:30.229306Z"
    },
    "papermill": {
     "duration": 0.016614,
     "end_time": "2022-08-30T05:14:30.232844",
     "exception": false,
     "start_time": "2022-08-30T05:14:30.216230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plt.imshow(result['probability'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea7ea0e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T05:14:30.251655Z",
     "iopub.status.busy": "2022-08-30T05:14:30.250981Z",
     "iopub.status.idle": "2022-08-30T05:14:30.414151Z",
     "shell.execute_reply": "2022-08-30T05:14:30.413253Z"
    },
    "papermill": {
     "duration": 0.17487,
     "end_time": "2022-08-30T05:14:30.416268",
     "exception": false,
     "start_time": "2022-08-30T05:14:30.241398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we can also transfer test set to training set\n",
    "import staintools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_target(organ_type):\n",
    "    if organ_type == \"kidney\":\n",
    "        target = staintools.read_image(\"/kaggle/input/hubmap-organ-segmentation/train_images/15005.tiff\")\n",
    "        return target\n",
    "    elif organ_type == \"largeintestine\":\n",
    "        target = staintools.read_image(\"/kaggle/input/hubmap-organ-segmentation/train_images/9777.tiff\")\n",
    "        return target\n",
    "    elif organ_type == \"lung\":\n",
    "        target = staintools.read_image(\"/kaggle/input/hubmap-organ-segmentation/train_images/4301.tiff\")\n",
    "        return target\n",
    "    elif organ_type == \"prostate\":\n",
    "        target = staintools.read_image(\"/kaggle/input/hubmap-organ-segmentation/train_images/12244.tiff\")\n",
    "        return target\n",
    "    elif organ_type == \"spleen\":\n",
    "        target = staintools.read_image(\"/kaggle/input/hubmap-organ-segmentation/train_images/1123.tiff\")\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bad519ae",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-08-30T05:14:30.435822Z",
     "iopub.status.busy": "2022-08-30T05:14:30.435537Z",
     "iopub.status.idle": "2022-08-30T05:14:56.873389Z",
     "shell.execute_reply": "2022-08-30T05:14:56.871952Z"
    },
    "papermill": {
     "duration": 26.451186,
     "end_time": "2022-08-30T05:14:56.876611",
     "exception": false,
     "start_time": "2022-08-30T05:14:30.425425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0\n",
      "      id                                                rle\n",
      "0  10078  383 139 2406 139 4429 139 6452 139 8475 139 10...\n",
      "submit_df ok!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_df = []\n",
    "# valid = {\n",
    "# #     0: [\n",
    "# #         '../input/fold-0/result/upernet-swin-v1-tiny-aux5-768/fold-0/checkpoint/00004992.model.pth',\n",
    "# #         '../input/fold-0/result/upernet-swin-v1-tiny-aux5-768/fold-0/checkpoint/00007776.model.pth',\n",
    "# #         '../input/fold-0/result/upernet-swin-v1-tiny-aux5-768/fold-0/checkpoint/00009216.model.pth',\n",
    "# #         ],\n",
    "# #     1: [\n",
    "# #         '../input/fold-1/result/upernet-swin-v1-tiny-aux5-768/fold-1/checkpoint/00007296.model.pth',\n",
    "# #         '../input/fold-1/result/upernet-swin-v1-tiny-aux5-768/fold-1/checkpoint/00008736.model.pth',\n",
    "# #         '../input/fold-1/result/upernet-swin-v1-tiny-aux5-768/fold-1/checkpoint/00009120.model.pth',\n",
    "# #         ],\n",
    "# #     2: [\n",
    "# #         '../input/fold-2/result/upernet-swin-v1-tiny-aux5-768/fold-2/checkpoint/00005376.model.pth',\n",
    "# #         '../input/fold-2/result/upernet-swin-v1-tiny-aux5-768/fold-2/checkpoint/00008832.model.pth',\n",
    "# #         '../input/fold-2/result/upernet-swin-v1-tiny-aux5-768/fold-2/checkpoint/00009888.model.pth',\n",
    "# #         ],\n",
    "#     3: [\n",
    "#         '../input/fold-3/result/upernet-swin-v1-tiny-aux5-768/fold-3/checkpoint/00008019.model.pth',\n",
    "#         '../input/fold-3/result/upernet-swin-v1-tiny-aux5-768/fold-3/checkpoint/00008910.model.pth',\n",
    "#         '../input/fold-3/result/upernet-swin-v1-tiny-aux5-768/fold-3/checkpoint/00009900.model.pth',\n",
    "#         ],\n",
    "# }\n",
    "\n",
    "\n",
    "result = {\n",
    "        'id': [],\n",
    "        'probability': [],\n",
    "        'rle': [],\n",
    "    }\n",
    "# df = pd.read_csv(\"../input/hubmap-organ-segmentation/train.csv\")\n",
    "start_timer = time.time()\n",
    "for t, d in df.iterrows():\n",
    "    target = get_target(d['organ'])\n",
    "    fname = d['id']\n",
    "    image = staintools.read_image(f'../input/hubmap-organ-segmentation/test_images/{fname}.tiff')\n",
    "    target = staintools.LuminosityStandardizer.standardize(target)\n",
    "    image = staintools.LuminosityStandardizer.standardize(image)\n",
    "    \n",
    "    # Stain normalize\n",
    "    normalizer = staintools.StainNormalizer(method='vahadane')\n",
    "    normalizer.fit(target)\n",
    "    transformed3 = normalizer.transform(image)\n",
    "    image = transformed3\n",
    "    \n",
    "    image = image.astype(np.float32)/255\n",
    "\n",
    "    H, W, _ = image.shape\n",
    "\n",
    "    s = d.pixel_size / 0.4 * (image_size / 3000)\n",
    "    image = cv2.resize(image, dsize=(image_size,image_size), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    image = image_to_tensor(image)\n",
    "    image = image.cuda()\n",
    "    batch = {\n",
    "        'image':\n",
    "            torch.stack([\n",
    "                image,\n",
    "                torch.flip(image, [1]),\n",
    "                torch.flip(image, [2]),\n",
    "            ]),  # simple TTA\n",
    "    }\n",
    "    \n",
    "    use = 0\n",
    "    probability = 0\n",
    "    with torch.no_grad():\n",
    "        with amp.autocast(enabled = True):\n",
    "            \n",
    "            net = Net().cuda()\n",
    "            state_dict = torch.load('../input/hubmap-swintf-unet-weights/00011880.model.pth',\n",
    "                        map_location=torch.device('cpu'))\n",
    "            net.load_state_dict(state_dict['state_dict'])\n",
    "#             state_dict = swa\n",
    "#             net.load_state_dict(state_dict, strict=False)  # True\n",
    "\n",
    "            net = net.eval()\n",
    "#             net.output_type = ['inference']\n",
    "            use += 1\n",
    "\n",
    "            output = net(batch)\n",
    "            #print(output['probability'].shape)\n",
    "            probability += F.interpolate(\n",
    "                output['probability'], size=(H, W), mode='bilinear', align_corners=False, antialias=True)\n",
    "\n",
    "        #probability = probability / 4\n",
    "        probability[0] = probability[0]   \n",
    "        probability[1] = torch.flip(probability[1], [1])\n",
    "        probability[2] = torch.flip(probability[2], [2])\n",
    "        probability = probability.mean(0, keepdims=True)\n",
    "        probability = probability[0,0].float()\n",
    "        #probability = probability.float().data.cpu().numpy().mean(0)[0]\n",
    "        \n",
    "    probability = probability.data.cpu().numpy()\n",
    "    #p = probability > organ_threshold[d.organ]\n",
    "    p = probability > organ_threshold[d.data_source][d.organ]\n",
    "    p = p.T\n",
    "\n",
    "    rle = rle_encode(p)\n",
    "    \n",
    "    result['rle'].append(rle)\n",
    "    result['probability'].append(p)\n",
    "    result['id'].append(fname)\n",
    "    print('\\r', t, end='', flush=True)\n",
    "    print('')\n",
    "\n",
    "submit_df = pd.DataFrame({'id': result['id'], 'rle': result['rle']})\n",
    "print(submit_df)\n",
    "print('submit_df ok!')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ec54856",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T05:14:56.898495Z",
     "iopub.status.busy": "2022-08-30T05:14:56.898155Z",
     "iopub.status.idle": "2022-08-30T05:14:56.903177Z",
     "shell.execute_reply": "2022-08-30T05:14:56.901951Z"
    },
    "papermill": {
     "duration": 0.02221,
     "end_time": "2022-08-30T05:14:56.909352",
     "exception": false,
     "start_time": "2022-08-30T05:14:56.887142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plt.imshow(result['probability'][0].reshape(3000,3000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "515019f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T05:14:56.930575Z",
     "iopub.status.busy": "2022-08-30T05:14:56.930311Z",
     "iopub.status.idle": "2022-08-30T05:14:56.934199Z",
     "shell.execute_reply": "2022-08-30T05:14:56.933246Z"
    },
    "papermill": {
     "duration": 0.016556,
     "end_time": "2022-08-30T05:14:56.936324",
     "exception": false,
     "start_time": "2022-08-30T05:14:56.919768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# plt.imshow(rle_decode(submit_df[\"rle\"].iloc[0], (2023,2023)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a672bc91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T05:14:56.955031Z",
     "iopub.status.busy": "2022-08-30T05:14:56.954725Z",
     "iopub.status.idle": "2022-08-30T05:14:56.958958Z",
     "shell.execute_reply": "2022-08-30T05:14:56.957959Z"
    },
    "papermill": {
     "duration": 0.016404,
     "end_time": "2022-08-30T05:14:56.961418",
     "exception": false,
     "start_time": "2022-08-30T05:14:56.945014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# type(submit_df[\"rle\"].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9bf7bf60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T05:14:56.980298Z",
     "iopub.status.busy": "2022-08-30T05:14:56.980016Z",
     "iopub.status.idle": "2022-08-30T05:14:56.984595Z",
     "shell.execute_reply": "2022-08-30T05:14:56.983702Z"
    },
    "papermill": {
     "duration": 0.016294,
     "end_time": "2022-08-30T05:14:56.986630",
     "exception": false,
     "start_time": "2022-08-30T05:14:56.970336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.imshow(p)\n",
    "# print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9d48eed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T05:14:57.005973Z",
     "iopub.status.busy": "2022-08-30T05:14:57.005727Z",
     "iopub.status.idle": "2022-08-30T05:14:57.163060Z",
     "shell.execute_reply": "2022-08-30T05:14:57.162137Z"
    },
    "papermill": {
     "duration": 0.169546,
     "end_time": "2022-08-30T05:14:57.165418",
     "exception": false,
     "start_time": "2022-08-30T05:14:56.995872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1247bc81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T05:14:57.185746Z",
     "iopub.status.busy": "2022-08-30T05:14:57.185463Z",
     "iopub.status.idle": "2022-08-30T05:14:57.192427Z",
     "shell.execute_reply": "2022-08-30T05:14:57.191464Z"
    },
    "papermill": {
     "duration": 0.019236,
     "end_time": "2022-08-30T05:14:57.194652",
     "exception": false,
     "start_time": "2022-08-30T05:14:57.175416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submit_df.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb57ef09",
   "metadata": {
    "papermill": {
     "duration": 0.008689,
     "end_time": "2022-08-30T05:14:57.212304",
     "exception": false,
     "start_time": "2022-08-30T05:14:57.203615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 102.308572,
   "end_time": "2022-08-30T05:14:59.569841",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-30T05:13:17.261269",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
