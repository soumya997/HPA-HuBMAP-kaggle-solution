{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-08-24T14:48:15.870113Z","iopub.status.busy":"2022-08-24T14:48:15.869742Z","iopub.status.idle":"2022-08-24T14:48:15.877428Z","shell.execute_reply":"2022-08-24T14:48:15.876250Z","shell.execute_reply.started":"2022-08-24T14:48:15.870082Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/lakshita/somusan/hubmap_kaggle/.venv/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import argparse\n","import os\n","import sys\n","import time\n","import numpy as np\n","import torch\n","from torch import nn\n","from torch.cuda import amp\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-08-24T14:48:15.880798Z","iopub.status.busy":"2022-08-24T14:48:15.880145Z","iopub.status.idle":"2022-08-24T14:48:15.894781Z","shell.execute_reply":"2022-08-24T14:48:15.893567Z","shell.execute_reply.started":"2022-08-24T14:48:15.880760Z"},"trusted":true},"outputs":[],"source":["import os.path\n","\n","\n","class config:\n","    def __init__(self, fold=0):\n","        self.root_dir = \".\"\n","        self.pretrain_dir = '/home/lakshita/somusan/hubmap_kaggle/nbs/swin-tiny-small-22k-pretrained/'\n","        self.is_amp = True\n","        self.TRAIN = '/home/lakshita/somusan/hubmap_kaggle/hubmap_data/hubmap-organ-segmentation/train_images/'\n","        self.MASKS = '/home/lakshita/somusan/hubmap_kaggle/hubmap_data/mask_png/train_binary_masks/'\n","        self.LABELS = '/home/lakshita/somusan/hubmap_kaggle/hubmap_data/hubmap-organ-segmentation/train.csv'\n","\n","        self.fold = fold\n","\n","        self.out_dir = self.root_dir + f'/models/fold_{self.fold}'\n","\n","        if not os.path.exists(self.out_dir):\n","            os.mkdir(self.out_dir)\n","\n","        self.initial_checkpoint = None\n","\n","        self.start_lr = 5e-5  # 0.0001\n","        self.batch_size = 8  # 32 #32\n","\n","        self.checkpoint = dict(\n","\n","            # configs/_base_/models/upernet_swin.py\n","            basic=dict(\n","                swin=dict(\n","                    embed_dim=96,\n","                    depths=[2, 2, 6, 2],\n","                    num_heads=[3, 6, 12, 24],\n","                    window_size=7,\n","                    mlp_ratio=4.,\n","                    qkv_bias=True,\n","                    qk_scale=None,\n","                    drop_rate=0.,\n","                    attn_drop_rate=0.,\n","                    drop_path_rate=0.3,\n","                    ape=False,\n","                    patch_norm=True,\n","                    out_indices=(0, 1, 2, 3),\n","                    use_checkpoint=False\n","                ),\n","\n","            ),\n","\n","            # configs/swin/upernet_swin_tiny_patch4_window7_512x512_160k_ade20k.py\n","            swin_tiny_patch4_window7_224=dict(\n","                checkpoint=self.pretrain_dir + '/swin_tiny_patch4_window7_224_22k.pth',\n","\n","                swin=dict(\n","                    embed_dim=96,\n","                    depths=[2, 2, 6, 2],\n","                    num_heads=[3, 6, 12, 24],\n","                    window_size=7,\n","                    ape=False,\n","                    drop_path_rate=0.3,\n","                    patch_norm=True,\n","                    use_checkpoint=False,\n","                ),\n","                upernet=dict(\n","                    in_channels=[96, 192, 384, 768],\n","                ),\n","            ),\n","\n","            # /configs/swin/upernet_swin_small_patch4_window7_512x512_160k_ade20k.py\n","            swin_small_patch4_window7_224_22k=dict(\n","                checkpoint=self.pretrain_dir + '/swin_small_patch4_window7_224_22k.pth',\n","\n","                swin=dict(\n","                    embed_dim=96,\n","                    depths=[2, 2, 18, 2],\n","                    num_heads=[3, 6, 12, 24],\n","                    window_size=7,\n","                    ape=False,\n","                    drop_path_rate=0.3,\n","                    patch_norm=True,\n","                    use_checkpoint=False\n","                ),\n","                upernet=dict(\n","                    in_channels=[96, 192, 384, 768],\n","                ),\n","            ),\n","        )\n"]},{"cell_type":"code","execution_count":3,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-08-24T14:48:15.897608Z","iopub.status.busy":"2022-08-24T14:48:15.897146Z","iopub.status.idle":"2022-08-24T14:48:15.918718Z","shell.execute_reply":"2022-08-24T14:48:15.917419Z","shell.execute_reply.started":"2022-08-24T14:48:15.897573Z"},"trusted":true},"outputs":[],"source":["import random\n","\n","import cv2\n","import numpy as np\n","\n","\n","def do_random_flip(image, mask):\n","    if np.random.rand() > 0.5:\n","        image = cv2.flip(image, 0)\n","        mask = cv2.flip(mask, 0)\n","    if np.random.rand() > 0.5:\n","        image = cv2.flip(image, 1)\n","        mask = cv2.flip(mask, 1)\n","    if np.random.rand() > 0.5:\n","        image = image.transpose(1, 0, 2)\n","        mask = mask.transpose(1, 0)\n","\n","    image = np.ascontiguousarray(image)\n","    mask = np.ascontiguousarray(mask)\n","    return image, mask\n","\n","\n","def do_random_rot90(image, mask):\n","    r = np.random.choice([\n","        0,\n","        cv2.ROTATE_90_CLOCKWISE,\n","        cv2.ROTATE_90_COUNTERCLOCKWISE,\n","        cv2.ROTATE_180,\n","    ])\n","    if r == 0:\n","        return image, mask\n","    else:\n","        image = cv2.rotate(image, r)\n","        mask = cv2.rotate(mask, r)\n","        return image, mask\n","\n","\n","def do_random_contast(image, mask, mag=0.3):\n","    alpha = 1 + random.uniform(-1, 1) * mag\n","    image = image * alpha\n","    image = np.clip(image, 0, 1)\n","    return image, mask\n","\n","\n","def do_random_hsv(image, mask, mag=[0.15, 0.25, 0.25]):\n","    image = (image * 255).astype(np.uint8)\n","    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n","\n","    h = hsv[:, :, 0].astype(np.float32)  # hue\n","    s = hsv[:, :, 1].astype(np.float32)  # saturation\n","    v = hsv[:, :, 2].astype(np.float32)  # value\n","    h = (h * (1 + random.uniform(-1, 1) * mag[0])) % 180\n","    s = s * (1 + random.uniform(-1, 1) * mag[1])\n","    v = v * (1 + random.uniform(-1, 1) * mag[2])\n","\n","    hsv[:, :, 0] = np.clip(h, 0, 180).astype(np.uint8)\n","    hsv[:, :, 1] = np.clip(s, 0, 255).astype(np.uint8)\n","    hsv[:, :, 2] = np.clip(v, 0, 255).astype(np.uint8)\n","    image = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n","    image = image.astype(np.float32) / 255\n","    return image, mask\n","\n","\n","def do_random_noise(image, mask, mag=0.1):\n","    height, width = image.shape[:2]\n","    noise = np.random.uniform(-1, 1, (height, width, 1)) * mag\n","    image = image + noise\n","    image = np.clip(image, 0, 1)\n","    return image, mask\n","\n","\n","def do_random_rotate_scale(image, mask, angle=30, scale=[0.8, 1.2]):\n","    angle = np.random.uniform(-angle, angle)\n","    scale = np.random.uniform(*scale) if scale is not None else 1\n","\n","    height, width = image.shape[:2]\n","    center = (height // 2, width // 2)\n","\n","    transform = cv2.getRotationMatrix2D(center, angle, scale)\n","    image = cv2.warpAffine(image, transform, (width, height), flags=cv2.INTER_LINEAR,\n","                           borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0))\n","    mask = cv2.warpAffine(mask, transform, (width, height), flags=cv2.INTER_LINEAR,\n","                          borderMode=cv2.BORDER_CONSTANT, borderValue=0)\n","    return image, mask"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-08-24T14:48:15.920867Z","iopub.status.busy":"2022-08-24T14:48:15.920474Z","iopub.status.idle":"2022-08-24T14:48:15.931312Z","shell.execute_reply":"2022-08-24T14:48:15.930106Z","shell.execute_reply.started":"2022-08-24T14:48:15.920834Z"},"trusted":true},"outputs":[],"source":["def compute_dice_score(probability, mask):\n","    N = len(probability)\n","\n","    probability = probability > 0.5\n","    mask = mask > 0.5\n","\n","    p = probability.reshape(N, -1)\n","    t = mask.reshape(N, -1)\n","\n","    uion = p.sum(-1) + t.sum(-1)\n","    overlap = (p*t).sum(-1)\n","    dice = 2*overlap/(uion+0.0001)\n","    return dice"]},{"cell_type":"code","execution_count":5,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-08-24T14:48:16.015519Z","iopub.status.busy":"2022-08-24T14:48:16.015002Z","iopub.status.idle":"2022-08-24T14:48:16.056388Z","shell.execute_reply":"2022-08-24T14:48:16.055118Z","shell.execute_reply.started":"2022-08-24T14:48:16.015485Z"},"trusted":true},"outputs":[],"source":["import math\n","import time\n","import warnings\n","from itertools import repeat\n","import collections.abc\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","from sklearn.model_selection import KFold\n","from torch import nn\n","from torch.cuda import amp\n","\n","\n","def image_to_tensor(image, mode='bgr'):  # image mode\n","    if mode == 'bgr':\n","        image = image[:, :, ::-1]\n","    x = image\n","    x = x.transpose(2, 0, 1)\n","    x = np.ascontiguousarray(x)\n","    x = torch.tensor(x, dtype=torch.float)\n","    return x\n","\n","\n","def mask_to_tensor(mask):\n","    x = mask\n","    x = torch.tensor(x, dtype=torch.float)\n","    return x\n","\n","\n","tensor_list = ['mask', 'image', 'organ']\n","\n","\n","def null_collate(batch):\n","    d = {}\n","    key = batch[0].keys()\n","    for k in key:\n","        v = [b[k] for b in batch]\n","        if k in tensor_list:\n","            v = torch.stack(v)\n","        d[k] = v\n","\n","    d['mask'] = d['mask'].unsqueeze(1)\n","    d['organ'] = d['organ'].reshape(-1)\n","    return d\n","\n","\n","def _ntuple(n):\n","    def parse(x):\n","        if isinstance(x, collections.abc.Iterable):\n","            return x\n","        return tuple(repeat(x, n))\n","\n","    return parse\n","\n","\n","to_2tuple = _ntuple(2)\n","\n","\n","def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n","    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n","    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n","    def norm_cdf(x):\n","        # Computes standard normal cumulative distribution function\n","        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n","\n","    if (mean < a - 2 * std) or (mean > b + 2 * std):\n","        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n","                      \"The distribution of values may be incorrect.\",\n","                      stacklevel=2)\n","\n","\n","def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n","    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n","\n","\n","def drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):\n","    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n","    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n","    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n","    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n","    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n","    'survival rate' as the argument.\n","    \"\"\"\n","    if drop_prob == 0. or not training:\n","        return x\n","    keep_prob = 1 - drop_prob\n","    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n","    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n","    if keep_prob > 0.0 and scale_by_keep:\n","        random_tensor.div_(keep_prob)\n","    return x * random_tensor\n","\n","\n","class DropPath(nn.Module):\n","    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n","    \"\"\"\n","\n","    def __init__(self, drop_prob: float = 0., scale_by_keep: bool = True):\n","        super(DropPath, self).__init__()\n","        self.drop_prob = drop_prob\n","        self.scale_by_keep = scale_by_keep\n","\n","    def forward(self, x):\n","        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n","\n","    def extra_repr(self):\n","        return f'drop_prob={round(self.drop_prob, 3):0.3f}'\n","\n","\n","class RGB(nn.Module):\n","    IMAGE_RGB_MEAN = [0.485, 0.456, 0.406]  # [0.5, 0.5, 0.5]\n","    IMAGE_RGB_STD = [0.229, 0.224, 0.225]  # [0.5, 0.5, 0.5]\n","\n","    def __init__(self, ):\n","        super(RGB, self).__init__()\n","        self.register_buffer('mean', torch.zeros(1, 3, 1, 1))\n","        self.register_buffer('std', torch.ones(1, 3, 1, 1))\n","        self.mean.data = torch.FloatTensor(self.IMAGE_RGB_MEAN).view(self.mean.shape)\n","        self.std.data = torch.FloatTensor(self.IMAGE_RGB_STD).view(self.std.shape)\n","\n","    def forward(self, x):\n","        x = (x - self.mean) / self.std\n","        return x\n","\n","\n","def message(batch_loss=0, train_loss=0, iteration=0, iter_save=0, rate=0, epoch=0, valid_loss=None,\n","            start_timer=None,\n","            mode='print'):\n","    asterisk = ' '\n","    if mode == 'print':\n","        loss = batch_loss\n","    if mode == 'log':\n","        loss = train_loss\n","        if (iteration % iter_save == 0): asterisk = '*'\n","\n","    text = \\\n","        ('%0.2e   %08d%s %6.2f | ' % (rate, iteration, asterisk, epoch,)).replace('e-0', 'e-').replace('e+0', 'e+') + \\\n","        '%4.3f  %4.3f  %4.4f  %4.3f   | ' % (*valid_loss,) + \\\n","        '%4.3f  %4.3f   | ' % (*loss,) + \\\n","        '%s' % ((time.time() - start_timer))\n","\n","    return text\n","\n","\n","def valid_augment5(image, mask, organ):\n","    # image, mask  = do_crop(image, mask, image_size, xy=(None,None))\n","    return image, mask\n","\n","\n","def train_augment5b(image, mask, organ):\n","    image, mask = do_random_flip(image, mask)\n","    image, mask = do_random_rot90(image, mask)\n","\n","    for fn in np.random.choice([\n","        lambda image, mask: (image, mask),\n","        lambda image, mask: do_random_noise(image, mask, mag=0.1),\n","        lambda image, mask: do_random_contast(image, mask, mag=0.40),\n","        lambda image, mask: do_random_hsv(image, mask, mag=[0.40, 0.40, 0])\n","    ], 2): image, mask = fn(image, mask)\n","\n","    for fn in np.random.choice([\n","        lambda image, mask: (image, mask),\n","        lambda image, mask: do_random_rotate_scale(image, mask, angle=45, scale=[0.50, 2.0]),\n","    ], 1): image, mask = fn(image, mask)\n","\n","    return image, mask\n","\n","\n","def make_fold(config, fold=0):\n","    df = pd.read_csv(\"/home/lakshita/somusan/hubmap_kaggle/hubmap_data/hubmap-organ-segmentation/train.csv\")\n","    # path_change\n","    num_fold = 5\n","    skf = KFold(n_splits=num_fold, shuffle=True, random_state=42)\n","\n","    df.loc[:, 'fold'] = -1\n","    for f, (t_idx, v_idx) in enumerate(skf.split(X=df['id'], y=df['organ'])):\n","        df.iloc[v_idx, -1] = f\n","\n","    # #check\n","    # if 0:\n","    #     for f in range(num_fold):\n","    #         train_df=df[df.fold!=f].reset_index(drop=True)\n","    #         valid_df=df[df.fold==f].reset_index(drop=True)\n","    #\n","    #         print('fold %d'%f)\n","    #         t = train_df.organ.value_counts().to_dict()\n","    #         v = valid_df.organ.value_counts().to_dict()\n","    #         for k in ['kidney', 'prostate', 'largeintestine', 'spleen', 'lung']:\n","    #             print('%32s %3d (%0.3f)  %3d (%0.3f)'%(k,t.get(k,0),t.get(k,0)/len(train_df),v.get(k,0),v.get(k,0)/len(valid_df)))\n","    #\n","    #         print('')\n","    #         zz=0\n","\n","    train_df = df[df.fold != fold].reset_index(drop=True)\n","    valid_df = df[df.fold == fold].reset_index(drop=True)\n","    return train_df, valid_df\n","\n","\n","def validate(net, valid_loader, config):\n","    valid_num = 0\n","    valid_probability = []\n","    valid_mask = []\n","    valid_loss = 0\n","\n","    net = net.eval()\n","    start_timer = time.time()\n","    for t, batch in enumerate(valid_loader):\n","\n","        net.output_type = ['loss', 'inference']\n","        with torch.no_grad():\n","            with amp.autocast(enabled=config.is_amp):\n","                batch_size = len(batch['index'])\n","                batch['image'] = batch['image'].cuda()\n","                batch['mask'] = batch['mask'].cuda()\n","                batch['organ'] = batch['organ'].cuda()\n","\n","                output = net(batch)\n","                loss0 = output['bce_loss'].mean()\n","\n","        valid_probability.append(output['probability'].data.cpu().numpy())\n","        valid_mask.append(batch['mask'].data.cpu().numpy())\n","        valid_num += batch_size\n","        valid_loss += batch_size * loss0.item()\n","\n","        # debug\n","        # if 0:\n","        #     organ = batch['organ'].data.cpu().numpy()\n","        #     image = batch['image']\n","        #     mask = batch['mask']\n","        #     probability = output['probability']\n","        #\n","        #     for b in range(batch_size):\n","        #         m = tensor_to_image(image[b])\n","        #         t = tensor_to_mask(mask[b, 0])\n","        #         p = tensor_to_mask(probability[b, 0])\n","        #         overlay = result_to_overlay(m, t, p)\n","        #\n","        #         text = label_to_organ[organ[b]]\n","        #         draw_shadow_text(overlay, text, (5, 15), 0.7, (1, 1, 1), 1)\n","        #\n","        #         image_show_norm('overlay', overlay, min=0, max=1, resize=1)\n","        #         cv2.waitKey(0)\n","\n","        print('\\r %8d / %d  %s' % (valid_num, len(valid_loader.dataset), (time.time() - start_timer)), end='',\n","              flush=True)\n","\n","    assert (valid_num == len(valid_loader.dataset))\n","\n","    probability = np.concatenate(valid_probability)\n","    mask = np.concatenate(valid_mask)\n","\n","    loss = valid_loss / valid_num\n","\n","    dice = compute_dice_score(probability, mask)\n","    dice = dice.mean()\n","\n","    return [dice, loss, 0, 0]"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-08-24T14:48:16.058445Z","iopub.status.busy":"2022-08-24T14:48:16.058127Z","iopub.status.idle":"2022-08-24T14:48:16.075499Z","shell.execute_reply":"2022-08-24T14:48:16.074545Z","shell.execute_reply.started":"2022-08-24T14:48:16.058412Z"},"trusted":true},"outputs":[],"source":["import os\n","\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch.utils.data import Dataset\n","\n","image_size = 512\n","\n","\n","class HapDataset(Dataset):\n","    def __init__(self, df, config, augment=None):\n","\n","        self.df = df\n","        self.augment = augment\n","        self.length = len(self.df)\n","        ids = pd.read_csv(config.LABELS).id.astype(str).values\n","        self.config = config\n","        self.fnames = [fname for fname in os.listdir(config.TRAIN) if fname in ids]\n","        self.organ_to_label = {'kidney': 0,\n","                               'prostate': 1,\n","                               'largeintestine': 2,\n","                               'spleen': 3,\n","                               'lung': 4}\n","\n","    def __str__(self):\n","        string = ''\n","        string += '\\tlen = %d\\n' % len(self)\n","\n","        d = self.df.organ.value_counts().to_dict()\n","        for k in ['kidney', 'prostate', 'largeintestine', 'spleen', 'lung']:\n","            string += '%24s %3d (%0.3f) \\n' % (k, d.get(k, 0), d.get(k, 0) / len(self.df))\n","        return string\n","\n","    def __len__(self):\n","        return self.length\n","\n","    def __getitem__(self, index):\n","        # print(self.fnames)\n","        # fname = self.fnames[index]\n","        d = self.df.iloc[index]\n","        organ = self.organ_to_label[d.organ]\n","\n","        image = cv2.cvtColor(cv2.imread(os.path.join(self.config.TRAIN, f'{d.id}.tiff')), cv2.COLOR_BGR2RGB)\n","        mask = cv2.imread(os.path.join(self.config.MASKS, f'{d.id}.png'), cv2.IMREAD_GRAYSCALE)\n","\n","        image = image.astype(np.float32) / 255\n","\n","        s = d.pixel_size / 0.4 * (image_size / 3000)\n","        image = cv2.resize(image,dsize=(image_size,image_size),interpolation=cv2.INTER_LINEAR)\n","        mask  = cv2.resize(mask, dsize=(image_size,image_size),interpolation=cv2.INTER_LINEAR)\n","        if self.augment is not None:\n","            image, mask = self.augment(image, mask, organ)\n","\n","        r = {}\n","        r['index'] = index\n","        r['id'] = d.id\n","        r['organ'] = torch.tensor([organ], dtype=torch.long)\n","        r['image'] = image_to_tensor(image)\n","        r['mask'] = mask_to_tensor(mask > 0.5)\n","        return r"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# import matplotlib.pyplot as plt\n","# df_ = pd.read_csv(\"/home/lakshita/somusan/hubmap_kaggle/hubmap_data/hubmap-organ-segmentation/train.csv\")\n","# fold = 0\n","# cfg = config(fold)\n","# train_dataset = HapDataset(df_, cfg, train_augment5b)\n","# # plt.imshow(train_dataset[0]['image'].permute(2,1,0).detach().numpy())\n","# train_loader  = DataLoader(\n","#     train_dataset,\n","#     sampler = RandomSampler(train_dataset),\n","#     batch_size  = 8,\n","#     drop_last   = True,\n","#     num_workers = 8,\n","#     pin_memory  = False,\n","#     worker_init_fn = lambda id: np.random.seed(torch.initial_seed() // 2 ** 32 + id),\n","#     collate_fn = null_collate,\n","# )"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# items = next(iter(train_loader))\n","# imgs = items[\"image\"].permute((0, 2, 3, 1))\n","# msks = items[\"mask\"].permute((0, 2, 3, 1))\n","# imgs.size(), msks.size()\n","\n","# # torch.unique(msks)\n","# def plot_batch(imgs, msks, size=3):\n","#     for idx in range(size):\n","#         plt.figure(figsize=(4*3, 5))\n","\n","#         plt.subplot(1, 3, 1); plt.imshow(imgs[idx])\n","#         plt.title('image', fontsize=15)\n","#         plt.axis('OFF')\n","\n","#         plt.subplot(1, 3, 2); plt.imshow(msks[idx])\n","#         plt.title('mask', fontsize=15)\n","#         plt.axis('OFF')\n","\n","#         plt.subplot(1, 3, 3); plt.imshow(imgs[idx]); plt.imshow(msks[idx], alpha=0.3)\n","#         plt.title('overlay', fontsize=15)\n","#         plt.axis('OFF')\n","        \n","#         plt.tight_layout()\n","#         plt.show()\n","\n","# plot_batch(imgs, msks, size=3)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-08-24T14:48:16.153570Z","iopub.status.busy":"2022-08-24T14:48:16.153164Z","iopub.status.idle":"2022-08-24T14:48:16.252097Z","shell.execute_reply":"2022-08-24T14:48:16.251078Z","shell.execute_reply.started":"2022-08-24T14:48:16.153536Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["import numpy as np\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","\n","class PatchEmbed(nn.Module):\n","    r\"\"\" Image to Patch Embedding\n","\n","    Args:\n","        patch_size (int): Patch token size. Default: 4.\n","        in_chans (int): Number of input image channels. Default: 3.\n","        embed_dim (int): Number of linear projection output channels. Default: 96.\n","        norm_layer (nn.Module, optional): Normalization layer. Default: None\n","    \"\"\"\n","\n","    def __init__(self,\n","                 patch_size=4,\n","                 in_chans=3,\n","                 embed_dim=96,\n","                 norm_layer=None\n","                 ):\n","        super().__init__()\n","        patch_size = to_2tuple(patch_size)\n","        self.patch_size = patch_size\n","\n","        self.in_chans = in_chans\n","        self.embed_dim = embed_dim\n","\n","        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n","        if norm_layer is not None:\n","            self.norm = norm_layer(embed_dim)\n","        else:\n","            self.norm = None\n","\n","    def forward(self, x):\n","        B, C, H, W = x.shape\n","\n","        # padding\n","        if W % self.patch_size[1] != 0:\n","            x = F.pad(x, (0, self.patch_size[1] - W % self.patch_size[1]))\n","        if H % self.patch_size[0] != 0:\n","            x = F.pad(x, (0, 0, 0, self.patch_size[0] - H % self.patch_size[0]))\n","\n","        x = self.proj(x)  # B C Wh Ww\n","        if self.norm is not None:\n","            Wh, Ww = x.size(2), x.size(3)\n","            x = x.flatten(2).transpose(1, 2)\n","            x = self.norm(x)\n","            x = x.transpose(1, 2).view(-1, self.embed_dim, Wh, Ww)\n","\n","        return x\n","\n","\n","class PatchMerging(nn.Module):\n","    r\"\"\" Patch Merging Layer.\n","\n","    Args:\n","        dim (int): Number of input channels.\n","        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n","    \"\"\"\n","\n","    def __init__(self, dim, norm_layer=nn.LayerNorm):\n","        super().__init__()\n","        self.dim = dim\n","        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n","        self.norm = norm_layer(4 * dim)\n","\n","    def forward(self, x, H, W):\n","        \"\"\"\n","        Args:\n","            x: Input feature, tensor size (B, H*W, C).\n","            H, W: Spatial resolution of the input feature.\n","        \"\"\"\n","\n","        B, L, C = x.shape\n","        assert L == H * W, \"input feature has wrong size\"\n","\n","        x = x.view(B, H, W, C)\n","        # padding\n","        pad_input = (H % 2 == 1) or (W % 2 == 1)\n","        if pad_input:\n","            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n","\n","        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n","        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n","        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n","        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n","        x = torch.cat([x0, x1, x2, x3], -1)  # B, H/2, W/2, 4*C\n","        x = x.view(B, -1, 4 * C)  # B, H/2*W/2, 4*C\n","\n","        x = self.norm(x)\n","        x = self.reduction(x)\n","\n","        return x\n","\n","\n","class BasicLayer(nn.Module):\n","    \"\"\" A basic Swin Transformer layer for one stage.\n","\n","    Args:\n","        dim (int): Number of input channels.\n","        depth (int): Number of blocks.\n","        num_heads (int): Number of attention heads.\n","        window_size (int): Local window size.\n","        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n","        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n","        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n","        drop (float, optional): Dropout rate. Default: 0.0\n","        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n","        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n","        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n","        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n","        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n","        fused_window_process (bool, optional): If True, use one kernel to fused window shift & window partition for acceleration, similar for the reversed part. Default: False\n","    \"\"\"\n","\n","    def __init__(self,\n","                 dim,\n","                 depth,\n","                 num_heads,\n","                 window_size,\n","                 mlp_ratio=4.,\n","                 qkv_bias=True,\n","                 qk_scale=None,\n","                 drop=0.,\n","                 attn_drop=0.,\n","                 drop_path=0.,\n","                 norm_layer=nn.LayerNorm,\n","                 downsample=None,\n","                 # use_checkpoint=False,\n","                 ):\n","        super().__init__()\n","        self.window_size = window_size\n","        self.shift_size = window_size // 2\n","        self.depth = depth\n","\n","        self.blocks = nn.ModuleList([\n","            SwinTransformerBlock(\n","                dim=dim,\n","                num_heads=num_heads,\n","                window_size=window_size,\n","                shift_size=0 if (i % 2 == 0) else window_size // 2,\n","                mlp_ratio=mlp_ratio,\n","                qkv_bias=qkv_bias,\n","                qk_scale=qk_scale,\n","                drop=drop,\n","                attn_drop=attn_drop,\n","                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n","                norm_layer=norm_layer,\n","            )\n","            for i in range(depth)\n","        ])\n","        # patch merging layer\n","        if downsample is not None:\n","            self.downsample = downsample(dim=dim, norm_layer=norm_layer)\n","        else:\n","            self.downsample = None\n","\n","    def forward(self, x, H, W):\n","        \"\"\"\n","        Args:\n","            x: Input feature, tensor size (B, H*W, C).\n","            H, W: Spatial resolution of the input feature.\n","        \"\"\"\n","\n","        # calculate attention mask for SW-MSA ----\n","        Hp = int(np.ceil(H / self.window_size)) * self.window_size\n","        Wp = int(np.ceil(W / self.window_size)) * self.window_size\n","        img_mask = torch.zeros((1, Hp, Wp, 1), device=x.device)  # 1 Hp Wp 1\n","        h_slices = (slice(0, -self.window_size),\n","                    slice(-self.window_size, -self.shift_size),\n","                    slice(-self.shift_size, None))\n","        w_slices = (slice(0, -self.window_size),\n","                    slice(-self.window_size, -self.shift_size),\n","                    slice(-self.shift_size, None))\n","        cnt = 0\n","        for h in h_slices:\n","            for w in w_slices:\n","                img_mask[:, h, w, :] = cnt\n","                cnt += 1\n","\n","        mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n","        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n","        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n","        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n","        # ------\n","\n","        for blk in self.blocks:\n","            x = blk(x, H, W, attn_mask)\n","\n","        if self.downsample is not None:\n","            x_down = self.downsample(x, H, W)\n","            Wh, Ww = (H + 1) // 2, (W + 1) // 2\n","            return x, H, W, x_down, Wh, Ww\n","        else:\n","            return x, H, W, x, H, W\n","\n","\n","class SwinTransformerBlock(nn.Module):\n","    r\"\"\" Swin Transformer Block.\n","\n","    Args:\n","        dim (int): Number of input channels.\n","        input_resolution (tuple[int]): Input resulotion.\n","        num_heads (int): Number of attention heads.\n","        window_size (int): Window size.\n","        shift_size (int): Shift size for SW-MSA.\n","        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n","        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n","        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n","        drop (float, optional): Dropout rate. Default: 0.0\n","        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n","        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n","        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n","        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n","        fused_window_process (bool, optional): If True, use one kernel to fused window shift & window partition for acceleration, similar for the reversed part. Default: False\n","    \"\"\"\n","\n","    def __init__(self,\n","                 dim,\n","                 num_heads,\n","                 window_size=7,\n","                 shift_size=0,\n","                 mlp_ratio=4.,\n","                 qkv_bias=True,\n","                 qk_scale=None,\n","                 drop=0.,\n","                 attn_drop=0.,\n","                 drop_path=0.,\n","                 act_layer=nn.GELU,\n","                 norm_layer=nn.LayerNorm,\n","                 ):\n","        super().__init__()\n","        self.dim = dim\n","        self.num_heads = num_heads\n","        self.window_size = window_size\n","        self.shift_size = shift_size\n","        self.mlp_ratio = mlp_ratio\n","        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n","\n","        self.norm1 = norm_layer(dim)\n","        self.attn = WindowAttention(\n","            dim,\n","            window_size=to_2tuple(self.window_size),\n","            num_heads=num_heads,\n","            qkv_bias=qkv_bias,\n","            qk_scale=qk_scale,\n","            attn_drop=attn_drop,\n","            proj_drop=drop,\n","        )\n","        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","\n","        self.norm2 = norm_layer(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n","\n","    def forward(self, x, H, W, mask_matrix):\n","\n","        B, L, C = x.shape\n","        assert L == H * W, \"input feature has wrong size\"\n","\n","        shortcut = x\n","        x = self.norm1(x)\n","        x = x.view(B, H, W, C)\n","\n","        # pad feature maps to multiples of window size\n","        pad_l = pad_t = 0\n","        pad_r = (self.window_size - W % self.window_size) % self.window_size\n","        pad_b = (self.window_size - H % self.window_size) % self.window_size\n","        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n","        _, Hp, Wp, _ = x.shape\n","\n","        # cyclic shift ---\n","        if self.shift_size > 0:\n","            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n","            attn_mask = mask_matrix\n","        else:\n","            shifted_x = x\n","            attn_mask = None\n","\n","        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n","        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n","        attn_windows = self.attn(x_windows, mask=attn_mask)  # nW*B, window_size*window_size, C\n","        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n","\n","        # reverse cyclic shift ---\n","        shifted_x = window_reverse(attn_windows, self.window_size, Hp, Wp)  # B H' W' C\n","        if self.shift_size > 0:\n","            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n","        else:\n","            x = shifted_x\n","\n","        if pad_r > 0 or pad_b > 0:\n","            x = x[:, :H, :W, :].contiguous()\n","        x = x.view(B, H * W, C)\n","\n","        # FFN\n","        x = shortcut + self.drop_path(x)\n","        x = x + self.drop_path(self.mlp(self.norm2(x)))\n","        return x\n","\n","    def extra_repr(self) -> str:\n","        return f\"dim={self.dim}, num_heads={self.num_heads}, \" \\\n","               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n","\n","\n","class SwinTransformerV1(nn.Module):\n","    def __init__(self,\n","                 pretrain_img_size=224,\n","                 patch_size=4,\n","                 in_chans=3,\n","                 embed_dim=96,\n","                 depths=[2, 2, 6, 2],\n","                 num_heads=[3, 6, 12, 24],\n","                 window_size=7,\n","                 mlp_ratio=4.,\n","                 qkv_bias=True,\n","                 qk_scale=None,\n","                 drop_rate=0.,\n","                 attn_drop_rate=0.,\n","                 drop_path_rate=0.1,\n","                 norm_layer=nn.LayerNorm,\n","                 patch_norm=True,\n","                 out_norm=nn.Identity,  # use nn.Identity, nn.BatchNorm2d, LayerNorm2d\n","                 **kwargs\n","                 ):\n","        super().__init__()\n","        self.pretrain_img_size = pretrain_img_size\n","        self.num_layers = len(depths)\n","        self.embed_dim = embed_dim\n","        self.mlp_ratio = mlp_ratio\n","\n","        self.patch_embed = PatchEmbed(\n","            patch_size=patch_size,\n","            in_chans=in_chans,\n","            embed_dim=embed_dim,\n","            norm_layer=norm_layer if patch_norm else None\n","        )\n","        self.pos_drop = nn.Dropout(p=drop_rate)\n","\n","        # stochastic depth\n","        dpr = np.linspace(0, drop_path_rate, sum(depths)).tolist()  # stochastic depth decay rule\n","\n","        # build layers\n","        self.layers = nn.ModuleList()\n","        for i in range(self.num_layers):\n","            layer = BasicLayer(\n","                dim=int(embed_dim * 2 ** i),\n","                depth=depths[i],\n","                num_heads=num_heads[i],\n","                window_size=window_size,\n","                mlp_ratio=self.mlp_ratio,\n","                qkv_bias=qkv_bias, qk_scale=qk_scale,\n","                drop=drop_rate,\n","                attn_drop=attn_drop_rate,\n","                drop_path=dpr[sum(depths[:i]):sum(depths[:i + 1])],\n","                norm_layer=norm_layer,\n","                downsample=PatchMerging if (i < self.num_layers - 1) else None,\n","            )\n","            self.layers.append(layer)\n","\n","        # ---\n","        # add a norm layer for each output\n","        self.out_norm = nn.ModuleList(\n","            [out_norm(int(embed_dim * 2 ** i)) for i in range(self.num_layers)]\n","        )\n","\n","        # ---\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    def forward(self, x):\n","        x = self.patch_embed(x)\n","        Wh, Ww = x.size(2), x.size(3)\n","\n","        # positional encode?\n","        x = x.flatten(2).transpose(1, 2)\n","        x = self.pos_drop(x)\n","\n","        outs = []\n","        for i in range(self.num_layers):\n","            x_out, H, W, x, Wh, Ww = self.layers[i](x, Wh, Ww)\n","            out = x_out.view(-1, H, W, int(self.embed_dim * 2 ** i)).permute(0, 3, 1, 2).contiguous()\n","            out = self.out_norm[i](out)\n","            outs.append(out)\n","\n","        return outs\n","\n","\n","class WindowAttention(nn.Module):\n","    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n","    It supports both of shifted and non-shifted window.\n","\n","    Args:\n","        dim (int): Number of input channels.\n","        window_size (tuple[int]): The height and width of the window.\n","        num_heads (int): Number of attention heads.\n","        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n","        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n","        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n","        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n","    \"\"\"\n","\n","    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n","        super().__init__()\n","        self.dim = dim\n","        self.window_size = window_size  # Wh, Ww\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        self.scale = qk_scale or head_dim ** (-0.5)\n","\n","        # define a parameter table of relative position bias\n","        self.relative_position_bias_table = nn.Parameter(\n","            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n","\n","        # get pair-wise relative position index for each token inside the window\n","        coords_h = torch.arange(self.window_size[0])\n","        coords_w = torch.arange(self.window_size[1])\n","        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n","        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n","        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n","        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n","        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n","        relative_coords[:, :, 1] += self.window_size[1] - 1\n","        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n","        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n","        self.register_buffer(\"relative_position_index\", relative_position_index)\n","\n","        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n","        self.proj = nn.Linear(dim, dim)\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","        trunc_normal_(self.relative_position_bias_table, std=.02)\n","\n","    def forward(self, x, mask=None):\n","        \"\"\"\n","        Args:\n","            x: input features with shape of (num_windows*B, N, C)\n","            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n","        \"\"\"\n","\n","        B_, N, C = x.shape\n","        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n","        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n","\n","        q = q * self.scale\n","        attn = (q @ k.transpose(-2, -1))\n","\n","        relative_position_bias = \\\n","            self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n","                self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], self.num_heads)\n","        # Wh*Ww,Wh*Ww,nH\n","        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n","\n","        attn = attn + relative_position_bias.unsqueeze(0)\n","\n","        if mask is not None:\n","            nW = mask.shape[0]\n","            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n","            attn = attn.view(-1, self.num_heads, N, N)\n","            attn = self.softmax(attn)\n","        else:\n","            attn = self.softmax(attn)\n","\n","        attn = self.attn_drop(attn)\n","        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","        return x\n","\n","    def extra_repr(self) -> str:\n","        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n","\n","\n","def window_partition(x, window_size):\n","    \"\"\"\n","    Args:\n","        x: (B, H, W, C)\n","        window_size (int): window size\n","\n","    Returns:\n","        windows: (num_windows*B, window_size, window_size, C)\n","    \"\"\"\n","    B, H, W, C = x.shape\n","    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n","    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n","    return windows\n","\n","\n","def window_reverse(windows, window_size, H, W):\n","    \"\"\"\n","    Args:\n","        windows: (num_windows*B, window_size, window_size, C)\n","        window_size (int): Window size\n","        H (int): Height of image\n","        W (int): Width of image\n","\n","    Returns:\n","        x: (B, H, W, C)\n","    \"\"\"\n","    B = int(windows.shape[0] / (H * W / window_size / window_size))\n","    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n","    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n","    return x\n","\n","\n","def conv3x3_bn_relu(in_planes, out_planes, stride=1):\n","    \"3x3 convolution + BN + relu\"\n","    return nn.Sequential(\n","        nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False),\n","        nn.BatchNorm2d(out_planes),\n","        nn.ReLU(inplace=True),\n","    )\n","\n","\n","class UPerDecoder(nn.Module):\n","    def __init__(self,\n","                 in_dim=[256, 512, 1024, 2048],\n","                 ppm_pool_scale=[1, 2, 3, 6],\n","                 ppm_dim=512,\n","                 fpn_out_dim=256\n","                 ):\n","        super(UPerDecoder, self).__init__()\n","\n","        # PPM ----\n","        dim = in_dim[-1]\n","        ppm_pooling = []\n","        ppm_conv = []\n","\n","        for scale in ppm_pool_scale:\n","            ppm_pooling.append(\n","                nn.AdaptiveAvgPool2d(scale)\n","            )\n","            ppm_conv.append(\n","                nn.Sequential(\n","                    nn.Conv2d(dim, ppm_dim, kernel_size=1, bias=False),\n","                    nn.BatchNorm2d(ppm_dim),\n","                    nn.ReLU(inplace=True)\n","                )\n","            )\n","        self.ppm_pooling = nn.ModuleList(ppm_pooling)\n","        self.ppm_conv = nn.ModuleList(ppm_conv)\n","        self.ppm_out = conv3x3_bn_relu(dim + len(ppm_pool_scale) * ppm_dim, fpn_out_dim, 1)\n","\n","        # FPN ----\n","        fpn_in = []\n","        for i in range(0, len(in_dim) - 1):  # skip the top layer\n","            fpn_in.append(\n","                nn.Sequential(\n","                    nn.Conv2d(in_dim[i], fpn_out_dim, kernel_size=1, bias=False),\n","                    nn.BatchNorm2d(fpn_out_dim),\n","                    nn.ReLU(inplace=True)\n","                )\n","            )\n","        self.fpn_in = nn.ModuleList(fpn_in)\n","\n","        fpn_out = []\n","        for i in range(len(in_dim) - 1):  # skip the top layer\n","            fpn_out.append(\n","                conv3x3_bn_relu(fpn_out_dim, fpn_out_dim, 1),\n","            )\n","        self.fpn_out = nn.ModuleList(fpn_out)\n","\n","        self.fpn_fuse = nn.Sequential(\n","            conv3x3_bn_relu(len(in_dim) * fpn_out_dim, fpn_out_dim, 1),\n","        )\n","\n","    def forward(self, feature):\n","        f = feature[-1]\n","        pool_shape = f.shape[2:]\n","\n","        ppm_out = [f]\n","        for pool, conv in zip(self.ppm_pooling, self.ppm_conv):\n","            p = pool(f)\n","            p = F.interpolate(p, size=pool_shape, mode='bilinear', align_corners=False)\n","            p = conv(p)\n","            ppm_out.append(p)\n","        ppm_out = torch.cat(ppm_out, 1)\n","        down = self.ppm_out(ppm_out)\n","\n","        fpn_out = [down]\n","        for i in reversed(range(len(feature) - 1)):\n","            lateral = feature[i]\n","            lateral = self.fpn_in[i](lateral)  # lateral branch\n","            down = F.interpolate(down, size=lateral.shape[2:], mode='bilinear', align_corners=False)  # top-down branch\n","            down = down + lateral\n","            fpn_out.append(self.fpn_out[i](down))\n","\n","        fpn_out.reverse()  # [P2 - P5]\n","        fusion_shape = fpn_out[0].shape[2:]\n","        fusion = [fpn_out[0]]\n","        for i in range(1, len(fpn_out)):\n","            fusion.append(\n","                F.interpolate(fpn_out[i], fusion_shape, mode='bilinear', align_corners=False)\n","            )\n","        x = self.fpn_fuse(torch.cat(fusion, 1))\n","\n","        return x, fusion\n","\n","\n","class LayerNorm2d(nn.Module):\n","    def __init__(self, dim, eps=1e-6):\n","        super().__init__()\n","        self.weight = nn.Parameter(torch.ones(dim))\n","        self.bias = nn.Parameter(torch.zeros(dim))\n","        self.eps = eps\n","\n","    def forward(self, x):\n","        u = x.mean(1, keepdim=True)\n","        s = (x - u).pow(2).mean(1, keepdim=True)\n","        x = (x - u) / torch.sqrt(s + self.eps)\n","        x = self.weight[:, None, None] * x + self.bias[:, None, None]\n","        return x\n","\n","\n","def criterion_aux_loss(logit, mask):\n","    mask = F.interpolate(mask, size=logit.shape[-2:], mode='nearest')\n","    loss = F.binary_cross_entropy_with_logits(logit, mask)\n","    return loss\n","\n","\n","class Net(nn.Module):\n","\n","    def load_pretrain(self):\n","\n","        checkpoint = self.config.checkpoint[self.arch]['checkpoint']\n","        print('loading %s ...' % checkpoint)\n","        checkpoint = torch.load(checkpoint, map_location=lambda storage, loc: storage)['model']\n","        # if 0:\n","        #     skip = ['relative_coords_table','relative_position_index']\n","        #     filtered={}\n","        #     for k,v in checkpoint.items():\n","        #         if any([s in k for s in skip ]): continue\n","        #         filtered[k]=v\n","        #     checkpoint = filtered\n","        print(self.encoder.load_state_dict(checkpoint, strict=False))  # True\n","\n","    def __init__(self, config):\n","        super(Net, self).__init__()\n","        self.config = config\n","        self.output_type = ['inference', 'loss']\n","\n","        self.rgb = RGB()\n","        self.arch = 'swin_tiny_patch4_window7_224'\n","\n","        self.encoder = SwinTransformerV1(\n","            **{**(config.checkpoint['basic']['swin']), **(config.checkpoint[self.arch]['swin']),\n","               **{'out_norm': LayerNorm2d}}\n","        )\n","        encoder_dim = config.checkpoint[self.arch]['upernet']['in_channels']\n","        \n","\n","        self.decoder = UPerDecoder(\n","            in_dim=encoder_dim,\n","            ppm_pool_scale=[1, 2, 3, 6],\n","            ppm_dim=512,        \n","            fpn_out_dim=256\n","        )\n","\n","        self.logit = nn.Sequential(\n","            nn.Conv2d(256, 1, kernel_size=1)\n","        )\n","        self.aux = nn.ModuleList([\n","            nn.Conv2d(256, 1, kernel_size=1, padding=0) for i in range(4)\n","        ])\n","\n","    def forward(self, batch):\n","        x = batch['image']\n","        B, C, H, W = x.shape\n","        x = self.rgb(x)\n","        encoder = self.encoder(x)\n","        last, decoder = self.decoder(encoder)\n","        logit = self.logit(last)\n","        logit = F.interpolate(logit, size=None, scale_factor=4, mode='bilinear', align_corners=False)\n","\n","        output = {}\n","        if 'loss' in self.output_type:\n","            output['bce_loss'] = F.binary_cross_entropy_with_logits(logit, batch['mask'])\n","            for i in range(4):\n","                output['aux%d_loss' % i] = criterion_aux_loss(self.aux[i](decoder[i]), batch['mask'])\n","\n","        if 'inference' in self.output_type:\n","            output['probability'] = torch.sigmoid(logit)\n","\n","        return output\n","\n","\n","def run_check_net(config):\n","    batch_size = 2\n","    image_size = 512\n","\n","    # ---\n","    batch = {\n","        'image': torch.from_numpy(np.random.uniform(-1, 1, (batch_size, 3, image_size, image_size))).float(),\n","        'mask': torch.from_numpy(np.random.choice(2, (batch_size, 1, image_size, image_size))).float(),\n","        'organ': torch.from_numpy(np.random.choice(5, (batch_size))).long(),\n","    }\n","    batch = {k: v.cuda() for k, v in batch.items()}\n","\n","    net = Net(config).cuda()\n","    net.load_pretrain()\n","\n","    with torch.no_grad():\n","        with torch.cuda.amp.autocast(enabled=True):\n","            output = net(batch)\n","\n","    print('batch')\n","    for k, v in batch.items():\n","        print('%32s :' % k, v.shape)\n","\n","    print('output')\n","    for k, v in output.items():\n","        if 'loss' not in k:\n","            print('%32s :' % k, v.shape)\n","    for k, v in output.items():\n","        if 'loss' in k:\n","            print('%32s :' % k, v.item())\n","\n","\n","class Mlp(nn.Module):\n","    \"\"\" Multilayer perceptron.\"\"\"\n","\n","    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n","        super().__init__()\n","        out_features = out_features or in_features\n","        hidden_features = hidden_features or in_features\n","        self.fc1 = nn.Linear(in_features, hidden_features)\n","        self.act = act_layer()\n","        self.fc2 = nn.Linear(hidden_features, out_features)\n","        self.drop = nn.Dropout(drop)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.act(x)\n","        x = self.drop(x)\n","        x = self.fc2(x)\n","        x = self.drop(x)\n","        return x\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-08-24T14:48:16.255238Z","iopub.status.busy":"2022-08-24T14:48:16.254509Z","iopub.status.idle":"2022-08-24T14:48:16.264675Z","shell.execute_reply":"2022-08-24T14:48:16.263562Z","shell.execute_reply.started":"2022-08-24T14:48:16.255183Z"},"trusted":true},"outputs":[],"source":["def freeze_bn(net):\n","    for m in net.modules():\n","        if isinstance(m, nn.BatchNorm2d):\n","            m.eval()\n","            m.weight.requires_grad = False\n","            m.bias.requires_grad = False\n","\n","\n","def get_learning_rate(optimizer):\n","    return optimizer.param_groups[0]['lr']"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-08-24T14:48:16.267513Z","iopub.status.busy":"2022-08-24T14:48:16.266610Z","iopub.status.idle":"2022-08-24T14:48:16.292244Z","shell.execute_reply":"2022-08-24T14:48:16.291135Z","shell.execute_reply.started":"2022-08-24T14:48:16.267473Z"},"trusted":true},"outputs":[],"source":["def main(config):\n","\n","    print('\\n--- [START %s] %s\\n\\n' % ('Swin', '-' * 64))\n","    print('\\n')\n","\n","    ## dataset ----------------------------------------\n","    print('** dataset setting **\\n')\n","\n","    train_df, valid_df = make_fold(config, config.fold)\n","\n","    train_dataset = HapDataset(train_df, config, train_augment5b)\n","    valid_dataset = HapDataset(valid_df, config, valid_augment5)\n","\n","    train_loader = DataLoader(\n","        train_dataset,\n","        sampler=RandomSampler(train_dataset),\n","        batch_size=config.batch_size,\n","        drop_last=True,\n","        num_workers=8,\n","        pin_memory=False,\n","        worker_init_fn=lambda id: np.random.seed(torch.initial_seed() // 2 ** 32 + id),\n","        collate_fn=null_collate,\n","    )\n","\n","    valid_loader = DataLoader(\n","        valid_dataset,\n","        sampler=SequentialSampler(valid_dataset),\n","        batch_size=8,\n","        drop_last=False,\n","        num_workers=4,\n","        pin_memory=False,\n","        collate_fn=null_collate,\n","    )\n","    \n","    \n","#     return train_loader, valid_loader\n","\n","    \n","    \n","    print('fold = %s\\n' % str(config.fold))\n","    print('train_dataset : \\n%s\\n' % (train_dataset))\n","    print('valid_dataset : \\n%s\\n' % (valid_dataset))\n","    print('\\n')\n","\n","    ## net ----------------------------------------\n","    print('** net setting **\\n')\n","\n","    scaler = amp.GradScaler(enabled=config.is_amp)\n","    net = Net(config).cuda()\n","\n","    if config.initial_checkpoint is not None:\n","        f = torch.load(config.initial_checkpoint, map_location=lambda storage, loc: storage)\n","        start_iteration = f['iteration']\n","        start_epoch = f['epoch']\n","        state_dict = f['state_dict']\n","        net.load_state_dict(state_dict, strict=False)  # True\n","    else:\n","        start_iteration = 0\n","        start_epoch = 0\n","        net.load_pretrain()\n","\n","    print('\\tinitial_checkpoint = %s\\n' % config.initial_checkpoint)\n","    print('\\n')\n","\n","    ## optimiser ----------------------------------\n","    # if 0:  ##freeze\n","    #     for p in net.stem.parameters():   p.requires_grad = False\n","    #     pass\n","\n","    # freeze_bn(net)\n","\n","    # -----------------------------------------------\n","\n","    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, net.parameters()), lr=config.start_lr)\n","\n","    print('optimizer\\n  %s\\n' % (optimizer))\n","    print('\\n')\n","\n","\n","    num_iteration = 20000 #* len(train_loader)\n","    print(f'\\nIteration Num: {num_iteration}')\n","    iter_log = len(train_loader) * 3  # 479\n","    iter_valid = iter_log\n","    print(f'\\nIteration for Valid: {iter_valid}')\n","    iter_save = iter_log\n","\n","    print('** start training here! **\\n')\n","    print('   batch_size = %d \\n' % (config.batch_size))\n","    print('                     |-------------- VALID---------|---- TRAIN/BATCH ----------------\\n')\n","    print('rate     iter  epoch | dice   loss   tp     tn     | loss           | time           \\n')\n","    print('-------------------------------------------------------------------------------------\\n')\n","\n","    valid_loss = np.zeros(4, np.float32)\n","    train_loss = np.zeros(2, np.float32)\n","    batch_loss = np.zeros_like(train_loss)\n","    sum_train_loss = np.zeros_like(train_loss)\n","    sum_train = 0\n","\n","    start_timer = time.time()\n","    iteration = start_iteration\n","    epoch = start_epoch\n","    rate = 0\n","    best_metric = 0\n","\n","    while iteration < num_iteration:\n","        for t, batch in enumerate(train_loader):\n","\n","            if iteration % iter_save == 0:\n","                if iteration != start_iteration and valid_loss[0] > best_metric:\n","                    best_metric = valid_loss[0]\n","                    # os.system(\"rm \" + config.out_dir + f'/fold_{config.fold}_*.pth')\n","                    torch.save({\n","                        'state_dict': net.state_dict(),\n","                        'iteration': iteration,\n","                        'epoch': epoch,\n","                    }, config.out_dir + '/checkpoint/%08d.model.pth' %  (iteration))\n","\n","            if iteration % iter_valid == 0:\n","                print(\"\\nFor validation\")\n","                valid_loss = validate(net, valid_loader, config)\n","\n","            if (iteration % iter_log == 0) or (iteration % iter_valid == 0):\n","                print('\\r', end='', flush=True)\n","                print(message(batch_loss, train_loss, iteration, iter_save, rate, epoch, valid_loss,\n","                              start_timer, mode='log') + '\\n')\n","\n","            # learning rate schduler ------------\n","            rate = get_learning_rate(optimizer)\n","\n","            # one iteration update  -------------\n","            batch_size = len(batch['index'])\n","            batch['image'] = batch['image'].half().cuda()\n","            batch['mask'] = batch['mask'].half().cuda()\n","            batch['organ'] = batch['organ'].cuda()\n","\n","            net.train()\n","            net.output_type = ['loss']\n","            if 1:\n","                with amp.autocast(enabled=config.is_amp):\n","                    output = net(batch)\n","                    loss0 = output['bce_loss'].mean()\n","                    loss1 = output['aux2_loss'].mean()\n","\n","                optimizer.zero_grad()\n","                scaler.scale(loss0 + 0.2 * loss1).backward()\n","\n","                scaler.unscale_(optimizer)\n","                scaler.step(optimizer)\n","                scaler.update()\n","\n","            # print statistics  --------\n","            batch_loss[:2] = [loss0.item(), loss1.item()]\n","            sum_train_loss += batch_loss\n","            sum_train += 1\n","            if t % 100 == 0:\n","                train_loss = sum_train_loss / (sum_train + 1e-12)\n","                sum_train_loss[...] = 0\n","                sum_train = 0\n","\n","            print('\\r', end='', flush=True)\n","            print(message(batch_loss, train_loss, iteration, iter_save, rate, epoch, valid_loss,\n","                          start_timer, mode='print'), end='', flush=True)\n","            epoch += 1 / len(train_loader)\n","            iteration += 1\n","\n","        torch.cuda.empty_cache()\n","\n","    print('\\n')"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-08-24T14:48:16.297325Z","iopub.status.busy":"2022-08-24T14:48:16.295485Z","iopub.status.idle":"2022-08-24T14:48:17.282669Z","shell.execute_reply":"2022-08-24T14:48:17.281402Z","shell.execute_reply.started":"2022-08-24T14:48:16.297296Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["mkdir: cannot create directory ‘models’: File exists\n","mkdir: cannot create directory ‘./models/fold_0/checkpoint’: File exists\n"]}],"source":["! mkdir models\n","!mkdir ./models/fold_0/checkpoint"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# cfg = config(0)\n","# train_df, valid_df = make_fold(cfg, cfg.fold)\n","# len(valid_df), len(train_df)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-08-24T14:48:17.287430Z","iopub.status.busy":"2022-08-24T14:48:17.285874Z","iopub.status.idle":"2022-08-24T14:48:58.595791Z","shell.execute_reply":"2022-08-24T14:48:58.594305Z","shell.execute_reply.started":"2022-08-24T14:48:17.287392Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","--- [START Swin] ----------------------------------------------------------------\n","\n","\n","\n","\n","** dataset setting **\n","\n","fold = 0\n","\n","train_dataset : \n","\tlen = 280\n","                  kidney  81 (0.289) \n","                prostate  72 (0.257) \n","          largeintestine  47 (0.168) \n","                  spleen  46 (0.164) \n","                    lung  34 (0.121) \n","\n","\n","valid_dataset : \n","\tlen = 71\n","                  kidney  18 (0.254) \n","                prostate  21 (0.296) \n","          largeintestine  11 (0.155) \n","                  spleen   7 (0.099) \n","                    lung  14 (0.197) \n","\n","\n","\n","\n","** net setting **\n","\n"]},{"name":"stderr","output_type":"stream","text":["/home/lakshita/somusan/hubmap_kaggle/.venv/lib/python3.7/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2895.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"]},{"name":"stdout","output_type":"stream","text":["loading /home/lakshita/somusan/hubmap_kaggle/nbs/swin-tiny-small-22k-pretrained//swin_tiny_patch4_window7_224_22k.pth ...\n","_IncompatibleKeys(missing_keys=['out_norm.0.weight', 'out_norm.0.bias', 'out_norm.1.weight', 'out_norm.1.bias', 'out_norm.2.weight', 'out_norm.2.bias', 'out_norm.3.weight', 'out_norm.3.bias'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias', 'layers.0.blocks.1.attn_mask', 'layers.1.blocks.1.attn_mask', 'layers.2.blocks.1.attn_mask', 'layers.2.blocks.3.attn_mask', 'layers.2.blocks.5.attn_mask'])\n","\tinitial_checkpoint = None\n","\n","\n","\n","optimizer\n","  AdamW (\n","Parameter Group 0\n","    amsgrad: False\n","    betas: (0.9, 0.999)\n","    capturable: False\n","    eps: 1e-08\n","    foreach: None\n","    lr: 5e-05\n","    maximize: False\n","    weight_decay: 0.01\n",")\n","\n","\n","\n","\n","Iteration Num: 20000\n","\n","Iteration for Valid: 105\n","** start training here! **\n","\n","   batch_size = 8 \n","\n","                     |-------------- VALID---------|---- TRAIN/BATCH ----------------\n","\n","rate     iter  epoch | dice   loss   tp     tn     | loss           | time           \n","\n","-------------------------------------------------------------------------------------\n","\n","\n","For validation\n","0.00e+0   00000000*   0.00 | 0.147  0.699  0.0000  0.000   | 0.000  0.000   | 9.431511163711548\n","\n","5.00e-5   00000104    2.97 | 0.147  0.699  0.0000  0.000   | 0.362  0.420   | 51.981033086776735\n","For validation\n","5.00e-5   00000105*   3.00 | 0.482  0.380  0.0000  0.000   | 0.493  0.528   | 60.96158242225647\n","\n","5.00e-5   00000209    5.97 | 0.482  0.380  0.0000  0.000   | 0.225  0.277   | 103.86877512931824\n","For validation\n","5.00e-5   00000210*   6.00 | 0.552  0.368  0.0000  0.000   | 0.310  0.342   | 112.80991291999817\n","\n","5.00e-5   00000314    8.97 | 0.552  0.368  0.0000  0.000   | 0.176  0.191   | 155.07926201820374\n","For validation\n","5.00e-5   00000315*   9.00 | 0.580  0.245  0.0000  0.000   | 0.244  0.262   | 164.23511958122253\n","\n","5.00e-5   00000419   11.97 | 0.580  0.245  0.0000  0.000   | 0.224  0.217   | 206.39837741851807\n","For validation\n","5.00e-5   00000420*  12.00 | 0.588  0.195  0.0000  0.000   | 0.208  0.229   | 215.6450412273407\n","\n","5.00e-5   00000524   14.97 | 0.588  0.195  0.0000  0.000   | 0.149  0.158   | 258.09666967391977\n","For validation\n","5.00e-5   00000525*  15.00 | 0.593  0.162  0.0000  0.000   | 0.188  0.201   | 267.239679813385\n","\n","5.00e-5   00000629   17.97 | 0.593  0.162  0.0000  0.000   | 0.163  0.187   | 309.49456429481506\n","For validation\n","5.00e-5   00000630*  18.00 | 0.595  0.167  0.0000  0.000   | 0.165  0.177   | 318.39426589012146\n","\n","5.00e-5   00000734   20.97 | 0.595  0.167  0.0000  0.000   | 0.189  0.192   | 360.70864653587345\n","For validation\n","5.00e-5   00000735*  21.00 | 0.589  0.137  0.0000  0.000   | 0.160  0.170   | 369.74687027931213\n","\n","5.00e-5   00000839   23.97 | 0.589  0.137  0.0000  0.000   | 0.174  0.178   | 412.09531974792487\n","For validation\n","5.00e-5   00000840*  24.00 | 0.617  0.139  0.0000  0.000   | 0.154  0.164   | 423.117794752121\n","\n","5.00e-5   00000944   26.97 | 0.617  0.139  0.0000  0.000   | 0.176  0.182   | 465.67188382148745\n","For validation\n","5.00e-5   00000945*  27.00 | 0.632  0.150  0.0000  0.000   | 0.144  0.153   | 474.9569237232208\n","\n","5.00e-5   00001049   29.97 | 0.632  0.150  0.0000  0.000   | 0.161  0.171   | 517.75289368629465\n","For validation\n","5.00e-5   00001050*  30.00 | 0.628  0.171  0.0000  0.000   | 0.143  0.152   | 526.8156561851501\n","\n","5.00e-5   00001154   32.97 | 0.628  0.171  0.0000  0.000   | 0.110  0.107   | 569.1876761913352\n","For validation\n","5.00e-5   00001155*  33.00 | 0.617  0.134  0.0000  0.000   | 0.135  0.142   | 578.2173371315002\n","\n","5.00e-5   00001259   35.97 | 0.617  0.134  0.0000  0.000   | 0.141  0.140   | 620.7527542114258\n","For validation\n","5.00e-5   00001260*  36.00 | 0.628  0.147  0.0000  0.000   | 0.122  0.132   | 629.7492549419403\n","\n","5.00e-5   00001364   38.97 | 0.628  0.147  0.0000  0.000   | 0.123  0.122   | 671.9507832527161\n","For validation\n","5.00e-5   00001365*  39.00 | 0.638  0.120  0.0000  0.000   | 0.118  0.129   | 680.9478030204773\n","\n","5.00e-5   00001469   41.97 | 0.638  0.120  0.0000  0.000   | 0.079  0.082   | 723.6062207221985\n","For validation\n","5.00e-5   00001470*  42.00 | 0.640  0.126  0.0000  0.000   | 0.118  0.127   | 732.8055012226105\n","\n","5.00e-5   00001574   44.97 | 0.640  0.126  0.0000  0.000   | 0.102  0.119   | 775.3954687118531\n","For validation\n","5.00e-5   00001575*  45.00 | 0.636  0.117  0.0000  0.000   | 0.118  0.128   | 784.5296723842621\n","\n","5.00e-5   00001679   47.97 | 0.636  0.117  0.0000  0.000   | 0.099  0.106   | 827.0703933238983\n","For validation\n","5.00e-5   00001680*  48.00 | 0.643  0.133  0.0000  0.000   | 0.124  0.133   | 836.1219291687012\n","\n","5.00e-5   00001784   50.97 | 0.643  0.133  0.0000  0.000   | 0.144  0.148   | 878.3738315105438\n","For validation\n","5.00e-5   00001785*  51.00 | 0.645  0.112  0.0000  0.000   | 0.114  0.123   | 887.3252873420715\n","\n","5.00e-5   00001889   53.97 | 0.645  0.112  0.0000  0.000   | 0.095  0.107   | 929.6030943393707\n","For validation\n","5.00e-5   00001890*  54.00 | 0.646  0.114  0.0000  0.000   | 0.113  0.124   | 938.6210472583771\n","\n","5.00e-5   00001994   56.97 | 0.646  0.114  0.0000  0.000   | 0.090  0.104   | 981.1426398754129\n","For validation\n","5.00e-5   00001995*  57.00 | 0.655  0.117  0.0000  0.000   | 0.106  0.117   | 990.2665252685547\n","\n","5.00e-5   00002099   59.97 | 0.655  0.117  0.0000  0.000   | 0.098  0.113   | 1032.8778245449066\n","For validation\n","5.00e-5   00002100*  60.00 | 0.658  0.101  0.0000  0.000   | 0.101  0.112   | 1041.8408250808716\n","\n","5.00e-5   00002204   62.97 | 0.658  0.101  0.0000  0.000   | 0.076  0.094   | 1087.6709504127502\n","For validation\n","5.00e-5   00002205*  63.00 | 0.652  0.106  0.0000  0.000   | 0.110  0.122   | 1096.7328696250916\n","\n","5.00e-5   00002309   65.97 | 0.652  0.106  0.0000  0.000   | 0.089  0.108   | 1139.4494547843933\n","For validation\n","5.00e-5   00002310*  66.00 | 0.663  0.109  0.0000  0.000   | 0.096  0.109   | 1148.309963464737\n","\n","5.00e-5   00002414   68.97 | 0.663  0.109  0.0000  0.000   | 0.069  0.084   | 1191.6804988384247\n","For validation\n","5.00e-5   00002415*  69.00 | 0.663  0.109  0.0000  0.000   | 0.095  0.107   | 1200.6833057403564\n","\n","5.00e-5   00002519   71.97 | 0.663  0.109  0.0000  0.000   | 0.067  0.071   | 1242.9364969730377\n","For validation\n","5.00e-5   00002520*  72.00 | 0.669  0.103  0.0000  0.000   | 0.087  0.100   | 1251.916831254959\n","\n","5.00e-5   00002624   74.97 | 0.669  0.103  0.0000  0.000   | 0.133  0.160   | 1294.3108365535736\n","For validation\n","5.00e-5   00002625*  75.00 | 0.657  0.108  0.0000  0.000   | 0.101  0.113   | 1303.2951662540436\n","\n","5.00e-5   00002729   77.97 | 0.657  0.108  0.0000  0.000   | 0.122  0.125   | 1345.6107153892517\n","For validation\n","5.00e-5   00002730*  78.00 | 0.651  0.107  0.0000  0.000   | 0.090  0.102   | 1354.5460178852081\n","\n","5.00e-5   00002834   80.97 | 0.651  0.107  0.0000  0.000   | 0.093  0.109   | 1396.8690762519836\n","For validation\n","5.00e-5   00002835*  81.00 | 0.659  0.104  0.0000  0.000   | 0.093  0.105   | 1405.8815834522247\n","\n","5.00e-5   00002939   83.97 | 0.659  0.104  0.0000  0.000   | 0.067  0.091   | 1448.6149919033056\n","For validation\n","5.00e-5   00002940*  84.00 | 0.655  0.098  0.0000  0.000   | 0.097  0.109   | 1457.5963802337646\n","\n","5.00e-5   00003044   86.97 | 0.655  0.098  0.0000  0.000   | 0.123  0.134   | 1500.8722770214087\n","For validation\n","5.00e-5   00003045*  87.00 | 0.660  0.116  0.0000  0.000   | 0.094  0.107   | 1509.905732870102\n","\n","5.00e-5   00003149   89.97 | 0.660  0.116  0.0000  0.000   | 0.049  0.052   | 1552.2587251663208\n","For validation\n","5.00e-5   00003150*  90.00 | 0.666  0.113  0.0000  0.000   | 0.085  0.099   | 1561.1830751895905\n","\n","5.00e-5   00003254   92.97 | 0.666  0.113  0.0000  0.000   | 0.084  0.101   | 1603.4896664619446\n","For validation\n","5.00e-5   00003255*  93.00 | 0.662  0.107  0.0000  0.000   | 0.082  0.095   | 1612.4185705184937\n","\n","5.00e-5   00003359   95.97 | 0.662  0.107  0.0000  0.000   | 0.092  0.106   | 1655.0202682018286\n","For validation\n","5.00e-5   00003360*  96.00 | 0.664  0.099  0.0000  0.000   | 0.084  0.097   | 1663.864467382431\n","\n","5.00e-5   00003464   98.97 | 0.664  0.099  0.0000  0.000   | 0.053  0.069   | 1706.0510518550873\n","For validation\n","5.00e-5   00003465*  99.00 | 0.661  0.095  0.0000  0.000   | 0.078  0.092   | 1715.0500495433807\n","\n","5.00e-5   00003569  101.97 | 0.661  0.095  0.0000  0.000   | 0.088  0.095   | 1757.3814072608948\n","For validation\n","5.00e-5   00003570* 102.00 | 0.659  0.099  0.0000  0.000   | 0.084  0.098   | 1766.37339925766\n","\n","5.00e-5   00003674  104.97 | 0.659  0.099  0.0000  0.000   | 0.117  0.117   | 1808.7147505283356\n","For validation\n","5.00e-5   00003675* 105.00 | 0.671  0.099  0.0000  0.000   | 0.078  0.091   | 1817.7122943401337\n","\n","5.00e-5   00003779  107.97 | 0.671  0.099  0.0000  0.000   | 0.088  0.111   | 1859.9319076538086\n","For validation\n","5.00e-5   00003780* 108.00 | 0.673  0.101  0.0000  0.000   | 0.080  0.093   | 1869.0211443901062\n","\n","5.00e-5   00003884  110.97 | 0.673  0.101  0.0000  0.000   | 0.084  0.104   | 1911.3500621318817\n","For validation\n","5.00e-5   00003885* 111.00 | 0.670  0.097  0.0000  0.000   | 0.080  0.095   | 1920.3121485710144\n","\n","5.00e-5   00003989  113.97 | 0.670  0.097  0.0000  0.000   | 0.113  0.107   | 1962.6039688587189\n","For validation\n","5.00e-5   00003990* 114.00 | 0.656  0.112  0.0000  0.000   | 0.082  0.095   | 1971.6488888263702\n","\n","5.00e-5   00004094  116.97 | 0.656  0.112  0.0000  0.000   | 0.141  0.178   | 2013.9840564727783\n","For validation\n","5.00e-5   00004095* 117.00 | 0.675  0.101  0.0000  0.000   | 0.079  0.092   | 2023.0055782794952\n","\n","5.00e-5   00004199  119.97 | 0.675  0.101  0.0000  0.000   | 0.036  0.044   | 2065.2042765617378\n","For validation\n","5.00e-5   00004200* 120.00 | 0.677  0.099  0.0000  0.000   | 0.074  0.089   | 2074.261469602585\n","\n","5.00e-5   00004304  122.97 | 0.677  0.099  0.0000  0.000   | 0.070  0.086   | 2116.6158294677734\n","For validation\n","5.00e-5   00004305* 123.00 | 0.672  0.098  0.0000  0.000   | 0.088  0.101   | 2125.6278698444366\n","\n","5.00e-5   00004409  125.97 | 0.672  0.098  0.0000  0.000   | 0.059  0.074   | 2167.8125281333923\n","For validation\n","5.00e-5   00004410* 126.00 | 0.670  0.105  0.0000  0.000   | 0.086  0.099   | 2176.7674527168274\n","\n","5.00e-5   00004514  128.97 | 0.670  0.105  0.0000  0.000   | 0.086  0.108   | 2218.7255139350895\n","For validation\n","5.00e-5   00004515* 129.00 | 0.666  0.106  0.0000  0.000   | 0.076  0.091   | 2227.681347131729\n","\n","5.00e-5   00004619  131.97 | 0.666  0.106  0.0000  0.000   | 0.048  0.059   | 2269.8975617885596\n","For validation\n","5.00e-5   00004620* 132.00 | 0.675  0.103  0.0000  0.000   | 0.073  0.086   | 2278.8883004188538\n","\n","5.00e-5   00004724  134.97 | 0.675  0.103  0.0000  0.000   | 0.057  0.080   | 2321.2610599994664\n","For validation\n","5.00e-5   00004725* 135.00 | 0.674  0.100  0.0000  0.000   | 0.079  0.091   | 2330.2476077079773\n","\n","5.00e-5   00004829  137.97 | 0.674  0.100  0.0000  0.000   | 0.147  0.214   | 2372.5152368545538\n","For validation\n","5.00e-5   00004830* 138.00 | 0.672  0.113  0.0000  0.000   | 0.073  0.087   | 2381.5107679367065\n","\n","5.00e-5   00004934  140.97 | 0.672  0.113  0.0000  0.000   | 0.048  0.055   | 2423.6410336494446\n","For validation\n","5.00e-5   00004935* 141.00 | 0.676  0.104  0.0000  0.000   | 0.071  0.086   | 2432.618925333023\n","\n","5.00e-5   00005039  143.97 | 0.676  0.104  0.0000  0.000   | 0.085  0.101   | 2474.7542741298676\n","For validation\n","5.00e-5   00005040* 144.00 | 0.669  0.104  0.0000  0.000   | 0.077  0.091   | 2483.69633936882\n","\n","5.00e-5   00005144  146.97 | 0.669  0.104  0.0000  0.000   | 0.096  0.098   | 2526.0950894355774\n","For validation\n","5.00e-5   00005145* 147.00 | 0.679  0.094  0.0000  0.000   | 0.071  0.085   | 2535.140294790268\n","\n","5.00e-5   00005249  149.97 | 0.679  0.094  0.0000  0.000   | 0.122  0.154   | 2577.3172848224647\n","For validation\n","5.00e-5   00005250* 150.00 | 0.681  0.095  0.0000  0.000   | 0.074  0.088   | 2586.330734014511\n","\n","5.00e-5   00005354  152.97 | 0.681  0.095  0.0000  0.000   | 0.063  0.077   | 2628.6910967826843\n","For validation\n","5.00e-5   00005355* 153.00 | 0.674  0.101  0.0000  0.000   | 0.068  0.082   | 2637.730928659439\n","\n","5.00e-5   00005459  155.97 | 0.674  0.101  0.0000  0.000   | 0.064  0.077   | 2679.7923359870916\n","For validation\n","5.00e-5   00005460* 156.00 | 0.678  0.106  0.0000  0.000   | 0.070  0.083   | 2688.764034509659\n","\n","5.00e-5   00005564  158.97 | 0.678  0.106  0.0000  0.000   | 0.042  0.052   | 2730.9685423374176\n","For validation\n","5.00e-5   00005565* 159.00 | 0.689  0.103  0.0000  0.000   | 0.069  0.083   | 2739.903464317322\n","\n","5.00e-5   00005669  161.97 | 0.689  0.103  0.0000  0.000   | 0.138  0.148   | 2782.5793955326087\n","For validation\n","5.00e-5   00005670* 162.00 | 0.679  0.100  0.0000  0.000   | 0.070  0.083   | 2791.6367197036743\n","\n","5.00e-5   00005774  164.97 | 0.679  0.100  0.0000  0.000   | 0.104  0.129   | 2834.2648270130157\n","For validation\n","5.00e-5   00005775* 165.00 | 0.676  0.100  0.0000  0.000   | 0.069  0.083   | 2843.173785686493\n","\n","5.00e-5   00005879  167.97 | 0.676  0.100  0.0000  0.000   | 0.055  0.064   | 2885.3764355182648\n","For validation\n","5.00e-5   00005880* 168.00 | 0.677  0.105  0.0000  0.000   | 0.065  0.078   | 2894.2243373394012\n","\n","5.00e-5   00005984  170.97 | 0.677  0.105  0.0000  0.000   | 0.088  0.101   | 2936.7577373981476\n","For validation\n","5.00e-5   00005985* 171.00 | 0.679  0.090  0.0000  0.000   | 0.067  0.082   | 2945.7043783664703\n","\n","5.00e-5   00006089  173.97 | 0.679  0.090  0.0000  0.000   | 0.123  0.112   | 2987.9375956058564\n","For validation\n","5.00e-5   00006090* 174.00 | 0.679  0.104  0.0000  0.000   | 0.067  0.081   | 2996.8786675930023\n","\n","5.00e-5   00006194  176.97 | 0.679  0.104  0.0000  0.000   | 0.051  0.058   | 3039.1655557155612\n","For validation\n","5.00e-5   00006195* 177.00 | 0.681  0.102  0.0000  0.000   | 0.069  0.081   | 3048.1967556476593\n","\n","5.00e-5   00006299  179.97 | 0.681  0.102  0.0000  0.000   | 0.040  0.048   | 3090.4049134254456\n","For validation\n","5.00e-5   00006300* 180.00 | 0.680  0.096  0.0000  0.000   | 0.066  0.081   | 3099.2344172000885\n","\n","5.00e-5   00006404  182.97 | 0.680  0.096  0.0000  0.000   | 0.079  0.104   | 3141.4964466094977\n","For validation\n","5.00e-5   00006405* 183.00 | 0.680  0.110  0.0000  0.000   | 0.065  0.080   | 3150.3923778533936\n","\n","5.00e-5   00006509  185.97 | 0.680  0.110  0.0000  0.000   | 0.059  0.069   | 3192.9552071094513\n","For validation\n","5.00e-5   00006510* 186.00 | 0.667  0.108  0.0000  0.000   | 0.070  0.083   | 3201.946875810623\n","\n","5.00e-5   00006614  188.97 | 0.667  0.108  0.0000  0.000   | 0.097  0.129   | 3244.2131071090718\n","For validation\n","5.00e-5   00006615* 189.00 | 0.681  0.103  0.0000  0.000   | 0.067  0.080   | 3253.2453413009644\n","\n","5.00e-5   00006719  191.97 | 0.681  0.103  0.0000  0.000   | 0.050  0.057   | 3295.4432225227356\n","For validation\n","5.00e-5   00006720* 192.00 | 0.681  0.106  0.0000  0.000   | 0.069  0.084   | 3304.344494819641\n","\n","5.00e-5   00006824  194.97 | 0.681  0.106  0.0000  0.000   | 0.070  0.078   | 3346.7185995578766\n","For validation\n","5.00e-5   00006825* 195.00 | 0.679  0.103  0.0000  0.000   | 0.066  0.079   | 3355.661374807358\n","\n","5.00e-5   00006929  197.97 | 0.679  0.103  0.0000  0.000   | 0.061  0.094   | 3398.4302656650543\n","For validation\n","5.00e-5   00006930* 198.00 | 0.677  0.097  0.0000  0.000   | 0.065  0.079   | 3407.3784539699554\n","\n","5.00e-5   00007034  200.97 | 0.677  0.097  0.0000  0.000   | 0.053  0.082   | 3449.5339980125427\n","For validation\n","5.00e-5   00007035* 201.00 | 0.675  0.100  0.0000  0.000   | 0.065  0.079   | 3458.4959950447083\n","\n","5.00e-5   00007139  203.97 | 0.675  0.100  0.0000  0.000   | 0.101  0.108   | 3500.7564592361453\n","For validation\n","5.00e-5   00007140* 204.00 | 0.680  0.102  0.0000  0.000   | 0.061  0.074   | 3509.7974812984467\n","\n","5.00e-5   00007244  206.97 | 0.680  0.102  0.0000  0.000   | 0.076  0.100   | 3552.2768723964695\n","For validation\n","5.00e-5   00007245* 207.00 | 0.675  0.096  0.0000  0.000   | 0.072  0.086   | 3561.218354701996\n","\n","5.00e-5   00007349  209.97 | 0.675  0.096  0.0000  0.000   | 0.081  0.093   | 3603.5566763877874\n","For validation\n","5.00e-5   00007350* 210.00 | 0.687  0.099  0.0000  0.000   | 0.062  0.076   | 3612.5611107349396\n","\n","5.00e-5   00007454  212.97 | 0.687  0.099  0.0000  0.000   | 0.100  0.110   | 3655.1587476730347\n","For validation\n","5.00e-5   00007455* 213.00 | 0.677  0.098  0.0000  0.000   | 0.065  0.080   | 3664.0626740455627\n","\n","5.00e-5   00007559  215.97 | 0.677  0.098  0.0000  0.000   | 0.022  0.028   | 3706.2471103668213\n","For validation\n","5.00e-5   00007560* 216.00 | 0.684  0.100  0.0000  0.000   | 0.062  0.076   | 3715.184948682785\n","\n","5.00e-5   00007664  218.97 | 0.684  0.100  0.0000  0.000   | 0.058  0.065   | 3757.7095611095436\n","For validation\n","5.00e-5   00007665* 219.00 | 0.685  0.105  0.0000  0.000   | 0.061  0.075   | 3766.6482706069946\n","\n","5.00e-5   00007769  221.97 | 0.685  0.105  0.0000  0.000   | 0.051  0.061   | 3808.8925583362587\n","For validation\n","5.00e-5   00007770* 222.00 | 0.674  0.092  0.0000  0.000   | 0.063  0.077   | 3817.9847605228424\n","\n","5.00e-5   00007874  224.97 | 0.674  0.092  0.0000  0.000   | 0.073  0.092   | 3860.4390494823456\n","For validation\n","5.00e-5   00007875* 225.00 | 0.688  0.093  0.0000  0.000   | 0.064  0.078   | 3869.3586156368256\n","\n","5.00e-5   00007979  227.97 | 0.688  0.093  0.0000  0.000   | 0.056  0.068   | 3911.6482546329594\n","For validation\n","5.00e-5   00007980* 228.00 | 0.686  0.099  0.0000  0.000   | 0.056  0.071   | 3921.2414677143097\n","\n","5.00e-5   00008084  230.97 | 0.686  0.099  0.0000  0.000   | 0.058  0.071   | 3963.5352549552917\n","For validation\n","5.00e-5   00008085* 231.00 | 0.681  0.107  0.0000  0.000   | 0.066  0.082   | 3972.42440533638\n","\n","5.00e-5   00008189  233.97 | 0.681  0.107  0.0000  0.000   | 0.042  0.050   | 4014.8365862369537\n","For validation\n","5.00e-5   00008190* 234.00 | 0.684  0.103  0.0000  0.000   | 0.057  0.072   | 4023.8597316741943\n","\n","5.00e-5   00008294  236.97 | 0.684  0.103  0.0000  0.000   | 0.083  0.109   | 4066.1088998317727\n","For validation\n","5.00e-5   00008295* 237.00 | 0.681  0.109  0.0000  0.000   | 0.061  0.074   | 4075.089013814926\n","\n","5.00e-5   00008399  239.97 | 0.681  0.109  0.0000  0.000   | 0.063  0.090   | 4117.4324114322665\n","For validation\n","5.00e-5   00008400* 240.00 | 0.684  0.095  0.0000  0.000   | 0.061  0.075   | 4126.414956331253\n","\n","5.00e-5   00008504  242.97 | 0.684  0.095  0.0000  0.000   | 0.040  0.053   | 4168.7356305122385\n","For validation\n","5.00e-5   00008505* 243.00 | 0.682  0.105  0.0000  0.000   | 0.063  0.077   | 4177.742366075516\n","\n","5.00e-5   00008609  245.97 | 0.682  0.105  0.0000  0.000   | 0.068  0.080   | 4220.0266253948215\n","For validation\n","5.00e-5   00008610* 246.00 | 0.674  0.098  0.0000  0.000   | 0.056  0.071   | 4228.954123973846\n","\n","5.00e-5   00008714  248.97 | 0.674  0.098  0.0000  0.000   | 0.074  0.099   | 4271.2029654979715\n","For validation\n","5.00e-5   00008715* 249.00 | 0.685  0.107  0.0000  0.000   | 0.059  0.074   | 4280.164680242538\n","\n","5.00e-5   00008819  251.97 | 0.685  0.107  0.0000  0.000   | 0.066  0.095   | 4322.2807252407075\n","For validation\n","5.00e-5   00008820* 252.00 | 0.678  0.105  0.0000  0.000   | 0.058  0.072   | 4331.177340745926\n","\n","5.00e-5   00008924  254.97 | 0.678  0.105  0.0000  0.000   | 0.059  0.074   | 4373.4216058254245\n","For validation\n","5.00e-5   00008925* 255.00 | 0.678  0.104  0.0000  0.000   | 0.061  0.075   | 4382.348288297653\n","\n","5.00e-5   00009029  257.97 | 0.678  0.104  0.0000  0.000   | 0.093  0.110   | 4424.8427743911745\n","For validation\n","5.00e-5   00009030* 258.00 | 0.682  0.107  0.0000  0.000   | 0.064  0.079   | 4433.814507007599\n","\n","5.00e-5   00009134  260.97 | 0.682  0.107  0.0000  0.000   | 0.045  0.063   | 4476.4546849727635\n","For validation\n","5.00e-5   00009135* 261.00 | 0.682  0.111  0.0000  0.000   | 0.059  0.074   | 4485.528231620789\n","\n","5.00e-5   00009239  263.97 | 0.682  0.111  0.0000  0.000   | 0.063  0.071   | 4527.9445390701295\n","For validation\n","5.00e-5   00009240* 264.00 | 0.681  0.102  0.0000  0.000   | 0.061  0.075   | 4536.863430738449\n","\n","5.00e-5   00009344  266.97 | 0.681  0.102  0.0000  0.000   | 0.053  0.058   | 4579.1644871234895\n","For validation\n","5.00e-5   00009345* 267.00 | 0.688  0.098  0.0000  0.000   | 0.056  0.072   | 4588.1747760772705\n","\n","5.00e-5   00009449  269.97 | 0.688  0.098  0.0000  0.000   | 0.065  0.069   | 4630.6930451393135\n","For validation\n","5.00e-5   00009450* 270.00 | 0.672  0.107  0.0000  0.000   | 0.059  0.073   | 4639.734122514725\n","\n","5.00e-5   00009554  272.97 | 0.672  0.107  0.0000  0.000   | 0.053  0.064   | 4682.2463002204895\n","For validation\n","5.00e-5   00009555* 273.00 | 0.684  0.103  0.0000  0.000   | 0.053  0.068   | 4691.169714450836\n","\n","5.00e-5   00009659  275.97 | 0.684  0.103  0.0000  0.000   | 0.059  0.063   | 4733.6528263092045\n","For validation\n","5.00e-5   00009660* 276.00 | 0.669  0.116  0.0000  0.000   | 0.051  0.065   | 4742.715902328491\n","\n","5.00e-5   00009764  278.97 | 0.669  0.116  0.0000  0.000   | 0.076  0.087   | 4784.9544947147375\n","For validation\n","5.00e-5   00009765* 279.00 | 0.680  0.099  0.0000  0.000   | 0.064  0.078   | 4793.951819181442\n","\n","5.00e-5   00009869  281.97 | 0.680  0.099  0.0000  0.000   | 0.039  0.055   | 4836.1703212261295\n","For validation\n","5.00e-5   00009870* 282.00 | 0.672  0.103  0.0000  0.000   | 0.060  0.074   | 4845.172808885574\n","\n","5.00e-5   00009974  284.97 | 0.672  0.103  0.0000  0.000   | 0.064  0.081   | 4887.7512199878695\n","For validation\n","5.00e-5   00009975* 285.00 | 0.679  0.109  0.0000  0.000   | 0.059  0.073   | 4896.670835018158\n","\n","5.00e-5   00010079  287.97 | 0.679  0.109  0.0000  0.000   | 0.094  0.121   | 4938.9956979751595\n","For validation\n","5.00e-5   00010080* 288.00 | 0.680  0.108  0.0000  0.000   | 0.054  0.069   | 4947.955624341965\n","\n","5.00e-5   00010184  290.97 | 0.680  0.108  0.0000  0.000   | 0.044  0.067   | 4990.3416755199435\n","For validation\n","5.00e-5   00010185* 291.00 | 0.688  0.107  0.0000  0.000   | 0.052  0.068   | 4999.270298957825\n","\n","5.00e-5   00010289  293.97 | 0.688  0.107  0.0000  0.000   | 0.029  0.039   | 5041.6649904251135\n","For validation\n","5.00e-5   00010290* 294.00 | 0.690  0.112  0.0000  0.000   | 0.048  0.063   | 5050.684163570404\n","\n","5.00e-5   00010394  296.97 | 0.690  0.112  0.0000  0.000   | 0.047  0.057   | 5092.9600446224215\n","For validation\n","5.00e-5   00010395* 297.00 | 0.677  0.112  0.0000  0.000   | 0.055  0.070   | 5101.9580936431885\n","\n","5.00e-5   00010499  299.97 | 0.677  0.112  0.0000  0.000   | 0.029  0.039   | 5144.3366782665255\n","For validation\n","5.00e-5   00010500* 300.00 | 0.685  0.113  0.0000  0.000   | 0.058  0.073   | 5153.22323513031\n","\n","5.00e-5   00010604  302.97 | 0.685  0.113  0.0000  0.000   | 0.065  0.080   | 5195.5931518077855\n","For validation\n","5.00e-5   00010605* 303.00 | 0.690  0.099  0.0000  0.000   | 0.050  0.065   | 5204.469137907028\n","\n","5.00e-5   00010709  305.97 | 0.690  0.099  0.0000  0.000   | 0.044  0.060   | 5246.6407918930055\n","For validation\n","5.00e-5   00010710* 306.00 | 0.680  0.103  0.0000  0.000   | 0.050  0.065   | 5255.718152761459\n","\n","5.00e-5   00010814  308.97 | 0.680  0.103  0.0000  0.000   | 0.053  0.057   | 5298.1201746463785\n","For validation\n","5.00e-5   00010815* 309.00 | 0.681  0.112  0.0000  0.000   | 0.052  0.067   | 5307.078809261322\n","\n","5.00e-5   00010919  311.97 | 0.681  0.112  0.0000  0.000   | 0.036  0.040   | 5349.3086731433875\n","For validation\n","5.00e-5   00010920* 312.00 | 0.682  0.113  0.0000  0.000   | 0.049  0.065   | 5358.174051523209\n","\n","5.00e-5   00011024  314.97 | 0.682  0.113  0.0000  0.000   | 0.048  0.055   | 5400.4650959968575\n","For validation\n","5.00e-5   00011025* 315.00 | 0.679  0.104  0.0000  0.000   | 0.051  0.066   | 5409.408304929733\n","\n","5.00e-5   00011129  317.97 | 0.679  0.104  0.0000  0.000   | 0.050  0.077   | 5451.6952211856845\n","For validation\n","5.00e-5   00011130* 318.00 | 0.702  0.113  0.0000  0.000   | 0.048  0.063   | 5460.599782943726\n","\n","5.00e-5   00011234  320.97 | 0.702  0.113  0.0000  0.000   | 0.072  0.076   | 5502.9117259979255\n","For validation\n","5.00e-5   00011235* 321.00 | 0.687  0.113  0.0000  0.000   | 0.052  0.068   | 5511.806199789047\n","\n","5.00e-5   00011339  323.97 | 0.687  0.113  0.0000  0.000   | 0.028  0.033   | 5554.1062302589425\n","For validation\n","5.00e-5   00011340* 324.00 | 0.700  0.108  0.0000  0.000   | 0.052  0.067   | 5563.069271326065\n","\n","5.00e-5   00011444  326.97 | 0.700  0.108  0.0000  0.000   | 0.045  0.064   | 5605.2246823310855\n","For validation\n","5.00e-5   00011445* 327.00 | 0.686  0.110  0.0000  0.000   | 0.053  0.067   | 5614.184396743774\n","\n","5.00e-5   00011549  329.97 | 0.686  0.110  0.0000  0.000   | 0.062  0.065   | 5656.4079504013065\n","For validation\n","5.00e-5   00011550* 330.00 | 0.697  0.106  0.0000  0.000   | 0.051  0.065   | 5665.327426195145\n","\n","5.00e-5   00011654  332.97 | 0.697  0.106  0.0000  0.000   | 0.086  0.111   | 5707.6283295154575\n","For validation\n","5.00e-5   00011655* 333.00 | 0.672  0.110  0.0000  0.000   | 0.051  0.065   | 5716.630761861801\n","\n","5.00e-5   00011759  335.97 | 0.672  0.110  0.0000  0.000   | 0.029  0.038   | 5758.8563909530645\n","For validation\n","5.00e-5   00011760* 336.00 | 0.677  0.103  0.0000  0.000   | 0.056  0.070   | 5767.8007135391235\n","\n","5.00e-5   00011864  338.97 | 0.677  0.103  0.0000  0.000   | 0.076  0.096   | 5810.3887395858765\n","For validation\n","5.00e-5   00011865* 339.00 | 0.673  0.105  0.0000  0.000   | 0.052  0.066   | 5819.323035001755\n","\n","5.00e-5   00011969  341.97 | 0.673  0.105  0.0000  0.000   | 0.045  0.052   | 5861.6993868350985\n","For validation\n","5.00e-5   00011970* 342.00 | 0.704  0.111  0.0000  0.000   | 0.049  0.064   | 5870.624708652496\n","\n","5.00e-5   00012074  344.97 | 0.704  0.111  0.0000  0.000   | 0.041  0.051   | 5912.6978344917365\n","For validation\n","5.00e-5   00012075* 345.00 | 0.679  0.111  0.0000  0.000   | 0.050  0.065   | 5921.686225414276\n","\n","5.00e-5   00012179  347.97 | 0.679  0.111  0.0000  0.000   | 0.058  0.079   | 5964.3886716365815\n","For validation\n","5.00e-5   00012180* 348.00 | 0.671  0.113  0.0000  0.000   | 0.047  0.062   | 5973.367209911346\n","\n","5.00e-5   00012284  350.97 | 0.671  0.113  0.0000  0.000   | 0.033  0.045   | 6015.7331407070165\n","For validation\n","5.00e-5   00012285* 351.00 | 0.689  0.108  0.0000  0.000   | 0.051  0.064   | 6024.673626422882\n","\n","5.00e-5   00012389  353.97 | 0.689  0.108  0.0000  0.000   | 0.045  0.053   | 6066.9982185363775\n","For validation\n","5.00e-5   00012390* 354.00 | 0.688  0.101  0.0000  0.000   | 0.048  0.064   | 6076.022863149643\n","\n","5.00e-5   00012494  356.97 | 0.688  0.101  0.0000  0.000   | 0.032  0.039   | 6118.3173582553865\n","For validation\n","5.00e-5   00012495* 357.00 | 0.686  0.100  0.0000  0.000   | 0.051  0.066   | 6127.28413939476\n","\n","5.00e-5   00012599  359.97 | 0.686  0.100  0.0000  0.000   | 0.025  0.027   | 6169.8502857685095\n","For validation\n","5.00e-5   00012600* 360.00 | 0.691  0.106  0.0000  0.000   | 0.055  0.069   | 6178.761993408203\n","\n","5.00e-5   00012704  362.97 | 0.691  0.106  0.0000  0.000   | 0.060  0.071   | 6221.0444121360785\n","For validation\n","5.00e-5   00012705* 363.00 | 0.684  0.116  0.0000  0.000   | 0.050  0.064   | 6230.04874587059\n","\n","5.00e-5   00012809  365.97 | 0.684  0.116  0.0000  0.000   | 0.037  0.046   | 6272.1968626976015\n","For validation\n","5.00e-5   00012810* 366.00 | 0.702  0.119  0.0000  0.000   | 0.050  0.064   | 6281.092162370682\n","\n","5.00e-5   00012914  368.97 | 0.702  0.119  0.0000  0.000   | 0.041  0.072   | 6323.4033341407785\n","For validation\n","5.00e-5   00012915* 369.00 | 0.686  0.116  0.0000  0.000   | 0.055  0.068   | 6332.3657858371735\n","\n","5.00e-5   00013019  371.97 | 0.686  0.116  0.0000  0.000   | 0.066  0.094   | 6374.7364010810855\n","For validation\n","5.00e-5   00013020* 372.00 | 0.692  0.117  0.0000  0.000   | 0.050  0.065   | 6383.729205131531\n","\n","5.00e-5   00013124  374.97 | 0.692  0.117  0.0000  0.000   | 0.066  0.083   | 6426.0365784168245\n","For validation\n","5.00e-5   00013125* 375.00 | 0.690  0.116  0.0000  0.000   | 0.052  0.067   | 6435.037270307541\n","\n","5.00e-5   00013229  377.97 | 0.690  0.116  0.0000  0.000   | 0.072  0.098   | 6477.8027744293215\n","For validation\n","5.00e-5   00013230* 378.00 | 0.700  0.109  0.0000  0.000   | 0.048  0.064   | 6486.772055387497\n","\n","5.00e-5   00013334  380.97 | 0.700  0.109  0.0000  0.000   | 0.055  0.063   | 6529.1636803150185\n","For validation\n","5.00e-5   00013335* 381.00 | 0.696  0.112  0.0000  0.000   | 0.048  0.063   | 6538.113316297531\n","\n","5.00e-5   00013439  383.97 | 0.696  0.112  0.0000  0.000   | 0.029  0.033   | 6580.5404672622685\n","For validation\n","5.00e-5   00013440* 384.00 | 0.692  0.112  0.0000  0.000   | 0.050  0.065   | 6589.554073810577\n","\n","5.00e-5   00013544  386.97 | 0.692  0.112  0.0000  0.000   | 0.039  0.050   | 6631.8640060424805\n","For validation\n","5.00e-5   00013545* 387.00 | 0.689  0.115  0.0000  0.000   | 0.051  0.067   | 6640.899689674377\n","\n","5.00e-5   00013649  389.97 | 0.689  0.115  0.0000  0.000   | 0.048  0.063   | 6683.2242786884315\n","For validation\n","5.00e-5   00013650* 390.00 | 0.681  0.120  0.0000  0.000   | 0.053  0.068   | 6692.162609577179\n","\n","5.00e-5   00013754  392.97 | 0.681  0.120  0.0000  0.000   | 0.052  0.071   | 6734.3644175529485\n","For validation\n","5.00e-5   00013755* 393.00 | 0.701  0.119  0.0000  0.000   | 0.054  0.070   | 6743.2976751327515\n","\n","5.00e-5   00013859  395.97 | 0.701  0.119  0.0000  0.000   | 0.042  0.059   | 6785.8275480270395\n","For validation\n","5.00e-5   00013860* 396.00 | 0.685  0.114  0.0000  0.000   | 0.051  0.066   | 6794.846879959106\n","\n","5.00e-5   00013964  398.97 | 0.685  0.114  0.0000  0.000   | 0.087  0.099   | 6837.2776401042945\n","For validation\n","5.00e-5   00013965* 399.00 | 0.688  0.107  0.0000  0.000   | 0.062  0.074   | 6846.184943675995\n","\n","5.00e-5   00014069  401.97 | 0.688  0.107  0.0000  0.000   | 0.046  0.059   | 6888.3145523071295\n","For validation\n","5.00e-5   00014070* 402.00 | 0.683  0.110  0.0000  0.000   | 0.052  0.066   | 6897.217157125473\n","\n","5.00e-5   00014174  404.97 | 0.683  0.110  0.0000  0.000   | 0.062  0.069   | 6939.4728362560275\n","For validation\n","5.00e-5   00014175* 405.00 | 0.684  0.110  0.0000  0.000   | 0.047  0.062   | 6948.431565999985\n","\n","5.00e-5   00014279  407.97 | 0.684  0.110  0.0000  0.000   | 0.033  0.039   | 6990.9951152801515\n","For validation\n","5.00e-5   00014280* 408.00 | 0.701  0.110  0.0000  0.000   | 0.046  0.061   | 6999.846862077713\n","\n","5.00e-5   00014384  410.97 | 0.701  0.110  0.0000  0.000   | 0.051  0.064   | 7041.9498002529145\n","For validation\n","5.00e-5   00014385* 411.00 | 0.698  0.109  0.0000  0.000   | 0.052  0.067   | 7050.83381652832\n","\n","5.00e-5   00014489  413.97 | 0.698  0.109  0.0000  0.000   | 0.036  0.048   | 7093.1963651180275\n","For validation\n","5.00e-5   00014490* 414.00 | 0.692  0.120  0.0000  0.000   | 0.049  0.065   | 7102.148237705231\n","\n","5.00e-5   00014594  416.97 | 0.692  0.120  0.0000  0.000   | 0.034  0.052   | 7144.6781384944925\n","For validation\n","5.00e-5   00014595* 417.00 | 0.689  0.108  0.0000  0.000   | 0.047  0.063   | 7153.674342632294\n","\n","5.00e-5   00014699  419.97 | 0.689  0.108  0.0000  0.000   | 0.058  0.077   | 7196.0375285148625\n","For validation\n","5.00e-5   00014700* 420.00 | 0.686  0.113  0.0000  0.000   | 0.049  0.064   | 7205.018198490143\n","\n","5.00e-5   00014804  422.97 | 0.686  0.113  0.0000  0.000   | 0.059  0.081   | 7247.1472508907325\n","For validation\n","5.00e-5   00014805* 423.00 | 0.694  0.116  0.0000  0.000   | 0.044  0.059   | 7256.181310176849\n","\n","5.00e-5   00014909  425.97 | 0.694  0.116  0.0000  0.000   | 0.052  0.057   | 7298.5673944950195\n","For validation\n","5.00e-5   00014910* 426.00 | 0.704  0.112  0.0000  0.000   | 0.048  0.063   | 7307.618717432022\n","\n","5.00e-5   00015014  428.97 | 0.704  0.112  0.0000  0.000   | 0.051  0.078   | 7350.1296212673195\n","For validation\n","5.00e-5   00015015* 429.00 | 0.678  0.107  0.0000  0.000   | 0.046  0.060   | 7359.114485740662\n","\n","5.00e-5   00015119  431.97 | 0.678  0.107  0.0000  0.000   | 0.046  0.057   | 7401.5406074523935\n","For validation\n","5.00e-5   00015120* 432.00 | 0.687  0.121  0.0000  0.000   | 0.048  0.064   | 7410.477450609207\n","\n","5.00e-5   00015224  434.97 | 0.687  0.121  0.0000  0.000   | 0.047  0.065   | 7452.9491174221045\n","For validation\n","5.00e-5   00015225* 435.00 | 0.690  0.117  0.0000  0.000   | 0.046  0.060   | 7461.933485984802\n","\n","5.00e-5   00015329  437.97 | 0.690  0.117  0.0000  0.000   | 0.057  0.087   | 7504.1589868068695\n","For validation\n","5.00e-5   00015330* 438.00 | 0.691  0.110  0.0000  0.000   | 0.047  0.063   | 7513.016596078873\n","\n","5.00e-5   00015434  440.97 | 0.691  0.110  0.0000  0.000   | 0.036  0.048   | 7555.3603923320775\n","For validation\n","5.00e-5   00015435* 441.00 | 0.681  0.117  0.0000  0.000   | 0.044  0.059   | 7564.330837011337\n","\n","5.00e-5   00015539  443.97 | 0.681  0.117  0.0000  0.000   | 0.056  0.080   | 7606.7471027374275\n","For validation\n","5.00e-5   00015540* 444.00 | 0.676  0.104  0.0000  0.000   | 0.046  0.061   | 7615.737866163254\n","\n","5.00e-5   00015644  446.97 | 0.676  0.104  0.0000  0.000   | 0.048  0.067   | 7658.2731187343665\n","For validation\n","5.00e-5   00015645* 447.00 | 0.698  0.110  0.0000  0.000   | 0.046  0.060   | 7667.232476234436\n","\n","5.00e-5   00015749  449.97 | 0.698  0.110  0.0000  0.000   | 0.047  0.079   | 7709.3315286636355\n","For validation\n","5.00e-5   00015750* 450.00 | 0.692  0.111  0.0000  0.000   | 0.051  0.065   | 7718.294179916382\n","\n","5.00e-5   00015854  452.97 | 0.692  0.111  0.0000  0.000   | 0.035  0.050   | 7760.6532535552985\n","For validation\n","5.00e-5   00015855* 453.00 | 0.690  0.121  0.0000  0.000   | 0.047  0.062   | 7769.560309886932\n","\n","5.00e-5   00015959  455.97 | 0.690  0.121  0.0000  0.000   | 0.029  0.037   | 7812.1509773731235\n","For validation\n","5.00e-5   00015960* 456.00 | 0.675  0.124  0.0000  0.000   | 0.044  0.059   | 7821.154701471329\n","\n","5.00e-5   00016064  458.97 | 0.675  0.124  0.0000  0.000   | 0.043  0.049   | 7863.6868429183965\n","For validation\n","5.00e-5   00016065* 459.00 | 0.685  0.107  0.0000  0.000   | 0.042  0.057   | 7872.634795188904\n","\n","5.00e-5   00016169  461.97 | 0.685  0.107  0.0000  0.000   | 0.035  0.042   | 7914.8593518733985\n","For validation\n","5.00e-5   00016170* 462.00 | 0.691  0.111  0.0000  0.000   | 0.053  0.068   | 7923.85115480423\n","\n","5.00e-5   00016274  464.97 | 0.691  0.111  0.0000  0.000   | 0.067  0.075   | 7966.0658836364755\n","For validation\n","5.00e-5   00016275* 465.00 | 0.688  0.122  0.0000  0.000   | 0.043  0.058   | 7975.050425052643\n","\n","5.00e-5   00016379  467.97 | 0.688  0.122  0.0000  0.000   | 0.075  0.098   | 8017.2850074768075\n","For validation\n","5.00e-5   00016380* 468.00 | 0.688  0.108  0.0000  0.000   | 0.045  0.060   | 8026.264028072357\n","\n","5.00e-5   00016484  470.97 | 0.688  0.108  0.0000  0.000   | 0.053  0.066   | 8068.8665752410895\n","For validation\n","5.00e-5   00016485* 471.00 | 0.697  0.120  0.0000  0.000   | 0.042  0.056   | 8077.8853640556335\n","\n","5.00e-5   00016589  473.97 | 0.697  0.120  0.0000  0.000   | 0.044  0.053   | 8120.1224911212925\n","For validation\n","5.00e-5   00016590* 474.00 | 0.689  0.111  0.0000  0.000   | 0.051  0.066   | 8129.021937847137\n","\n","5.00e-5   00016694  476.97 | 0.689  0.111  0.0000  0.000   | 0.024  0.029   | 8171.5757708549595\n","For validation\n","5.00e-5   00016695* 477.00 | 0.701  0.117  0.0000  0.000   | 0.044  0.060   | 8180.708433628082\n","\n","5.00e-5   00016799  479.97 | 0.701  0.117  0.0000  0.000   | 0.034  0.048   | 8223.1688652038575\n","For validation\n","5.00e-5   00016800* 480.00 | 0.694  0.125  0.0000  0.000   | 0.045  0.059   | 8232.102217674255\n","\n","5.00e-5   00016904  482.97 | 0.694  0.125  0.0000  0.000   | 0.043  0.064   | 8274.529525041585\n","For validation\n","5.00e-5   00016905* 483.00 | 0.694  0.124  0.0000  0.000   | 0.045  0.060   | 8283.459475755692\n","\n","5.00e-5   00017009  485.97 | 0.694  0.124  0.0000  0.000   | 0.043  0.059   | 8325.769765853882\n","For validation\n","5.00e-5   00017010* 486.00 | 0.686  0.116  0.0000  0.000   | 0.043  0.058   | 8334.729830741882\n","\n","5.00e-5   00017114  488.97 | 0.686  0.116  0.0000  0.000   | 0.028  0.041   | 8377.186242580414\n","For validation\n","5.00e-5   00017115* 489.00 | 0.688  0.129  0.0000  0.000   | 0.043  0.057   | 8386.203423976898\n","\n","5.00e-5   00017219  491.97 | 0.688  0.129  0.0000  0.000   | 0.037  0.041   | 8428.479784250266\n","For validation\n","5.00e-5   00017220* 492.00 | 0.685  0.124  0.0000  0.000   | 0.043  0.058   | 8437.383931398392\n","\n","5.00e-5   00017324  494.97 | 0.685  0.124  0.0000  0.000   | 0.042  0.065   | 8479.664676666264\n","For validation\n","5.00e-5   00017325* 495.00 | 0.685  0.127  0.0000  0.000   | 0.043  0.057   | 8488.669345140457\n","\n","5.00e-5   00017429  497.97 | 0.685  0.127  0.0000  0.000   | 0.035  0.051   | 8530.853477001197\n","For validation\n","5.00e-5   00017430* 498.00 | 0.691  0.116  0.0000  0.000   | 0.047  0.061   | 8539.868841648102\n","\n","5.00e-5   00017534  500.97 | 0.691  0.116  0.0000  0.000   | 0.054  0.078   | 8582.290342092514\n","For validation\n","5.00e-5   00017535* 501.00 | 0.691  0.115  0.0000  0.000   | 0.041  0.056   | 8591.236874818802\n","\n","5.00e-5   00017639  503.97 | 0.691  0.115  0.0000  0.000   | 0.033  0.043   | 8633.438367605213\n","For validation\n","5.00e-5   00017640* 504.00 | 0.683  0.113  0.0000  0.000   | 0.047  0.062   | 8642.362369537354\n","\n","5.00e-5   00017744  506.97 | 0.683  0.113  0.0000  0.000   | 0.053  0.065   | 8684.656487464905\n","For validation\n","5.00e-5   00017745* 507.00 | 0.689  0.118  0.0000  0.000   | 0.043  0.057   | 8693.647636175156\n","\n","5.00e-5   00017849  509.97 | 0.689  0.118  0.0000  0.000   | 0.057  0.095   | 8735.737504959106\n","For validation\n","5.00e-5   00017850* 510.00 | 0.687  0.122  0.0000  0.000   | 0.046  0.060   | 8744.679997444153\n","\n","5.00e-5   00017954  512.97 | 0.687  0.122  0.0000  0.000   | 0.044  0.055   | 8786.635687351227\n","For validation\n","5.00e-5   00017955* 513.00 | 0.696  0.123  0.0000  0.000   | 0.045  0.059   | 8795.569728851318\n","\n","5.00e-5   00018059  515.97 | 0.696  0.123  0.0000  0.000   | 0.048  0.069   | 8837.744218349457\n","For validation\n","5.00e-5   00018060* 516.00 | 0.688  0.127  0.0000  0.000   | 0.050  0.065   | 8846.79654288292\n","\n","5.00e-5   00018164  518.97 | 0.688  0.127  0.0000  0.000   | 0.054  0.056   | 8889.211321592331\n","For validation\n","5.00e-5   00018165* 519.00 | 0.686  0.124  0.0000  0.000   | 0.044  0.059   | 8898.154158353806\n","\n","5.00e-5   00018269  521.97 | 0.686  0.124  0.0000  0.000   | 0.055  0.072   | 8940.345764160156\n","For validation\n","5.00e-5   00018270* 522.00 | 0.692  0.127  0.0000  0.000   | 0.042  0.058   | 8949.32451915741\n","\n","5.00e-5   00018374  524.97 | 0.692  0.127  0.0000  0.000   | 0.032  0.042   | 8991.634414196014\n","For validation\n","5.00e-5   00018375* 525.00 | 0.690  0.120  0.0000  0.000   | 0.046  0.061   | 9000.670527219772\n","\n","5.00e-5   00018479  527.97 | 0.690  0.120  0.0000  0.000   | 0.052  0.063   | 9042.892123222351\n","For validation\n","5.00e-5   00018480* 528.00 | 0.693  0.125  0.0000  0.000   | 0.047  0.061   | 9051.879691839218\n","\n","5.00e-5   00018584  530.97 | 0.693  0.125  0.0000  0.000   | 0.042  0.049   | 9094.047932386398\n","For validation\n","5.00e-5   00018585* 531.00 | 0.682  0.128  0.0000  0.000   | 0.051  0.064   | 9103.010403633118\n","\n","5.00e-5   00018689  533.97 | 0.682  0.128  0.0000  0.000   | 0.033  0.048   | 9145.363904237747\n","For validation\n","5.00e-5   00018690* 534.00 | 0.686  0.116  0.0000  0.000   | 0.042  0.057   | 9154.370903015137\n","\n","5.00e-5   00018794  536.97 | 0.686  0.116  0.0000  0.000   | 0.032  0.054   | 9196.631948947906\n","For validation\n","5.00e-5   00018795* 537.00 | 0.685  0.117  0.0000  0.000   | 0.054  0.067   | 9205.557747364044\n","\n","5.00e-5   00018899  539.97 | 0.685  0.117  0.0000  0.000   | 0.021  0.027   | 9247.887097597122\n","For validation\n","5.00e-5   00018900* 540.00 | 0.688  0.126  0.0000  0.000   | 0.046  0.061   | 9257.642663478851\n","\n","5.00e-5   00019004  542.97 | 0.688  0.126  0.0000  0.000   | 0.026  0.036   | 9301.693076610565\n","For validation\n","5.00e-5   00019005* 543.00 | 0.676  0.119  0.0000  0.000   | 0.051  0.065   | 9311.069416999817\n","\n","5.00e-5   00019109  545.97 | 0.676  0.119  0.0000  0.000   | 0.030  0.038   | 9355.558645725256\n","For validation\n","5.00e-5   00019110* 546.00 | 0.693  0.122  0.0000  0.000   | 0.039  0.055   | 9365.295578956604\n","\n","5.00e-5   00019214  548.97 | 0.693  0.122  0.0000  0.000   | 0.029  0.037   | 9412.468864679337\n","For validation\n","5.00e-5   00019215* 549.00 | 0.695  0.125  0.0000  0.000   | 0.041  0.056   | 9422.447186231613\n","\n","5.00e-5   00019319  551.97 | 0.695  0.125  0.0000  0.000   | 0.066  0.072   | 9467.985888719559\n","For validation\n","5.00e-5   00019320* 552.00 | 0.695  0.122  0.0000  0.000   | 0.047  0.062   | 9477.36615037918\n","\n","5.00e-5   00019424  554.97 | 0.695  0.122  0.0000  0.000   | 0.039  0.056   | 9521.007777929306\n","For validation\n","5.00e-5   00019425* 555.00 | 0.696  0.114  0.0000  0.000   | 0.045  0.059   | 9530.45656299591\n","\n","5.00e-5   00019529  557.97 | 0.696  0.114  0.0000  0.000   | 0.070  0.080   | 9573.740627527237\n","For validation\n","5.00e-5   00019530* 558.00 | 0.700  0.120  0.0000  0.000   | 0.042  0.057   | 9583.260295629501\n","\n","5.00e-5   00019634  560.97 | 0.700  0.120  0.0000  0.000   | 0.031  0.046   | 9626.479950428009\n","For validation\n","5.00e-5   00019635* 561.00 | 0.693  0.127  0.0000  0.000   | 0.039  0.054   | 9636.002428293228\n","\n","5.00e-5   00019739  563.97 | 0.693  0.127  0.0000  0.000   | 0.033  0.045   | 9679.825988054276\n","For validation\n","5.00e-5   00019740* 564.00 | 0.704  0.127  0.0000  0.000   | 0.042  0.057   | 9689.15692782402\n","\n","5.00e-5   00019844  566.97 | 0.704  0.127  0.0000  0.000   | 0.058  0.079   | 9732.679439544678\n","For validation\n","5.00e-5   00019845* 567.00 | 0.691  0.125  0.0000  0.000   | 0.041  0.057   | 9742.073189020157\n","\n","5.00e-5   00019949  569.97 | 0.691  0.125  0.0000  0.000   | 0.030  0.040   | 9784.978121042252\n","For validation\n","5.00e-5   00019950* 570.00 | 0.688  0.121  0.0000  0.000   | 0.044  0.058   | 9794.325194358826\n","\n","5.00e-5   00020019  571.97 | 0.688  0.121  0.0000  0.000   | 0.032  0.040   | 9821.512841224674\n","\n"]}],"source":["fold = 0\n","cfg = config(fold)\n","# tdl, vdl = main(cfg)\n","main(cfg)"]},{"cell_type":"markdown","metadata":{},"source":["# Ideas to try:\n","- change loss"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.status.busy":"2022-08-24T14:48:58.597217Z","iopub.status.idle":"2022-08-24T14:48:58.598618Z","shell.execute_reply":"2022-08-24T14:48:58.598365Z","shell.execute_reply.started":"2022-08-24T14:48:58.598338Z"},"trusted":true},"outputs":[],"source":["# import matplotlib.pyplot as plt\n","# def plot_batch(imgs, msks, size=3):\n","#     for idx in range(size):\n","#         plt.figure(figsize=(4*3, 5))\n","\n","#         plt.subplot(1, 3, 1); plt.imshow(imgs[idx])\n","#         plt.title('image', fontsize=15)\n","#         plt.axis('OFF')\n","\n","#         plt.subplot(1, 3, 2); plt.imshow(msks[idx])\n","#         plt.title('mask', fontsize=15)\n","#         plt.axis('OFF')\n","\n","#         plt.subplot(1, 3, 3); plt.imshow(imgs[idx]); plt.imshow(msks[idx], alpha=0.3)\n","#         plt.title('overlay', fontsize=15)\n","#         plt.axis('OFF')\n","        \n","#         plt.tight_layout()\n","#         plt.show()\n","\n","        \n","# items = next(iter(vdl))\n","# imgs = items[\"image\"].permute((0, 2, 3, 1))\n","# msks = items[\"mask\"].permute((0, 2, 3, 1))\n","# imgs.size(), msks.size()\n","\n","# # torch.unique(msks)\n","\n","# plot_batch(imgs, msks, size=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.7.0 ('.venv': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"vscode":{"interpreter":{"hash":"21a2a557654fc1676068684031cf9bb9dfda94e124d3623f4e9c9ed764d794ac"}}},"nbformat":4,"nbformat_minor":4}
