{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lakshita/somusan/hubmap_kaggle/.venv/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.cuda.amp as amp\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import RandomSampler \n",
    "from torch.utils.data import SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "import albumentations as A\n",
    "import gc\n",
    "\n",
    "# from torchmetrics.functional import dice_score\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import tifffile\n",
    "is_amp = True\n",
    "import logging\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import numpy as np\n",
    "from itertools import repeat\n",
    "import collections.abc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/lakshita/somusan/hubmap_kaggle/nbs/result’: File exists\n",
      "mkdir: cannot create directory ‘/home/lakshita/somusan/hubmap_kaggle/nbs/checkpoint’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir /home/lakshita/somusan/hubmap_kaggle/nbs/result\n",
    "!mkdir /home/lakshita/somusan/hubmap_kaggle/nbs/checkpoint\n",
    "\n",
    "root_dir = '/home/lakshita/somusan/hubmap_kaggle/nbs'\n",
    "pretrain_dir = '/home/lakshita/somusan/hubmap_kaggle/nbs/swin-tiny-small-22k-pretrained/'\n",
    "\n",
    "TRAIN = '/home/lakshita/somusan/hubmap_kaggle/hubmap_data/hubmap-organ-segmentation/train_images'\n",
    "LABELS = '/home/lakshita/somusan/hubmap_kaggle/hubmap_data/hubmap-organ-segmentation/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_tensor(image, mode='bgr'): #image mode\n",
    "    if mode=='bgr':\n",
    "        image = image[:,:,::-1]\n",
    "    x = image\n",
    "    x = x.transpose(2,0,1)\n",
    "    x = np.ascontiguousarray(x)\n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    return x\n",
    "\n",
    "\n",
    "def mask_to_tensor(mask):\n",
    "    x = mask\n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    return x\n",
    "\n",
    "\n",
    "tensor_list = ['mask', 'image', 'organ']\n",
    "\n",
    "def null_collate(batch):\n",
    "    d = {}\n",
    "    key = batch[0].keys()\n",
    "    for k in key:\n",
    "        v = [b[k] for b in batch]\n",
    "        if k in tensor_list:\n",
    "            v = torch.stack(v)\n",
    "        d[k] = v\n",
    "\n",
    "    d['mask'] = d['mask'].unsqueeze(1)\n",
    "    d['organ'] = d['organ'].reshape(-1)\n",
    "    return d\n",
    "\n",
    "\n",
    "def _ntuple(n):\n",
    "    def parse(x):\n",
    "        if isinstance(x, collections.abc.Iterable):\n",
    "            return x\n",
    "        return tuple(repeat(x, n))\n",
    "    return parse\n",
    "\n",
    "\n",
    "to_2tuple = _ntuple(2)\n",
    "\n",
    "\n",
    "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
    "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
    "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
    "    def norm_cdf(x):\n",
    "        # Computes standard normal cumulative distribution function\n",
    "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
    "\n",
    "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
    "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
    "                      \"The distribution of values may be incorrect.\",\n",
    "                      stacklevel=2)\n",
    "\n",
    "        \n",
    "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
    "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
    "\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n",
    "    if keep_prob > 0.0 and scale_by_keep:\n",
    "        random_tensor.div_(keep_prob)\n",
    "    return x * random_tensor\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob: float = 0., scale_by_keep: bool = True):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.scale_by_keep = scale_by_keep\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f'drop_prob={round(self.drop_prob,3):0.3f}'\n",
    "    \n",
    "    \n",
    "class RGB(nn.Module):\n",
    "    IMAGE_RGB_MEAN = [0.485, 0.456, 0.406] #[0.5, 0.5, 0.5]\n",
    "    IMAGE_RGB_STD  = [0.229, 0.224, 0.225] #[0.5, 0.5, 0.5]\n",
    "\n",
    "    def __init__(self,):\n",
    "        super(RGB, self).__init__()\n",
    "        self.register_buffer('mean', torch.zeros(1,3,1,1))\n",
    "        self.register_buffer('std', torch.ones(1,3,1,1))\n",
    "        self.mean.data = torch.FloatTensor(self.IMAGE_RGB_MEAN).view(self.mean.shape)\n",
    "        self.std.data = torch.FloatTensor(self.IMAGE_RGB_STD).view(self.std.shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x-self.mean)/self.std\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def message(mode='print'):\n",
    "    asterisk = ' '\n",
    "    if mode==('print'):\n",
    "        loss = batch_loss\n",
    "    if mode==('log'):\n",
    "        loss = train_loss\n",
    "        if (iteration % iter_save == 0): asterisk = '*'\n",
    "\n",
    "    text = \\\n",
    "        ('%0.2e   %08d%s %6.2f | '%(rate, iteration, asterisk, epoch,)).replace('e-0','e-').replace('e+0','e+') + \\\n",
    "        '%4.3f  %4.3f  %4.4f  %4.3f   | '%(*valid_loss,) + \\\n",
    "        '%4.3f  %4.3f   | '%(*loss,) + \\\n",
    "        '%s' % ((time.time() - start_timer))\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_augment5(image, mask, organ):\n",
    "    #image, mask  = do_crop(image, mask, image_size, xy=(None,None))\n",
    "    return image, mask\n",
    "\n",
    "def train_augment5b(image, mask, organ):\n",
    "    image, mask = do_random_flip(image, mask)\n",
    "    image, mask = do_random_rot90(image, mask)\n",
    "\n",
    "    for fn in np.random.choice([\n",
    "        lambda image, mask: (image, mask),\n",
    "        lambda image, mask: do_random_noise(image, mask, mag=0.1),\n",
    "        lambda image, mask: do_random_contast(image, mask, mag=0.40),\n",
    "        lambda image, mask: do_random_hsv(image, mask, mag=[0.40, 0.40, 0])\n",
    "    ], 2): image, mask = fn(image, mask)\n",
    "\n",
    "    for fn in np.random.choice([\n",
    "        lambda image, mask: (image, mask),\n",
    "        lambda image, mask: do_random_rotate_scale(image, mask, angle=45, scale=[0.50, 2.0]),\n",
    "    ], 1): image, mask = fn(image, mask)\n",
    "\n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_random_flip(image, mask):\n",
    "    if np.random.rand()>0.5:\n",
    "        image = cv2.flip(image,0)\n",
    "        mask = cv2.flip(mask,0)\n",
    "    if np.random.rand()>0.5:\n",
    "        image = cv2.flip(image,1)\n",
    "        mask = cv2.flip(mask,1)\n",
    "    if np.random.rand()>0.5:\n",
    "        image = image.transpose(1,0,2)\n",
    "        mask = mask.transpose(1,0)\n",
    "    \n",
    "    image = np.ascontiguousarray(image)\n",
    "    mask = np.ascontiguousarray(mask)\n",
    "    return image, mask\n",
    "\n",
    "def do_random_rot90(image, mask):\n",
    "    r = np.random.choice([\n",
    "        0,\n",
    "        cv2.ROTATE_90_CLOCKWISE,\n",
    "        cv2.ROTATE_90_COUNTERCLOCKWISE,\n",
    "        cv2.ROTATE_180,\n",
    "    ])\n",
    "    if r==0:\n",
    "        return image, mask\n",
    "    else:\n",
    "        image = cv2.rotate(image, r)\n",
    "        mask = cv2.rotate(mask, r)\n",
    "        return image, mask\n",
    "    \n",
    "def do_random_contast(image, mask, mag=0.3):\n",
    "    alpha = 1 + random.uniform(-1,1)*mag\n",
    "    image = image * alpha\n",
    "    image = np.clip(image,0,1)\n",
    "    return image, mask\n",
    "\n",
    "def do_random_hsv(image, mask, mag=[0.15,0.25,0.25]):\n",
    "    image = (image*255).astype(np.uint8)\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    h = hsv[:, :, 0].astype(np.float32)  # hue\n",
    "    s = hsv[:, :, 1].astype(np.float32)  # saturation\n",
    "    v = hsv[:, :, 2].astype(np.float32)  # value\n",
    "    h = (h*(1 + random.uniform(-1,1)*mag[0]))%180\n",
    "    s =  s*(1 + random.uniform(-1,1)*mag[1])\n",
    "    v =  v*(1 + random.uniform(-1,1)*mag[2])\n",
    "\n",
    "    hsv[:, :, 0] = np.clip(h,0,180).astype(np.uint8)\n",
    "    hsv[:, :, 1] = np.clip(s,0,255).astype(np.uint8)\n",
    "    hsv[:, :, 2] = np.clip(v,0,255).astype(np.uint8)\n",
    "    image = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "    image = image.astype(np.float32)/255\n",
    "    return image, mask\n",
    "\n",
    "def do_random_noise(image, mask, mag=0.1):\n",
    "    height, width = image.shape[:2]\n",
    "    noise = np.random.uniform(-1,1, (height, width,1))*mag\n",
    "    image = image + noise\n",
    "    image = np.clip(image,0,1)\n",
    "    return image, mask\n",
    "\n",
    "def do_random_rotate_scale(image, mask, angle=30, scale=[0.8,1.2] ):\n",
    "    angle = np.random.uniform(-angle, angle)\n",
    "    scale = np.random.uniform(*scale) if scale is not None else 1\n",
    "    \n",
    "    height, width = image.shape[:2]\n",
    "    center = (height // 2, width // 2)\n",
    "    \n",
    "    transform = cv2.getRotationMatrix2D(center, angle, scale)\n",
    "    image = cv2.warpAffine( image, transform, (width, height), flags=cv2.INTER_LINEAR,\n",
    "                            borderMode=cv2.BORDER_CONSTANT, borderValue=(0,0,0))\n",
    "    mask  = cv2.warpAffine( mask, transform, (width, height), flags=cv2.INTER_LINEAR,\n",
    "                            borderMode=cv2.BORDER_CONSTANT, borderValue=0)\n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_tensor(image, mode='bgr'): #image mode\n",
    "    if mode=='bgr':\n",
    "        image = image[:,:,::-1]\n",
    "    x = image\n",
    "    x = x.transpose(2,0,1)\n",
    "    x = np.ascontiguousarray(x)\n",
    "    x = torch.tensor(x, dtype = torch.float)\n",
    "    return x\n",
    "\n",
    "def mask_to_tensor(mask):\n",
    "    x = mask\n",
    "    #x = x.transpose(2, 0, 1)\n",
    "    x = torch.tensor(x, dtype = torch.float)\n",
    "    return x\n",
    "\n",
    "\n",
    "class RGB(nn.Module):\n",
    "    IMAGE_RGB_MEAN = [0.485, 0.456, 0.406] #[0.5, 0.5, 0.5]\n",
    "    IMAGE_RGB_STD  = [0.229, 0.224, 0.225] #[0.5, 0.5, 0.5]\n",
    "\n",
    "    def __init__(self,):\n",
    "        super(RGB, self).__init__()\n",
    "        self.register_buffer('mean', torch.zeros(1,3,1,1))\n",
    "        self.register_buffer('std', torch.ones(1,3,1,1))\n",
    "        self.mean.data = torch.FloatTensor(self.IMAGE_RGB_MEAN).view(self.mean.shape)\n",
    "        self.std.data = torch.FloatTensor(self.IMAGE_RGB_STD).view(self.std.shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x-self.mean)/self.std\n",
    "        return x\n",
    "\n",
    "def message(mode='print'):\n",
    "    asterisk = ' '\n",
    "    if mode==('print'):\n",
    "        loss = batch_loss\n",
    "    if mode==('log'):\n",
    "        loss = train_loss\n",
    "        if (iteration % iter_save == 0): asterisk = '*'\n",
    "\n",
    "    text = \\\n",
    "        ('%0.2e   %08d%s %6.2f | '%(rate, iteration, asterisk, epoch,)).replace('e-0','e-').replace('e+0','e+') + \\\n",
    "        '%4.3f  %4.3f   | '%(*valid_loss,) + \\\n",
    "        '%4.3f   | '%(loss) + \\\n",
    "        '%s' % ((time.time() - start_timer))\n",
    "\n",
    "    return text\n",
    "\n",
    "def rle_decode(mask_rle, shape):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape).T\n",
    "\n",
    "\n",
    "# ref.: https://www.kaggle.com/stainsby/fast-tested-rle\n",
    "def rle_encode(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = img.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "\n",
    "\n",
    "def get_learning_rate(optimizer):\n",
    "    return optimizer.param_groups[0]['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 512 #768 #512 # 256 #changed\n",
    "\n",
    "class HubmapDataset(Dataset):\n",
    "    def __init__(self, df, augment=None):\n",
    "\n",
    "        self.df = df\n",
    "        self.augment = augment\n",
    "        self.length = len(self.df)\n",
    "        self.organ_to_label = {'kidney' : 0,\n",
    "                               'prostate' : 1,\n",
    "                               'largeintestine' : 2,\n",
    "                               'spleen' : 3,\n",
    "                               'lung' : 4}\n",
    "\n",
    "    def __str__(self):\n",
    "        string = ''\n",
    "        string += '\\tlen = %d\\n' % len(self)\n",
    "\n",
    "        d = self.df.organ.value_counts().to_dict()\n",
    "        for k in ['kidney', 'prostate', 'largeintestine', 'spleen', 'lung']:\n",
    "            string +=  '%24s %3d (%0.3f) \\n'%(k, d.get(k,0), d.get(k,0)/len(self.df))\n",
    "        return string\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        d = self.df.iloc[index]\n",
    "        img_height = self.df.loc[index, 'img_height']\n",
    "        img_width = self.df.loc[index, 'img_width']\n",
    "        organ = self.organ_to_label[d.organ]\n",
    "\n",
    "        image = cv2.cvtColor(tifffile.imread(os.path.join(TRAIN, f'{d.id}.tiff')), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        rle_mask = self.df.loc[index, 'rle']\n",
    "        mask = rle_decode(rle_mask, (img_height, img_width))\n",
    "        #mask = cv2.cvtColor(mask, cv2.IMREAD_GRAYSCALE)\n",
    "        #mask = cv2.imread(os.path.join(MASKS,fname),cv2.IMREAD_GRAYSCALE)\n",
    "        mask = np.expand_dims(mask, axis = 2)\n",
    "        #print(mask.shape)\n",
    "        \n",
    "        image = image.astype(np.float32)/255\n",
    "        #mask  = mask.astype(np.float32)/255\n",
    "        mask = mask.astype(np.float32)\n",
    "\n",
    "        s = d.pixel_size/0.4 * (image_size/3000)\n",
    "        image = cv2.resize(image,dsize=(image_size,image_size),interpolation=cv2.INTER_LINEAR)\n",
    "        mask  = cv2.resize(mask, dsize=(image_size,image_size),interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        if self.augment is not None:\n",
    "            image, mask = self.augment(image, mask, organ)\n",
    "\n",
    "\n",
    "        r ={}\n",
    "        r[\"id\"] = d.id\n",
    "        r['index']= index\n",
    "        r['organ'] = torch.tensor([organ], dtype=torch.long)\n",
    "        r['image'] = image_to_tensor(image)\n",
    "        r['mask' ] = mask_to_tensor(mask)\n",
    "        return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    r\"\"\" Image to Patch Embedding\n",
    "\n",
    "    Args:\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 patch_size=4,\n",
    "                 in_chans=3,\n",
    "                 embed_dim=96,\n",
    "                 norm_layer=None\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        # padding\n",
    "        if W % self.patch_size[1] != 0:\n",
    "            x = F.pad(x, (0, self.patch_size[1] - W % self.patch_size[1]))\n",
    "        if H % self.patch_size[0] != 0:\n",
    "            x = F.pad(x, (0, 0, 0, self.patch_size[0] - H % self.patch_size[0]))\n",
    "\n",
    "\n",
    "        x = self.proj(x)  # B C Wh Ww\n",
    "        if self.norm is not None:\n",
    "            Wh, Ww = x.size(2), x.size(3)\n",
    "            x = x.flatten(2).transpose(1, 2)\n",
    "            x = self.norm(x)\n",
    "            x = x.transpose(1, 2).view(-1, self.embed_dim, Wh, Ww)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class PatchMerging(nn.Module):\n",
    "    r\"\"\" Patch Merging Layer.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input feature, tensor size (B, H*W, C).\n",
    "            H, W: Spatial resolution of the input feature.\n",
    "        \"\"\"\n",
    "\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "        # padding\n",
    "        pad_input = (H % 2 == 1) or (W % 2 == 1)\n",
    "        if pad_input:\n",
    "            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B, H/2, W/2, 4*C\n",
    "        x = x.view(B, -1, 4 * C)  # B, H/2*W/2, 4*C\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicLayer(nn.Module):\n",
    "    \"\"\" A basic Swin Transformer layer for one stage.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Local window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "        fused_window_process (bool, optional): If True, use one kernel to fused window shift & window partition for acceleration, similar for the reversed part. Default: False\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        dim,\n",
    "        depth,\n",
    "        num_heads,\n",
    "        window_size,\n",
    "        mlp_ratio=4.,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop=0.,\n",
    "        attn_drop=0.,\n",
    "        drop_path=0.,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        downsample=None,\n",
    "        #use_checkpoint=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = window_size // 2\n",
    "        self.depth = depth\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SwinTransformerBlock(\n",
    "                dim=dim,\n",
    "                num_heads=num_heads,\n",
    "                window_size=window_size,\n",
    "                shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale,\n",
    "                drop=drop,\n",
    "                attn_drop=attn_drop,\n",
    "                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                norm_layer=norm_layer,\n",
    "            )\n",
    "            for i in range(depth)\n",
    "        ])\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input feature, tensor size (B, H*W, C).\n",
    "            H, W: Spatial resolution of the input feature.\n",
    "        \"\"\"\n",
    "\n",
    "        # calculate attention mask for SW-MSA ----\n",
    "        Hp = int(np.ceil(H / self.window_size)) * self.window_size\n",
    "        Wp = int(np.ceil(W / self.window_size)) * self.window_size\n",
    "        img_mask = torch.zeros((1, Hp, Wp, 1), device=x.device)  # 1 Hp Wp 1\n",
    "        h_slices = (slice(0, -self.window_size),\n",
    "                    slice(-self.window_size, -self.shift_size),\n",
    "                    slice(-self.shift_size, None))\n",
    "        w_slices = (slice(0, -self.window_size),\n",
    "                    slice(-self.window_size, -self.shift_size),\n",
    "                    slice(-self.shift_size, None))\n",
    "        cnt = 0\n",
    "        for h in h_slices:\n",
    "            for w in w_slices:\n",
    "                img_mask[:, h, w, :] = cnt\n",
    "                cnt += 1\n",
    "\n",
    "        mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "        #------\n",
    "\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, H, W, attn_mask)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            x_down = self.downsample(x, H, W)\n",
    "            Wh, Ww = (H + 1) // 2, (W + 1) // 2\n",
    "            return x, H, W, x_down, Wh, Ww\n",
    "        else:\n",
    "            return x, H, W, x, H, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerBlock(nn.Module):\n",
    "    r\"\"\" Swin Transformer Block.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resulotion.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        shift_size (int): Shift size for SW-MSA.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "        fused_window_process (bool, optional): If True, use one kernel to fused window shift & window partition for acceleration, similar for the reversed part. Default: False\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        dim,\n",
    "        num_heads,\n",
    "        window_size=7,\n",
    "        shift_size=0,\n",
    "        mlp_ratio=4.,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop=0.,\n",
    "        attn_drop=0.,\n",
    "        drop_path=0.,\n",
    "        act_layer=nn.GELU,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim,\n",
    "            window_size=to_2tuple(self.window_size),\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop,\n",
    "        )\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "\n",
    "    def forward(self, x, H, W, mask_matrix):\n",
    "\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # pad feature maps to multiples of window size\n",
    "        pad_l = pad_t = 0\n",
    "        pad_r = (self.window_size - W % self.window_size) % self.window_size\n",
    "        pad_b = (self.window_size - H % self.window_size) % self.window_size\n",
    "        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n",
    "        _, Hp, Wp, _ = x.shape\n",
    "\n",
    "\n",
    "        # cyclic shift ---\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "            attn_mask = mask_matrix\n",
    "        else:\n",
    "            shifted_x = x\n",
    "            attn_mask = None\n",
    "\n",
    "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n",
    "        attn_windows = self.attn(x_windows, mask=attn_mask)  # nW*B, window_size*window_size, C\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "\n",
    "\n",
    "        # reverse cyclic shift ---\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, Hp, Wp)  # B H' W' C\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "\n",
    "        if pad_r > 0 or pad_b > 0:\n",
    "            x = x[:, :H, :W, :].contiguous()\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, num_heads={self.num_heads}, \" \\\n",
    "               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerV1(nn.Module):\n",
    "    def __init__(self,\n",
    "        pretrain_img_size=224,\n",
    "        patch_size=4,\n",
    "        in_chans=3,\n",
    "        embed_dim=96,\n",
    "        depths=[2, 2, 6, 2],\n",
    "        num_heads=[3, 6, 12, 24],\n",
    "        window_size=7,\n",
    "        mlp_ratio=4.,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.,\n",
    "        attn_drop_rate=0.,\n",
    "        drop_path_rate=0.1,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        patch_norm=True,\n",
    "        out_norm = nn.Identity, #use nn.Identity, nn.BatchNorm2d, LayerNorm2d\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pretrain_img_size = pretrain_img_size\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if patch_norm else None\n",
    "        )\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = np.linspace(0, drop_path_rate, sum(depths)).tolist() # stochastic depth decay rule\n",
    "\n",
    "        # build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(self.num_layers):\n",
    "            layer = BasicLayer(\n",
    "                dim=int(embed_dim * 2 ** i),\n",
    "                depth=depths[i],\n",
    "                num_heads=num_heads[i],\n",
    "                window_size=window_size,\n",
    "                mlp_ratio=self.mlp_ratio,\n",
    "                qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[sum(depths[:i]):sum(depths[:i + 1])],\n",
    "                norm_layer=norm_layer,\n",
    "                downsample=PatchMerging if (i < self.num_layers - 1) else None,\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        #---\n",
    "        # add a norm layer for each output\n",
    "        self.out_norm = nn.ModuleList(\n",
    "            [ out_norm(int(embed_dim * 2 ** i)) for i in range(self.num_layers)]\n",
    "        )\n",
    "\n",
    "        #---\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        Wh, Ww = x.size(2), x.size(3)\n",
    "\n",
    "        #positional encode?\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        outs = []\n",
    "        for i in range(self.num_layers):\n",
    "            x_out, H, W, x, Wh, Ww = self.layers[i](x, Wh, Ww)\n",
    "            out = x_out.view(-1, H, W, int(self.embed_dim * 2 ** i)).permute(0, 3, 1, 2).contiguous()\n",
    "            out = self.out_norm[i](out)\n",
    "            outs.append(out)\n",
    "\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowAttention(nn.Module):\n",
    "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** (-0.5)\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        relative_position_bias = \\\n",
    "            self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], self.num_heads)\n",
    "            # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n",
    "    \n",
    "    \n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (int): window size\n",
    "\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (int): Window size\n",
    "        H (int): Height of image\n",
    "        W (int): Width of image\n",
    "\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3_bn_relu(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution + BN + relu\"\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "        nn.BatchNorm2d(out_planes),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UPerDecoder(nn.Module):\n",
    "    def __init__(self,\n",
    "        in_dim=[256, 512, 1024, 2048],\n",
    "        ppm_pool_scale=[1, 2, 3, 6],\n",
    "        ppm_dim=512,\n",
    "        fpn_out_dim=256\n",
    "    ):\n",
    "        super(UPerDecoder, self).__init__()\n",
    "\n",
    "        # PPM ----\n",
    "        dim = in_dim[-1]\n",
    "        ppm_pooling = []\n",
    "        ppm_conv = []\n",
    "\n",
    "        for scale in ppm_pool_scale:\n",
    "            ppm_pooling.append(\n",
    "                nn.AdaptiveAvgPool2d(scale)\n",
    "            )\n",
    "            ppm_conv.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(dim, ppm_dim, kernel_size=1, bias=False),\n",
    "                    nn.BatchNorm2d(ppm_dim),\n",
    "                    nn.ReLU(inplace=True)\n",
    "                )\n",
    "            )\n",
    "        self.ppm_pooling   = nn.ModuleList(ppm_pooling)\n",
    "        self.ppm_conv      = nn.ModuleList(ppm_conv)\n",
    "        self.ppm_out = conv3x3_bn_relu(dim + len(ppm_pool_scale)*ppm_dim, fpn_out_dim, 1)\n",
    "\n",
    "        # FPN ----\n",
    "        fpn_in = []\n",
    "        for i in range(0, len(in_dim)-1):  # skip the top layer\n",
    "            fpn_in.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_dim[i], fpn_out_dim, kernel_size=1, bias=False),\n",
    "                    nn.BatchNorm2d(fpn_out_dim),\n",
    "                    nn.ReLU(inplace=True)\n",
    "                )\n",
    "            )\n",
    "        self.fpn_in = nn.ModuleList(fpn_in)\n",
    "\n",
    "        fpn_out = []\n",
    "        for i in range(len(in_dim) - 1):  # skip the top layer\n",
    "            fpn_out.append(\n",
    "                conv3x3_bn_relu(fpn_out_dim, fpn_out_dim, 1),\n",
    "            )\n",
    "        self.fpn_out = nn.ModuleList(fpn_out)\n",
    "\n",
    "        self.fpn_fuse = nn.Sequential(\n",
    "            conv3x3_bn_relu(len(in_dim) * fpn_out_dim, fpn_out_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, feature):\n",
    "        f = feature[-1]\n",
    "        pool_shape = f.shape[2:]\n",
    "\n",
    "        ppm_out = [f]\n",
    "        for pool, conv in zip(self.ppm_pooling, self.ppm_conv):\n",
    "            p = pool(f)\n",
    "            p = F.interpolate(p, size=pool_shape, mode='bilinear', align_corners=False)\n",
    "            p = conv(p)\n",
    "            ppm_out.append(p)\n",
    "        ppm_out = torch.cat(ppm_out, 1)\n",
    "        down = self.ppm_out(ppm_out)\n",
    "\n",
    "        fpn_out = [down]\n",
    "        for i in reversed(range(len(feature) - 1)):\n",
    "            lateral = feature[i]\n",
    "            lateral = self.fpn_in[i](lateral) # lateral branch\n",
    "            down = F.interpolate(down, size=lateral.shape[2:], mode='bilinear', align_corners=False) # top-down branch\n",
    "            down = down + lateral\n",
    "            fpn_out.append(self.fpn_out[i](down))\n",
    "\n",
    "        fpn_out.reverse() # [P2 - P5]\n",
    "        fusion_shape = fpn_out[0].shape[2:]\n",
    "        fusion = [fpn_out[0]]\n",
    "        for i in range(1, len(fpn_out)):\n",
    "            fusion.append(\n",
    "                F.interpolate( fpn_out[i], fusion_shape, mode='bilinear', align_corners=False)\n",
    "            )\n",
    "        x = self.fpn_fuse( torch.cat(fusion, 1))\n",
    "\n",
    "        return x, fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm2d(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.bias = nn.Parameter(torch.zeros(dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.mean(1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.eps)\n",
    "        x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "        return x\n",
    "    \n",
    "def criterion_aux_loss(logit, mask):\n",
    "    mask = F.interpolate(mask,size=logit.shape[-2:], mode='nearest')\n",
    "    loss = F.binary_cross_entropy_with_logits(logit,mask)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def load_pretrain( self,):\n",
    "\n",
    "        checkpoint = cfg[self.arch]['checkpoint']\n",
    "        print('loading %s ...'%checkpoint)\n",
    "        checkpoint = torch.load(checkpoint, map_location=lambda storage, loc: storage)['model']\n",
    "        if 0:\n",
    "            skip = ['relative_coords_table','relative_position_index']\n",
    "            filtered={}\n",
    "            for k,v in checkpoint.items():\n",
    "                if any([s in k for s in skip ]): continue\n",
    "                filtered[k]=v\n",
    "            checkpoint = filtered\n",
    "        print(self.encoder.load_state_dict(checkpoint,strict=False))  #True\n",
    "\n",
    "\n",
    "    def __init__( self,):\n",
    "        super(Net, self).__init__()\n",
    "        self.output_type = ['inference']\n",
    "\n",
    "        self.rgb = RGB()\n",
    "        self.arch = 'swin_tiny_patch4_window7_224'\n",
    "\n",
    "        self.encoder = SwinTransformerV1(\n",
    "            ** {**cfg['basic']['swin'], **cfg[self.arch]['swin'],\n",
    "                **{'out_norm' : LayerNorm2d} }\n",
    "        )\n",
    "        encoder_dim =cfg[self.arch]['upernet']['in_channels']\n",
    "        #[96, 192, 384, 768]\n",
    "\n",
    "        self.decoder = UPerDecoder(\n",
    "            in_dim=encoder_dim,\n",
    "            ppm_pool_scale=[1, 2, 3, 6],\n",
    "            ppm_dim=512,\n",
    "            fpn_out_dim=256\n",
    "        )\n",
    "\n",
    "        self.logit = nn.Sequential(\n",
    "            nn.Conv2d(256, 1, kernel_size=1)\n",
    "        )\n",
    "        self.aux = nn.ModuleList([\n",
    "            nn.Conv2d(256, 1, kernel_size=1, padding=0) for i in range(4)\n",
    "        ])\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x = batch['image']\n",
    "        B,C,H,W = x.shape\n",
    "        x = self.rgb(x)\n",
    "        encoder = self.encoder(x)\n",
    "        last, decoder = self.decoder(encoder)\n",
    "        logit = self.logit(last)\n",
    "        logit = F.interpolate(logit, size=None, scale_factor=4, mode='bilinear', align_corners=False)\n",
    "\n",
    "        output = {}\n",
    "        if 'loss' in self.output_type:\n",
    "            output['bce_loss'] = F.binary_cross_entropy_with_logits(logit,batch['mask'])\n",
    "            for i in range(4):\n",
    "                output['aux%d_loss'%i] = criterion_aux_loss(self.aux[i](decoder[i]),batch['mask'])\n",
    "\n",
    "        if 'inference' in self.output_type:\n",
    "            output['probability'] = torch.sigmoid(logit)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_check_net():\n",
    "    batch_size = 2\n",
    "    image_size = 512\n",
    "\n",
    "    #---\n",
    "    batch = {\n",
    "        'image' : torch.from_numpy( np.random.uniform(-1,1,(batch_size,3,image_size,image_size)) ).float(),\n",
    "        'mask'  : torch.from_numpy( np.random.choice(2,(batch_size,1,image_size,image_size)) ).float(),\n",
    "        'organ' : torch.from_numpy( np.random.choice(5,(batch_size)) ).long(),\n",
    "    }\n",
    "    batch = {k:v.cuda() for k,v in batch.items()}\n",
    "\n",
    "    net = Net().cuda()\n",
    "    net.load_pretrain()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            output = net(batch)\n",
    "\n",
    "    print('batch')\n",
    "    for k,v in batch.items():\n",
    "        print('%32s :'%k, v.shape)\n",
    "\n",
    "    print('output')\n",
    "    for k,v in output.items():\n",
    "        if 'loss' not in k:\n",
    "            print('%32s :'%k, v.shape)\n",
    "    for k,v in output.items():\n",
    "        if 'loss' in k:\n",
    "            print('%32s :'%k, v.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    \"\"\" Multilayer perceptron.\"\"\"\n",
    "\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = dict(\n",
    "\n",
    "        #configs/_base_/models/upernet_swin.py\n",
    "        basic = dict(\n",
    "            swin=dict(\n",
    "                embed_dim=96,\n",
    "                depths=[2, 2, 6, 2],\n",
    "                num_heads=[3, 6, 12, 24],\n",
    "                window_size=7,\n",
    "                mlp_ratio=4.,\n",
    "                qkv_bias=True,\n",
    "                qk_scale=None,\n",
    "                drop_rate=0.,\n",
    "                attn_drop_rate=0.,\n",
    "                drop_path_rate=0.3,\n",
    "                ape=False,\n",
    "                patch_norm=True,\n",
    "                out_indices=(0, 1, 2, 3),\n",
    "                use_checkpoint=False\n",
    "            ),\n",
    "\n",
    "        ),\n",
    "\n",
    "        #configs/swin/upernet_swin_tiny_patch4_window7_512x512_160k_ade20k.py\n",
    "        swin_tiny_patch4_window7_224=dict(\n",
    "            checkpoint = pretrain_dir+'/swin_tiny_patch4_window7_224_22k.pth',\n",
    "\n",
    "            swin = dict(\n",
    "                embed_dim=96,\n",
    "                depths=[2, 2, 6, 2],\n",
    "                num_heads=[3, 6, 12, 24],\n",
    "                window_size=7,\n",
    "                ape=False,\n",
    "                drop_path_rate=0.3,\n",
    "                patch_norm=True,\n",
    "                use_checkpoint=False,\n",
    "            ),\n",
    "            upernet=dict(\n",
    "                in_channels=[96, 192, 384, 768],\n",
    "            ),\n",
    "        ),\n",
    "\n",
    "        #/configs/swin/upernet_swin_small_patch4_window7_512x512_160k_ade20k.py\n",
    "        swin_small_patch4_window7_224_22k=dict(\n",
    "            checkpoint = pretrain_dir+'/swin_small_patch4_window7_224_22k.pth',\n",
    "\n",
    "            swin = dict(\n",
    "                embed_dim=96,\n",
    "                depths=[2, 2, 18, 2],\n",
    "                num_heads=[3, 6, 12, 24],\n",
    "                window_size=7,\n",
    "                ape=False,\n",
    "                drop_path_rate=0.3,\n",
    "                patch_norm=True,\n",
    "                use_checkpoint=False\n",
    "            ),\n",
    "            upernet=dict(\n",
    "                in_channels=[96, 192, 384, 768],\n",
    "            ),\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = Net().cuda()\n",
    "# del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_check_net():\n",
    "    batch_size = 2\n",
    "    image_size = 512\n",
    "\n",
    "    #---\n",
    "    batch = {\n",
    "        'image' : torch.from_numpy( np.random.uniform(-1,1,(batch_size,3,image_size,image_size)) ).float(),\n",
    "        'mask'  : torch.from_numpy( np.random.choice(2,(batch_size,1,image_size,image_size)) ).float(),\n",
    "        'organ' : torch.from_numpy( np.random.choice(5,(batch_size)) ).long(),\n",
    "    }\n",
    "    batch = {k:v.cuda() for k,v in batch.items()}\n",
    "    print(batch[\"image\"].shape)\n",
    "    net = load_model(\"/home/lakshita/somusan/hubmap_kaggle/nbs/result/upernet-swin-v1-tiny-aux5-768/fold-0/checkpoint/00026250.model.pth\")\n",
    "    net.load_pretrain()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            output = net(batch)\n",
    "\n",
    "    print('batch')\n",
    "    for k,v in batch.items():\n",
    "        print('%32s :'%k, v.shape)\n",
    "\n",
    "    print('output')\n",
    "    for k,v in output.items():\n",
    "        if 'loss' not in k:\n",
    "            print('%32s :'%k, v.shape)\n",
    "    for k,v in output.items():\n",
    "        if 'loss' in k:\n",
    "            print('%32s :'%k, v.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_check_net()\n",
    "class CFG:\n",
    "    img_size = [512, 512]\n",
    "class HuBMAP_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, labeled=True, transforms=None):\n",
    "        self.df = df\n",
    "        self.labeled = labeled\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.df.loc[index, 'image_path']\n",
    "        img_height = self.df.loc[index, 'img_height']\n",
    "        img_width = self.df.loc[index, 'img_width']\n",
    "        id_ = self.df.loc[index, 'id']\n",
    "        img = cv2.cvtColor(tifffile.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "        img = img.astype(np.float32)/255\n",
    "\n",
    "        if self.labeled:\n",
    "            rle_mask = self.df.loc[index, 'rle']\n",
    "            mask = rle_decode(rle_mask, (img_height, img_width))\n",
    "\n",
    "            if self.transforms:\n",
    "                data = self.transforms(image=img, mask=mask)\n",
    "                img  = data['image']\n",
    "                mask  = data['mask']\n",
    "            \n",
    "            mask = np.expand_dims(mask, axis=0)\n",
    "            img = np.transpose(img, (2, 0, 1))\n",
    "            \n",
    "            return torch.tensor(img), torch.tensor(mask)\n",
    "        \n",
    "        else:\n",
    "            if self.transforms:\n",
    "                data = self.transforms(image=img)\n",
    "                img  = data['image']\n",
    "                \n",
    "            img = np.transpose(img, (2, 0, 1))\n",
    "            \n",
    "            return torch.tensor(img), img_height, img_width, id_\n",
    "\n",
    "data_transforms = {\n",
    "    \"inference\": A.Compose([\n",
    "        A.Resize(*CFG.img_size, interpolation=cv2.INTER_NEAREST),\n",
    "        ], p=1.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_loaders(df):\n",
    "\n",
    "    infer_dataset = HuBMAP_Dataset(df, labeled=False, transforms=data_transforms['inference'])\n",
    "\n",
    "    infer_loader = torch.utils.data.DataLoader(infer_dataset, batch_size=8,\n",
    "                              num_workers=2, shuffle=False, pin_memory=True, drop_last=False)\n",
    "    \n",
    "    return infer_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_encode_less_memory(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    This simplified method requires first and last pixel to be zero\n",
    "    '''\n",
    "    pixels = img.T.flatten()\n",
    "    \n",
    "    # This simplified method requires first and last pixel to be zero\n",
    "    pixels[0] = 0\n",
    "    pixels[-1] = 0\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
    "    runs[1::2] -= runs[::2]\n",
    "    \n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023 2023\n"
     ]
    }
   ],
   "source": [
    "base_path = \"/home/lakshita/somusan/hubmap_kaggle/hubmap_data/hubmap-organ-segmentation/\"\n",
    "df = pd.read_csv(\"/home/lakshita/somusan/hubmap_kaggle/hubmap_data/hubmap-organ-segmentation/test.csv\")\n",
    "df['image_path'] = df['id'].apply(lambda x: os.path.join(base_path, 'test_images', str(x) + '.tiff'))\n",
    "infer_loader = prepare_loaders(df)\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "def load_model(path):\n",
    "    model = Net().cuda()\n",
    "    model.load_state_dict(torch.load(path, map_location=DEVICE)['state_dict'])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "model = load_model(\"/home/lakshita/somusan/hubmap_kaggle/nbs/result/upernet-swin-v1-tiny-aux5-768/fold-0/checkpoint/00029925.model.pth\")\n",
    "\n",
    "pred_ids = []\n",
    "pred_rles = []\n",
    "with torch.no_grad():\n",
    "    for (images, heights, widths, ids) in infer_loader:\n",
    "        \n",
    "        images = images.to(DEVICE)\n",
    "        inf_data = {'image': images}\n",
    "\n",
    "        \n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            output = model(inf_data)\n",
    "\n",
    "        # output = nn.Sigmoid()(output)\n",
    "        msks = (output[\"probability\"].permute((0,2,3,1))>0.5).to(torch.uint8).cpu().detach().numpy()\n",
    "\n",
    "        for idx in range(msks.shape[0]):\n",
    "            height = heights[idx].item()\n",
    "            width = widths[idx].item()\n",
    "            id_ = ids[idx].item()\n",
    "            print(width, height)\n",
    "            \n",
    "            msk = cv2.resize(msks[idx].squeeze(), \n",
    "                             dsize=(width, height), \n",
    "                             interpolation=cv2.INTER_NEAREST)\n",
    "            \n",
    "            # print(msk.shape)\n",
    "            msk = msk.T\n",
    "            rle = rle_encode_less_memory(msk)\n",
    "            pred_rles.append(rle)\n",
    "            pred_ids.append(id_)\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f19c7b9f9b0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def rle_decode(mask_rle, shape, color=1):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.float32)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo : hi] = color\n",
    "    return img.reshape(shape)\n",
    "\n",
    "plt.imshow(rle_decode(pred_rles[0], (2023,2023)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(msks.reshape(512,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 29925 -> 0.706  0.121 \n",
    "# 17115 -> 0.700  0.116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run_check_net()\n",
    "# images = cv2.cvtColor(tifffile.imread(\"/home/lakshita/somusan/hubmap_kaggle/hubmap_data/hubmap-organ-segmentation/test_images/10078.tiff\"), cv2.COLOR_BGR2RGB)\n",
    "# images = images.astype(np.float32)/255\n",
    "# images = cv2.resize(images,dsize=(image_size,image_size),interpolation=cv2.INTER_LINEAR)\n",
    "# h, w, c = images.shape[0],images.shape[1],images.shape[2]\n",
    "# images = torch.from_numpy(images.reshape(1, h,w,c)).permute(0,3,1,2)\n",
    "# image = images.to(DEVICE)\n",
    "\n",
    "# test_data = {'image': image}\n",
    "\n",
    "# print(images.shape)\n",
    "# with torch.no_grad():\n",
    "#         with torch.cuda.amp.autocast(enabled=True):\n",
    "#             output = model(test_data)\n",
    "#             # output = nn.Sigmoid()(output)\n",
    "#             print(output[\"probability\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(nn.Sigmoid(output[\"probability\"]).reshape(512,512).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Sigmoid(output[\"probability\"].reshape(512,512).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msks = (output[\"probability\"].permute((0,2,3,1))>0.5).to(torch.uint8).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(msks.reshape(512,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "21a2a557654fc1676068684031cf9bb9dfda94e124d3623f4e9c9ed764d794ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
