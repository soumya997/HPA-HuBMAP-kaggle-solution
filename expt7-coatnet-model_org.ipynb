{"cells":[{"cell_type":"markdown","metadata":{"papermill":{"duration":0.010745,"end_time":"2022-08-24T15:49:13.730757","exception":false,"start_time":"2022-08-24T15:49:13.720012","status":"completed"},"tags":[]},"source":["<div style=\"height:200px;width:100%;margin: 0;\">\n","    <img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/34547/logos/header.png?t=2022-02-15-22-37-27\" style=\"width:100%;\" />\n","</div>"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.009077,"end_time":"2022-08-24T15:49:13.749342","exception":false,"start_time":"2022-08-24T15:49:13.740265","status":"completed"},"tags":[]},"source":["<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:#c3448b; background:#efe9e9; border:1px dashed #efe50b;\" role=\"tab\" aria-controls=\"credits\"><center>Credits</center></h3>"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.009102,"end_time":"2022-08-24T15:49:13.767721","exception":false,"start_time":"2022-08-24T15:49:13.758619","status":"completed"},"tags":[]},"source":["This is the reverse engineering of the [hengck23 discussion](https://www.kaggle.com/code/hengck23/lb-0-75-variable-size-swin-transformer-v1-and-v2).<br>\n","Please upvote both discussion/notebooks if you are planning to use Swin Transformers or any part of the code.\n","\n","**hengck23 owner Disclaimer**\n","\n","[1] the code is taken from a larger project and is by no means complete. It will has missing import modules, etc. But these are trival functions that you can ignore or fill in yourself.\n","\n","[2] you are free to use, modify the code for your own notebook or submission"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.00937,"end_time":"2022-08-24T15:49:13.787671","exception":false,"start_time":"2022-08-24T15:49:13.778301","status":"completed"},"tags":[]},"source":["<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:#c3448b; background:#efe9e9; border:1px dashed #efe50b;\" role=\"tab\" aria-controls=\"imports\"><center>Imports</center></h3>"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-08-30T17:33:34.483925Z","iopub.status.busy":"2022-08-30T17:33:34.482915Z","iopub.status.idle":"2022-08-30T17:33:58.186612Z","shell.execute_reply":"2022-08-30T17:33:58.185298Z","shell.execute_reply.started":"2022-08-30T17:33:34.483787Z"},"trusted":true},"outputs":[],"source":["!cp -r ../input/pytorch-segmentation-models-lib/ ./\n","!pip config set global.disable-pip-version-check true\n","!pip install -q ./pytorch-segmentation-models-lib/timm-0.4.12-py3-none-any.whl\n","\n","\n","!cp -r ../input/einops-041-wheel/ ./\n","!pip config set global.disable-pip-version-check true\n","!pip install -q ../input/einops-041-wheel/einops-0.4.1-py3-none-any.whl"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-08-30T17:33:58.189989Z","iopub.status.busy":"2022-08-30T17:33:58.189490Z","iopub.status.idle":"2022-08-30T17:33:59.493887Z","shell.execute_reply":"2022-08-30T17:33:59.492911Z","shell.execute_reply.started":"2022-08-30T17:33:58.189945Z"},"papermill":{"duration":3.967121,"end_time":"2022-08-24T15:49:17.764143","exception":false,"start_time":"2022-08-24T15:49:13.797022","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import os\n","import cv2\n","import time\n","import random\n","\n","import torch\n","from torch import nn\n","import torch.cuda.amp as amp\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torch.utils.data import RandomSampler \n","from torch.utils.data import SequentialSampler\n","import torch.nn.functional as F\n","from torchmetrics.functional import dice_score\n","from torch.optim.lr_scheduler import StepLR\n","import tifffile\n","\n","is_amp = True\n","import logging\n","import pandas as pd\n","from sklearn.model_selection import KFold\n","\n","import numpy as np\n","from itertools import repeat\n","import collections.abc\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.00923,"end_time":"2022-08-24T15:49:17.783031","exception":false,"start_time":"2022-08-24T15:49:17.773801","status":"completed"},"tags":[]},"source":["<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:#c3448b; background:#efe9e9; border:1px dashed #efe50b;\" role=\"tab\" aria-controls=\"paths\"><center>Paths</center></h3>"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-08-30T17:33:59.496258Z","iopub.status.busy":"2022-08-30T17:33:59.495229Z","iopub.status.idle":"2022-08-30T17:34:01.505026Z","shell.execute_reply":"2022-08-30T17:34:01.503871Z","shell.execute_reply.started":"2022-08-30T17:33:59.496209Z"},"papermill":{"duration":1.96185,"end_time":"2022-08-24T15:49:19.754141","exception":false,"start_time":"2022-08-24T15:49:17.792291","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["!mkdir /kaggle/working/result\n","!mkdir /kaggle/working/checkpoint\n","\n","root_dir = '/kaggle/working/'\n","pretrain_dir = '/kaggle/input/swin-tiny-small-22k-pretrained/'\n","\n","TRAIN = '../input/hubmap-organ-segmentation/train_images'\n","MASKS = '../input/hubmap-2022-256x256/masks/'\n","LABELS = '../input/hubmap-organ-segmentation/train.csv'"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.00939,"end_time":"2022-08-24T15:49:19.773612","exception":false,"start_time":"2022-08-24T15:49:19.764222","status":"completed"},"tags":[]},"source":["<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:#c3448b; background:#efe9e9; border:1px dashed #efe50b;\" role=\"tab\" aria-controls=\"additionals\"><center>Additionals</center></h3>"]},{"cell_type":"code","execution_count":4,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-08-30T17:34:01.508484Z","iopub.status.busy":"2022-08-30T17:34:01.508075Z","iopub.status.idle":"2022-08-30T17:34:01.531895Z","shell.execute_reply":"2022-08-30T17:34:01.530911Z","shell.execute_reply.started":"2022-08-30T17:34:01.508441Z"},"papermill":{"duration":0.037266,"end_time":"2022-08-24T15:49:19.820442","exception":false,"start_time":"2022-08-24T15:49:19.783176","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def image_to_tensor(image, mode='bgr'): #image mode\n","    if mode=='bgr':\n","        image = image[:,:,::-1]\n","    x = image\n","    x = x.transpose(2,0,1)\n","    x = np.ascontiguousarray(x)\n","    x = torch.tensor(x, dtype=torch.float)\n","    return x\n","\n","\n","def mask_to_tensor(mask):\n","    x = mask\n","    x = torch.tensor(x, dtype=torch.float)\n","    return x\n","\n","\n","tensor_list = ['mask', 'image', 'organ']\n","\n","def null_collate(batch):\n","    d = {}\n","    key = batch[0].keys()\n","    for k in key:\n","        v = [b[k] for b in batch]\n","        if k in tensor_list:\n","            v = torch.stack(v)\n","        d[k] = v\n","\n","    d['mask'] = d['mask'].unsqueeze(1)\n","    d['organ'] = d['organ'].reshape(-1)\n","    return d\n","\n","\n","def _ntuple(n):\n","    def parse(x):\n","        if isinstance(x, collections.abc.Iterable):\n","            return x\n","        return tuple(repeat(x, n))\n","    return parse\n","\n","\n","to_2tuple = _ntuple(2)\n","\n","\n","def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n","    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n","    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n","    def norm_cdf(x):\n","        # Computes standard normal cumulative distribution function\n","        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n","\n","    if (mean < a - 2 * std) or (mean > b + 2 * std):\n","        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n","                      \"The distribution of values may be incorrect.\",\n","                      stacklevel=2)\n","\n","        \n","def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n","    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n","\n","\n","def drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):\n","    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n","    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n","    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n","    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n","    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n","    'survival rate' as the argument.\n","    \"\"\"\n","    if drop_prob == 0. or not training:\n","        return x\n","    keep_prob = 1 - drop_prob\n","    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n","    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n","    if keep_prob > 0.0 and scale_by_keep:\n","        random_tensor.div_(keep_prob)\n","    return x * random_tensor\n","\n","\n","class DropPath(nn.Module):\n","    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n","    \"\"\"\n","    def __init__(self, drop_prob: float = 0., scale_by_keep: bool = True):\n","        super(DropPath, self).__init__()\n","        self.drop_prob = drop_prob\n","        self.scale_by_keep = scale_by_keep\n","\n","    def forward(self, x):\n","        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n","\n","    def extra_repr(self):\n","        return f'drop_prob={round(self.drop_prob,3):0.3f}'\n","    \n","    \n","class RGB(nn.Module):\n","    IMAGE_RGB_MEAN = [0.485, 0.456, 0.406] #[0.5, 0.5, 0.5]\n","    IMAGE_RGB_STD  = [0.229, 0.224, 0.225] #[0.5, 0.5, 0.5]\n","\n","    def __init__(self,):\n","        super(RGB, self).__init__()\n","        self.register_buffer('mean', torch.zeros(1,3,1,1))\n","        self.register_buffer('std', torch.ones(1,3,1,1))\n","        self.mean.data = torch.FloatTensor(self.IMAGE_RGB_MEAN).view(self.mean.shape)\n","        self.std.data = torch.FloatTensor(self.IMAGE_RGB_STD).view(self.std.shape)\n","\n","    def forward(self, x):\n","        x = (x-self.mean)/self.std\n","        return x\n","    \n","    \n","def message(mode='print'):\n","    asterisk = ' '\n","    if mode==('print'):\n","        loss = batch_loss\n","    if mode==('log'):\n","        loss = train_loss\n","        if (iteration % iter_save == 0): asterisk = '*'\n","\n","    text = \\\n","        ('%0.2e   %08d%s %6.2f | '%(rate, iteration, asterisk, epoch,)).replace('e-0','e-').replace('e+0','e+') + \\\n","        '%4.3f  %4.3f  %4.4f  %4.3f   | '%(*valid_loss,) + \\\n","        '%4.3f  %4.3f   | '%(*loss,) + \\\n","        '%s' % ((time.time() - start_timer))\n","\n","    return text"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.009463,"end_time":"2022-08-24T15:49:19.839590","exception":false,"start_time":"2022-08-24T15:49:19.830127","status":"completed"},"tags":[]},"source":["<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:#c3448b; background:#efe9e9; border:1px dashed #efe50b;\" role=\"tab\" aria-controls=\"randoms\"><center>Random choice</center></h3>"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-08-30T17:34:01.537159Z","iopub.status.busy":"2022-08-30T17:34:01.536791Z","iopub.status.idle":"2022-08-30T17:34:01.546744Z","shell.execute_reply":"2022-08-30T17:34:01.545752Z","shell.execute_reply.started":"2022-08-30T17:34:01.537112Z"},"papermill":{"duration":0.021247,"end_time":"2022-08-24T15:49:19.870806","exception":false,"start_time":"2022-08-24T15:49:19.849559","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def valid_augment5(image, mask, organ):\n","    #image, mask  = do_crop(image, mask, image_size, xy=(None,None))\n","    return image, mask\n","\n","def train_augment5b(image, mask, organ):\n","    image, mask = do_random_flip(image, mask)\n","    image, mask = do_random_rot90(image, mask)\n","\n","    for fn in np.random.choice([\n","        lambda image, mask: (image, mask),\n","        lambda image, mask: do_random_noise(image, mask, mag=0.1),\n","        lambda image, mask: do_random_contast(image, mask, mag=0.40),\n","        lambda image, mask: do_random_hsv(image, mask, mag=[0.40, 0.40, 0])\n","    ], 2): image, mask = fn(image, mask)\n","\n","    for fn in np.random.choice([\n","        lambda image, mask: (image, mask),\n","        lambda image, mask: do_random_rotate_scale(image, mask, angle=45, scale=[0.50, 2.0]),\n","    ], 1): image, mask = fn(image, mask)\n","\n","    return image, mask"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.009878,"end_time":"2022-08-24T15:49:19.890372","exception":false,"start_time":"2022-08-24T15:49:19.880494","status":"completed"},"tags":[]},"source":["<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:#c3448b; background:#efe9e9; border:1px dashed #efe50b;\" role=\"tab\" aria-controls=\"augmentations\"><center>Augmentations</center></h3>"]},{"cell_type":"code","execution_count":6,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-08-30T17:34:01.549139Z","iopub.status.busy":"2022-08-30T17:34:01.548575Z","iopub.status.idle":"2022-08-30T17:34:01.568232Z","shell.execute_reply":"2022-08-30T17:34:01.566941Z","shell.execute_reply.started":"2022-08-30T17:34:01.549102Z"},"papermill":{"duration":0.030946,"end_time":"2022-08-24T15:49:19.930738","exception":false,"start_time":"2022-08-24T15:49:19.899792","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def do_random_flip(image, mask):\n","    if np.random.rand()>0.5:\n","        image = cv2.flip(image,0)\n","        mask = cv2.flip(mask,0)\n","    if np.random.rand()>0.5:\n","        image = cv2.flip(image,1)\n","        mask = cv2.flip(mask,1)\n","    if np.random.rand()>0.5:\n","        image = image.transpose(1,0,2)\n","        mask = mask.transpose(1,0)\n","    \n","    image = np.ascontiguousarray(image)\n","    mask = np.ascontiguousarray(mask)\n","    return image, mask\n","\n","def do_random_rot90(image, mask):\n","    r = np.random.choice([\n","        0,\n","        cv2.ROTATE_90_CLOCKWISE,\n","        cv2.ROTATE_90_COUNTERCLOCKWISE,\n","        cv2.ROTATE_180,\n","    ])\n","    if r==0:\n","        return image, mask\n","    else:\n","        image = cv2.rotate(image, r)\n","        mask = cv2.rotate(mask, r)\n","        return image, mask\n","    \n","def do_random_contast(image, mask, mag=0.3):\n","    alpha = 1 + random.uniform(-1,1)*mag\n","    image = image * alpha\n","    image = np.clip(image,0,1)\n","    return image, mask\n","\n","def do_random_hsv(image, mask, mag=[0.15,0.25,0.25]):\n","    image = (image*255).astype(np.uint8)\n","    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n","\n","    h = hsv[:, :, 0].astype(np.float32)  # hue\n","    s = hsv[:, :, 1].astype(np.float32)  # saturation\n","    v = hsv[:, :, 2].astype(np.float32)  # value\n","    h = (h*(1 + random.uniform(-1,1)*mag[0]))%180\n","    s =  s*(1 + random.uniform(-1,1)*mag[1])\n","    v =  v*(1 + random.uniform(-1,1)*mag[2])\n","\n","    hsv[:, :, 0] = np.clip(h,0,180).astype(np.uint8)\n","    hsv[:, :, 1] = np.clip(s,0,255).astype(np.uint8)\n","    hsv[:, :, 2] = np.clip(v,0,255).astype(np.uint8)\n","    image = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n","    image = image.astype(np.float32)/255\n","    return image, mask\n","\n","def do_random_noise(image, mask, mag=0.1):\n","    height, width = image.shape[:2]\n","    noise = np.random.uniform(-1,1, (height, width,1))*mag\n","    image = image + noise\n","    image = np.clip(image,0,1)\n","    return image, mask\n","\n","def do_random_rotate_scale(image, mask, angle=30, scale=[0.8,1.2] ):\n","    angle = np.random.uniform(-angle, angle)\n","    scale = np.random.uniform(*scale) if scale is not None else 1\n","    \n","    height, width = image.shape[:2]\n","    center = (height // 2, width // 2)\n","    \n","    transform = cv2.getRotationMatrix2D(center, angle, scale)\n","    image = cv2.warpAffine( image, transform, (width, height), flags=cv2.INTER_LINEAR,\n","                            borderMode=cv2.BORDER_CONSTANT, borderValue=(0,0,0))\n","    mask  = cv2.warpAffine( mask, transform, (width, height), flags=cv2.INTER_LINEAR,\n","                            borderMode=cv2.BORDER_CONSTANT, borderValue=0)\n","    return image, mask"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.009393,"end_time":"2022-08-24T15:49:19.949637","exception":false,"start_time":"2022-08-24T15:49:19.940244","status":"completed"},"tags":[]},"source":["<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:#c3448b; background:#efe9e9; border:1px dashed #efe50b;\" role=\"tab\" aria-controls=\"dataset\"><center>Dataset</center></h3>"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-08-30T17:34:01.572006Z","iopub.status.busy":"2022-08-30T17:34:01.571706Z","iopub.status.idle":"2022-08-30T17:34:01.583068Z","shell.execute_reply":"2022-08-30T17:34:01.582102Z","shell.execute_reply.started":"2022-08-30T17:34:01.571980Z"},"papermill":{"duration":0.021538,"end_time":"2022-08-24T15:49:19.980585","exception":false,"start_time":"2022-08-24T15:49:19.959047","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def rle_decode(mask_rle, shape):\n","    '''\n","    mask_rle: run-length as string formated (start length)\n","    shape: (height,width) of array to return \n","    Returns numpy array, 1 - mask, 0 - background\n","\n","    '''\n","    s = mask_rle.split()\n","    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n","    starts -= 1\n","    ends = starts + lengths\n","    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n","    for lo, hi in zip(starts, ends):\n","        img[lo:hi] = 1\n","    return img.reshape(shape).T  # Needed to align to RLE direction\n","\n","\n","# ref.: https://www.kaggle.com/stainsby/fast-tested-rle\n","def rle_encode(img):\n","    '''\n","    img: numpy array, 1 - mask, 0 - background\n","    Returns run length as string formated\n","    '''\n","    pixels = img.flatten()\n","    pixels = np.concatenate([[0], pixels, [0]])\n","    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n","    runs[1::2] -= runs[::2]\n","    return ' '.join(str(x) for x in runs)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-08-30T17:34:01.585815Z","iopub.status.busy":"2022-08-30T17:34:01.585028Z","iopub.status.idle":"2022-08-30T17:34:01.600349Z","shell.execute_reply":"2022-08-30T17:34:01.599368Z","shell.execute_reply.started":"2022-08-30T17:34:01.585780Z"},"papermill":{"duration":0.025093,"end_time":"2022-08-24T15:49:20.015057","exception":false,"start_time":"2022-08-24T15:49:19.989964","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["image_size = 768\n","\n","class HubmapDataset(Dataset):\n","    def __init__(self, df, augment=None):\n","\n","        self.df = df\n","        self.augment = augment\n","        self.length = len(self.df)\n","#         ids = pd.read_csv(LABELS).id.astype(str).values\n","#         self.fnames = [fname for fname in os.listdir(TRAIN) if fname.split('_')[0] in ids]\n","        self.organ_to_label = {'kidney' : 0,\n","                               'prostate' : 1,\n","                               'largeintestine' : 2,\n","                               'spleen' : 3,\n","                               'lung' : 4}\n","\n","    def __str__(self):\n","        string = ''\n","        string += '\\tlen = %d\\n' % len(self)\n","\n","        d = self.df.organ.value_counts().to_dict()\n","        for k in ['kidney', 'prostate', 'largeintestine', 'spleen', 'lung']:\n","            string +=  '%24s %3d (%0.3f) \\n'%(k,d.get(k,0),d.get(k,0)/len(self.df))\n","        return string\n","\n","    def __len__(self):\n","        return self.length\n","\n","    def __getitem__(self, index):\n","        #fname = self.fnames[index]\n","        #img_id = self.df.iloc[index, 'id']\n","        d = self.df.iloc[index]\n","        img_height = self.df.loc[index, 'img_height']\n","        img_width = self.df.loc[index, 'img_width']\n","        organ = self.organ_to_label[d.organ]\n","\n","        #image = cv2.cvtColor(tifffile.imread(os.path.join(TRAIN, f'{d.id}.tiff')), cv2.COLOR_BGR2RGB)\n","        \n","        image = cv2.imread(os.path.join('../input/hubmap-22-aug-pixel-size', f'{d.id}.png'), cv2.COLOR_BGR2RGB)\n","        \n","        rle_mask = self.df.loc[index, 'rle']\n","        mask = rle_decode(rle_mask, (img_height, img_width))\n","        #mask = cv2.cvtColor(mask, cv2.IMREAD_GRAYSCALE)\n","        #mask = cv2.imread(os.path.join(MASKS,fname),cv2.IMREAD_GRAYSCALE)\n","        mask = np.expand_dims(mask, axis = 2)\n","        #print(mask.shape)\n","        \n","        image = image.astype(np.float32)/255\n","        #mask  = mask.astype(np.float32)/255\n","        mask = mask.astype(np.float32)\n","\n","        s = d.pixel_size/0.4 * (image_size/3000)\n","        image = cv2.resize(image,dsize=(image_size,image_size),interpolation=cv2.INTER_LINEAR)\n","        mask  = cv2.resize(mask, dsize=(image_size,image_size),interpolation=cv2.INTER_LINEAR)\n","\n","        if self.augment is not None:\n","            image, mask = self.augment(image, mask, organ)\n","\n","\n","        r ={}\n","        r['index']= index\n","        r['organ'] = torch.tensor([organ], dtype=torch.long)\n","        r['image'] = image_to_tensor(image)\n","        r['mask' ] = mask_to_tensor(mask)\n","        return r"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-08-30T17:34:01.602466Z","iopub.status.busy":"2022-08-30T17:34:01.601677Z","iopub.status.idle":"2022-08-30T17:34:01.613217Z","shell.execute_reply":"2022-08-30T17:34:01.612169Z","shell.execute_reply.started":"2022-08-30T17:34:01.602424Z"},"papermill":{"duration":0.017444,"end_time":"2022-08-24T15:49:20.042179","exception":false,"start_time":"2022-08-24T15:49:20.024735","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# df = pd.read_csv('../input/hubmap-organ-segmentation/train.csv')\n","# ds = HubmapDataset(df)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.009501,"end_time":"2022-08-24T15:49:20.061236","exception":false,"start_time":"2022-08-24T15:49:20.051735","status":"completed"},"tags":[]},"source":["<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:#c3448b; background:#efe9e9; border:1px dashed #efe50b;\" role=\"tab\" aria-controls=\"patching\"><center>Image Patching</center></h3>"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-08-30T17:34:01.616067Z","iopub.status.busy":"2022-08-30T17:34:01.615361Z","iopub.status.idle":"2022-08-30T17:34:02.153138Z","shell.execute_reply":"2022-08-30T17:34:02.151746Z","shell.execute_reply.started":"2022-08-30T17:34:01.616029Z"},"trusted":true},"outputs":[],"source":["import pdb\n","\n","class MixUpSample(nn.Module):\n","\tdef __init__( self, scale_factor=2):\n","\t\tsuper().__init__()\n","\t\tself.mixing = nn.Parameter(torch.tensor(0.5))\n","\t\tself.scale_factor = scale_factor\n","\t\n","\tdef forward(self, x):\n","\t\tx = self.mixing *F.interpolate(x, scale_factor=self.scale_factor, mode='bilinear', align_corners=False) \\\n","\t\t\t+ (1-self.mixing )*F.interpolate(x, scale_factor=self.scale_factor, mode='nearest')\n","\t\treturn x\n","\n","#https://github.com/lhoyer/DAFormer/blob/master/mmseg/models/decode_heads/daformer_head.py\n","def Conv2dBnReLU(in_channel, out_channel, kernel_size=3, padding=1,stride=1, dilation=1):\n","\treturn nn.Sequential(\n","\t\tnn.Conv2d(in_channel, out_channel, kernel_size=kernel_size, padding=padding, stride=stride, dilation=dilation, bias=False),\n","\t\tnn.BatchNorm2d(out_channel),\n","\t\tnn.ReLU(inplace=True),\n","\t)\n","\n","class ASPP(nn.Module):\n","\t\n","\tdef __init__(self,\n","\t\t\t\t in_channel,\n","\t\t\t\t channel,\n","\t\t\t\t dilation,\n","\t\t\t\t ):\n","\t\tsuper(ASPP, self).__init__()\n","\t\t\n","\t\tself.conv = nn.ModuleList()\n","\t\tfor d in dilation:\n","\t\t\tself.conv.append(\n","\t\t\t\tConv2dBnReLU(\n","\t\t\t\t\tin_channel,\n","\t\t\t\t\tchannel,\n","\t\t\t\t\tkernel_size=1 if d == 1 else 3,\n","\t\t\t\t\tdilation=d,\n","\t\t\t\t\tpadding=0 if d == 1 else d,\n","\t\t\t\t)\n","\t\t\t)\n","\t\t\n","\t\tself.out = Conv2dBnReLU(\n","\t\t\tlen(dilation) * channel,\n","\t\t\tchannel,\n","\t\t\tkernel_size=3,\n","\t\t\tpadding=1,\n","\t\t\t)\n","\t\n","\tdef forward(self, x):\n","\t\taspp = []\n","\t\tfor conv in self.conv:\n","\t\t\taspp.append(conv(x))\n","\t\taspp = torch.cat(aspp, dim=1)\n","\t\tout = self.out(aspp)\n","\t\treturn out\n","\n","#DepthwiseSeparable\n","class DSConv2d(nn.Module):\n","\tdef __init__(self,\n","\t\t\t\t in_channel,\n","\t\t\t\t out_channel,\n","\t\t\t\t kernel_size,\n","\t\t\t\t stride   = 1,\n","\t\t\t\t padding  = 0,\n","\t\t\t\t dilation = 1\n","\t\t):\n","\t\tsuper().__init__()\n","\t\t\n","\t\tself.depthwise = nn.Sequential(\n","\t\t\tnn.Conv2d( in_channel, in_channel, kernel_size, stride=stride, padding=padding, dilation=dilation),\n","\t\t\tnn.BatchNorm2d(in_channel),\n","\t\t\tnn.ReLU(inplace=True)\n","\t\t)\n","\t\t\n","\t\tself.pointwise = nn.Sequential(\n","\t\t\tnn.Conv2d( in_channel, out_channel, kernel_size=1, stride=1, padding=0),\n","\t\t\tnn.BatchNorm2d(out_channel),\n","\t\t\tnn.ReLU(inplace=True)\n","\t\t)\n","\t\n","\tdef forward(self, x):\n","\t\tx = self.depthwise(x)\n","\t\tx = self.pointwise(x)\n","\t\treturn x\n","\n","class DSASPP(nn.Module):\n","\t\n","\tdef __init__(self,\n","\t\t\t\t in_channel,\n","\t\t\t\t channel,\n","\t\t\t\t dilation,\n","\t\t\t\t ):\n","\t\tsuper(DSASPP, self).__init__()\n","\t\t\n","\t\tself.conv = nn.ModuleList()\n","\t\tfor d in dilation:\n","\t\t\tif d == 1:\n","\t\t\t\tself.conv.append(\n","\t\t\t\t\tConv2dBnReLU(\n","\t\t\t\t\t\tin_channel,\n","\t\t\t\t\t\tchannel,\n","\t\t\t\t\t\tkernel_size=1 if d == 1 else 3,\n","\t\t\t\t\t\tdilation=d,\n","\t\t\t\t\t\tpadding=0 if d == 1 else d,\n","\t\t\t\t\t)\n","\t\t\t\t)\n","\t\t\telse:\n","\t\t\t\tself.conv.append(\n","\t\t\t\t\tDSConv2d(\n","\t\t\t\t\t\tin_channel,\n","\t\t\t\t\t\tchannel,\n","\t\t\t\t\t\tkernel_size=3,\n","\t\t\t\t\t\tdilation=d,\n","\t\t\t\t\t\tpadding=d,\n","\t\t\t\t\t)\n","\t\t\t\t)\n","\t\t\n","\t\tself.out = Conv2dBnReLU(\n","\t\t\tlen(dilation) * channel,\n","\t\t\tchannel,\n","\t\t\tkernel_size=3,\n","\t\t\tpadding=1,\n","\t\t)\n","\t \n","\tdef forward(self, x):\n","\t\taspp = []\n","\t\tfor conv in self.conv:\n","\t\t\taspp.append(conv(x))\n","\t\taspp = torch.cat(aspp, dim=1)\n","\t\tout = self.out(aspp)\n","\t\treturn out\n","\n","\t\n","##############################################################################################33\n","#   [32, 64, 160, 256],\n","class DaformerDecoder(nn.Module):\n","\tdef __init__(\n","\t\t\tself,\n","\n","\t\t\tencoder_dim = [152, 320, 320, 320,320],\n","\t\t\tdecoder_dim = 320,\n","\t\t\tdilation = [1, 6, 12, 18],\n","\t\t\tuse_bn_mlp  = True,\n","\t\t\tfuse = 'conv3x3',\n","\t):\n","\t\tsuper().__init__()\n","\t\tself.mlp = nn.ModuleList([\n","\t\t\tnn.Sequential(\n","\t\t\t\t# Conv2dBnReLU(dim, decoder_dim, 1, padding=0), #follow mmseg to use conv-bn-relu\n","\t\t\t\t*(\n","\t\t\t\t  ( nn.Conv2d(dim, decoder_dim, 1, padding= 0,  bias=False),\n","\t\t\t\t\tnn.BatchNorm2d(decoder_dim),\n","\t\t\t\t\tnn.ReLU(inplace=True),\n","\t\t\t\t)if use_bn_mlp else\n","\t\t\t\t  ( nn.Conv2d(dim, decoder_dim, 1, padding= 0,  bias=True),)\n","\t\t\t\t),\n","\t\t\t\t\n","\t\t\t\tMixUpSample(2**i) if i!=0 else nn.Identity(),\n","\t\t\t) for i, dim in enumerate(encoder_dim)])\n","\t  \n","\t\tif fuse=='conv1x1':\n","\t\t\tself.fuse = nn.Sequential(\n","\t\t\t\tnn.Conv2d(len(encoder_dim) * decoder_dim, decoder_dim, 1, padding=0, bias=False),\n","\t\t\t\tnn.BatchNorm2d(decoder_dim),\n","\t\t\t\tnn.ReLU(inplace=True),\n","\t\t\t)\n","\t\t\n","\t\tif fuse=='conv3x3':\n","\t\t\tself.fuse = nn.Sequential(\n","\t\t\t\tnn.Conv2d(len(encoder_dim) * decoder_dim, decoder_dim, 3, padding=1, bias=False),\n","\t\t\t\tnn.BatchNorm2d(decoder_dim),\n","\t\t\t\tnn.ReLU(inplace=True),\n","\t\t\t)\n","\t\t\n","\t\tif fuse=='aspp':\n","\t\t\tself.fuse = ASPP(\n","\t\t\t\tdecoder_dim*len(encoder_dim),\n","\t\t\t\tdecoder_dim,\n","\t\t\t\tdilation,\n","\t\t\t)\n","\t\t\t\n","\t\tif fuse=='ds-aspp':\n","\t\t\tself.fuse = DSASPP(\n","\t\t\t\tdecoder_dim*len(encoder_dim),\n","\t\t\t\tdecoder_dim,\n","\t\t\t\tdilation,\n","\t\t\t)\n","\t\t\n","\t\n","\tdef forward(self, feature):\n","\t\t\n","\t\tout = []\n","\t\tfor i,f in enumerate(feature):\n","\t\t\tf = self.mlp[i](f)\n","\t\t\tout.append(f)\n","\t\t\t# pdb.set_trace()\n","\t\t\t#print(f.shape)\n","\t\tx = self.fuse(torch.cat(out, dim = 1))\n","\t\treturn x, out  \n","\n","class daformer_conv1x1 (DaformerDecoder):\n","\tdef __init__(self, **kwargs):\n","\t\tsuper(daformer_conv1x1, self).__init__(\n","\t\t\tfuse = 'conv1x1',\n","\t\t\t**kwargs\n","\t\t)\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n","from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n","from timm.models.registry import register_model\n","\n","from einops import rearrange\n","from functools import partial\n","from torch import nn, einsum\n","import pdb\n","\n","\n","#---------------------------------------------\n","#https://github.com/facebookresearch/ConvNeXt/blob/main/models/convnext.py#L15\n","class LayerNorm2d(nn.Module):\n","\tdef __init__(self, dim, eps=1e-6):\n","\t\tsuper().__init__()\n","\t\tself.dim = dim\n","\t\tself.weight = nn.Parameter(torch.ones(dim))\n","\t\tself.bias = nn.Parameter(torch.zeros(dim))\n","\t\tself.eps = eps\n","\t\n","\tdef forward(self, x):\n","\t\tbatch_size,C,H,W = x.shape\n","\t\t#assert C==self.dim, 'C=%d, self.dim=%d'%(C,self.dim)\n","\t\t#print('C=%d, self.dim=%d'%(C,self.dim))\n","\t\t\n","\t\tu = x.mean(1, keepdim=True)\n","\t\ts = (x - u).pow(2).mean(1, keepdim=True)\n","\t\tx = (x - u) / torch.sqrt(s + self.eps)\n","\t\tx = self.weight[:, None, None] * x + self.bias[:, None, None]\n","\t\treturn x\n","#---------------------------------------------\n","\n","def _cfg_coat(url='', **kwargs):\n","\treturn {\n","\t\t'url': url,\n","\t\t'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n","\t\t'crop_pct': .9, 'interpolation': 'bicubic',\n","\t\t'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n","\t\t'first_conv': 'patch_embed.proj', 'classifier': 'head',\n","\t\t**kwargs\n","\t}\n","\n","\n","class Mlp(nn.Module):\n","\t\"\"\" Feed-forward network (FFN, a.k.a. MLP) class. \"\"\"\n","\tdef __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n","\t\tsuper().__init__()\n","\t\tout_features = out_features or in_features\n","\t\thidden_features = hidden_features or in_features\n","\t\tself.fc1 = nn.Linear(in_features, hidden_features)\n","\t\tself.act = act_layer()\n","\t\tself.fc2 = nn.Linear(hidden_features, out_features)\n","\t\tself.drop = nn.Dropout(drop)\n","\t\n","\tdef forward(self, x):\n","\t\tx = self.fc1(x)\n","\t\tx = self.act(x)\n","\t\tx = self.drop(x)\n","\t\tx = self.fc2(x)\n","\t\tx = self.drop(x)\n","\t\treturn x\n","\n","\n","class ConvRelPosEnc(nn.Module):\n","\t\"\"\" Convolutional relative position encoding. \"\"\"\n","\tdef __init__(self, Ch, h, window):\n","\t\t\"\"\"\n","\t\tInitialization.\n","\t\t\tCh: Channels per head.\n","\t\t\th: Number of heads.\n","\t\t\twindow: Window size(s) in convolutional relative positional encoding. It can have two forms:\n","\t\t\t\t\t1. An integer of window size, which assigns all attention heads with the same window size in ConvRelPosEnc.\n","\t\t\t\t\t2. A dict mapping window size to #attention head splits (e.g. {window size 1: #attention head split 1, window size 2: #attention head split 2})\n","\t\t\t\t\t   It will apply different window size to the attention head splits.\n","\t\t\"\"\"\n","\t\tsuper().__init__()\n","\t\t\n","\t\tif isinstance(window, int):\n","\t\t\twindow = {window: h}                                                         # Set the same window size for all attention heads.\n","\t\t\tself.window = window\n","\t\telif isinstance(window, dict):\n","\t\t\tself.window = window\n","\t\telse:\n","\t\t\traise ValueError()\n","\t\t\n","\t\tself.conv_list = nn.ModuleList()\n","\t\tself.head_splits = []\n","\t\tfor cur_window, cur_head_split in window.items():\n","\t\t\tdilation = 1                                                                 # Use dilation=1 at default.\n","\t\t\tpadding_size = (cur_window + (cur_window - 1) * (dilation - 1)) // 2         # Determine padding size. Ref: https://discuss.pytorch.org/t/how-to-keep-the-shape-of-input-and-output-same-when-dilation-conv/14338\n","\t\t\tcur_conv = nn.Conv2d(cur_head_split*Ch, cur_head_split*Ch,\n","\t\t\t                     kernel_size=(cur_window, cur_window),\n","\t\t\t                     padding=(padding_size, padding_size),\n","\t\t\t                     dilation=(dilation, dilation),\n","\t\t\t                     groups=cur_head_split*Ch,\n","\t\t\t                     )\n","\t\t\tself.conv_list.append(cur_conv)\n","\t\t\tself.head_splits.append(cur_head_split)\n","\t\tself.channel_splits = [x*Ch for x in self.head_splits]\n","\t\n","\tdef forward(self, q, v, size):\n","\t\tB, h, N, Ch = q.shape\n","\t\tH, W = size\n","\t\tassert N == 1 + H * W\n","\t\t\n","\t\t# Convolutional relative position encoding.\n","\t\tq_img = q[:,:,1:,:]                                                              # Shape: [B, h, H*W, Ch].\n","\t\tv_img = v[:,:,1:,:]                                                              # Shape: [B, h, H*W, Ch].\n","\t\t\n","\t\tv_img = rearrange(v_img, 'B h (H W) Ch -> B (h Ch) H W', H=H, W=W)               # Shape: [B, h, H*W, Ch] -> [B, h*Ch, H, W].\n","\t\tv_img_list = torch.split(v_img, self.channel_splits, dim=1)                      # Split according to channels.\n","\t\tconv_v_img_list = [conv(x) for conv, x in zip(self.conv_list, v_img_list)]\n","\t\tconv_v_img = torch.cat(conv_v_img_list, dim=1)\n","\t\tconv_v_img = rearrange(conv_v_img, 'B (h Ch) H W -> B h (H W) Ch', h=h)          # Shape: [B, h*Ch, H, W] -> [B, h, H*W, Ch].\n","\t\t\n","\t\tEV_hat_img = q_img * conv_v_img\n","\t\tzero = torch.zeros((B, h, 1, Ch), dtype=q.dtype, layout=q.layout, device=q.device)\n","\t\tEV_hat = torch.cat((zero, EV_hat_img), dim=2)                                # Shape: [B, h, N, Ch].\n","\t\t\n","\t\treturn EV_hat\n","\n","\n","class FactorAtt_ConvRelPosEnc(nn.Module):\n","\t\"\"\" Factorized attention with convolutional relative position encoding class. \"\"\"\n","\tdef __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., shared_crpe=None):\n","\t\tsuper().__init__()\n","\t\tself.num_heads = num_heads\n","\t\thead_dim = dim // num_heads\n","\t\tself.scale = qk_scale or head_dim ** -0.5\n","\t\t\n","\t\tself.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n","\t\tself.attn_drop = nn.Dropout(attn_drop)                                           # Note: attn_drop is actually not used.\n","\t\tself.proj = nn.Linear(dim, dim)\n","\t\tself.proj_drop = nn.Dropout(proj_drop)\n","\t\t\n","\t\t# Shared convolutional relative position encoding.\n","\t\tself.crpe = shared_crpe\n","\t\n","\tdef forward(self, x, size):\n","\t\tB, N, C = x.shape\n","\t\t\n","\t\t# Generate Q, K, V.\n","\t\tqkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)  # Shape: [3, B, h, N, Ch].\n","\t\tq, k, v = qkv[0], qkv[1], qkv[2]                                                 # Shape: [B, h, N, Ch].\n","\t\t\n","\t\t# Factorized attention.\n","\t\tk_softmax = k.softmax(dim=2)                                                     # Softmax on dim N.\n","\t\tk_softmax_T_dot_v = einsum('b h n k, b h n v -> b h k v', k_softmax, v)          # Shape: [B, h, Ch, Ch].\n","\t\tfactor_att        = einsum('b h n k, b h k v -> b h n v', q, k_softmax_T_dot_v)  # Shape: [B, h, N, Ch].\n","\t\t\n","\t\t# Convolutional relative position encoding.\n","\t\tcrpe = self.crpe(q, v, size=size)                                                # Shape: [B, h, N, Ch].\n","\t\t\n","\t\t# Merge and reshape.\n","\t\tx = self.scale * factor_att + crpe\n","\t\tx = x.transpose(1, 2).reshape(B, N, C)                                           # Shape: [B, h, N, Ch] -> [B, N, h, Ch] -> [B, N, C].\n","\t\t\n","\t\t# Output projection.\n","\t\tx = self.proj(x)\n","\t\tx = self.proj_drop(x)\n","\t\t\n","\t\treturn x                                                                         # Shape: [B, N, C].\n","\n","\n","class ConvPosEnc(nn.Module):\n","\t\"\"\" Convolutional Position Encoding.\n","\t\tNote: This module is similar to the conditional position encoding in CPVT.\n","\t\"\"\"\n","\tdef __init__(self, dim, k=3):\n","\t\tsuper(ConvPosEnc, self).__init__()\n","\t\tself.proj = nn.Conv2d(dim, dim, k, 1, k//2, groups=dim)\n","\t\n","\tdef forward(self, x, size):\n","\t\tB, N, C = x.shape\n","\t\tH, W = size\n","\t\tassert N == 1 + H * W\n","\t\t\n","\t\t# Extract CLS token and image tokens.\n","\t\tcls_token, img_tokens = x[:, :1], x[:, 1:]                                       # Shape: [B, 1, C], [B, H*W, C].\n","\t\t\n","\t\t# Depthwise convolution.\n","\t\tfeat = img_tokens.transpose(1, 2).view(B, C, H, W)\n","\t\tx = self.proj(feat) + feat\n","\t\tx = x.flatten(2).transpose(1, 2)\n","\t\t\n","\t\t# Combine with CLS token.\n","\t\tx = torch.cat((cls_token, x), dim=1)\n","\t\t\n","\t\treturn x\n","\n","\n","class SerialBlock(nn.Module):\n","\t\"\"\" Serial block class.\n","\t\tNote: In this implementation, each serial block only contains a conv-attention and a FFN (MLP) module. \"\"\"\n","\tdef __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n","\t             drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n","\t             shared_cpe=None, shared_crpe=None):\n","\t\tsuper().__init__()\n","\t\t\n","\t\t# Conv-Attention.\n","\t\tself.cpe = shared_cpe\n","\t\t\n","\t\tself.norm1 = norm_layer(dim)\n","\t\tself.factoratt_crpe = FactorAtt_ConvRelPosEnc(\n","\t\t\tdim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop,\n","\t\t\tshared_crpe=shared_crpe)\n","\t\tself.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","\t\t\n","\t\t# MLP.\n","\t\tself.norm2 = norm_layer(dim)\n","\t\tmlp_hidden_dim = int(dim * mlp_ratio)\n","\t\tself.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n","\t\n","\tdef forward(self, x, size):\n","\t\t# Conv-Attention.\n","\t\tx = self.cpe(x, size)                  # Apply convolutional position encoding.\n","\t\tcur = self.norm1(x)\n","\t\tcur = self.factoratt_crpe(cur, size)   # Apply factorized attention and convolutional relative position encoding.\n","\t\tx = x + self.drop_path(cur)\n","\t\t\n","\t\t# MLP.\n","\t\tcur = self.norm2(x)\n","\t\tcur = self.mlp(cur)\n","\t\tx = x + self.drop_path(cur)\n","\t\t\n","\t\treturn x\n","\n","\n","class ParallelBlock(nn.Module):\n","\t\"\"\" Parallel block class. \"\"\"\n","\tdef __init__(self, dims, num_heads, mlp_ratios=[], qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n","\t             drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n","\t             shared_cpes=None, shared_crpes=None):\n","\t\tsuper().__init__()\n","\t\t\n","\t\t# Conv-Attention.\n","\t\tself.cpes = shared_cpes\n","\t\t\n","\t\tself.norm12 = norm_layer(dims[1])\n","\t\tself.norm13 = norm_layer(dims[2])\n","\t\tself.norm14 = norm_layer(dims[3])\n","\t\tself.norm15 = norm_layer(dims[4])\n","\n","\n","\t\tself.factoratt_crpe2 = FactorAtt_ConvRelPosEnc(\n","\t\t\tdims[1], num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop,\n","\t\t\tshared_crpe=shared_crpes[1]\n","\t\t)\n","\t\tself.factoratt_crpe3 = FactorAtt_ConvRelPosEnc(\n","\t\t\tdims[2], num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop,\n","\t\t\tshared_crpe=shared_crpes[2]\n","\t\t)\n","\t\tself.factoratt_crpe4 = FactorAtt_ConvRelPosEnc(\n","\t\t\tdims[3], num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop,\n","\t\t\tshared_crpe=shared_crpes[3]\n","\t\t)\n","\t\tself.factoratt_crpe5 = FactorAtt_ConvRelPosEnc(\n","\t\t\tdims[4], num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop,\n","\t\t\tshared_crpe=shared_crpes[4]\n","\t\t)\n","\n","\n","\t\tself.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","\t\t\n","\t\t# MLP.\n","\t\tself.norm22 = norm_layer(dims[1])\n","\t\tself.norm23 = norm_layer(dims[2])\n","\t\tself.norm24 = norm_layer(dims[3])\n","\t\tself.norm25 = norm_layer(dims[4])\n","\n","\t\tassert dims[1] == dims[2] == dims[3] ==dims[4]                             # In parallel block, we assume dimensions are the same and share the linear transformation.\n","\t\tassert mlp_ratios[1] == mlp_ratios[2] == mlp_ratios[3]\n","\t\tmlp_hidden_dim = int(dims[1] * mlp_ratios[1])\n","\t\tself.mlp2 = self.mlp3 = self.mlp4 =self.mlp5= Mlp(in_features=dims[1], hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n","\t\n","\tdef upsample(self, x, output_size, size):\n","\t\t\"\"\" Feature map up-sampling. \"\"\"\n","\t\treturn self.interpolate(x, output_size=output_size, size=size)\n","\t\n","\tdef downsample(self, x, output_size, size):\n","\t\t\"\"\" Feature map down-sampling. \"\"\"\n","\t\treturn self.interpolate(x, output_size=output_size, size=size)\n","\t\n","\tdef interpolate(self, x, output_size, size):\n","\t\t\"\"\" Feature map interpolation. \"\"\"\n","\t\tB, N, C = x.shape\n","\t\tH, W = size\n","\t\tassert N == 1 + H * W\n","\t\t\n","\t\tcls_token  = x[:, :1, :]\n","\t\timg_tokens = x[:, 1:, :]\n","\t\t\n","\t\timg_tokens = img_tokens.transpose(1, 2).reshape(B, C, H, W)\n","\t\timg_tokens = F.interpolate(img_tokens, size=output_size, mode='bilinear')  # FIXME: May have alignment issue.\n","\t\timg_tokens = img_tokens.reshape(B, C, -1).transpose(1, 2)\n","\t\t\n","\t\tout = torch.cat((cls_token, img_tokens), dim=1)\n","\t\t\n","\t\treturn out\n","\t\n","\tdef forward(self, x1, x2, x3, x4, x5,sizes):\n","\t\t_, (H2, W2), (H3, W3), (H4, W4),(H5,W5) = sizes\n","\t\t\n","\t\t# Conv-Attention.\n","\t\tx2 = self.cpes[1](x2, size=(H2, W2))  # Note: x1 is ignored.\n","\t\tx3 = self.cpes[2](x3, size=(H3, W3))\n","\t\tx4 = self.cpes[3](x4, size=(H4, W4))\n","\t\tx5 = self.cpes[4](x5, size=(H5, W5))\n","\t\t\n","\t\tcur2 = self.norm12(x2)\n","\t\tcur3 = self.norm13(x3)\n","\t\tcur4 = self.norm14(x4)\n","\t\tcur5 = self.norm15(x5)\n","\n","\t\tcur2 = self.factoratt_crpe2(cur2, size=(H2,W2))\n","\t\tcur3 = self.factoratt_crpe3(cur3, size=(H3,W3))\n","\t\tcur4 = self.factoratt_crpe4(cur4, size=(H4,W4))\n","\t\tcur5 = self.factoratt_crpe4(cur5, size=(H5,W5))\n","\n","\n","\t\tupsample3_2 = self.upsample(cur3, output_size=(H2,W2), size=(H3,W3))\n","\t\tupsample4_3 = self.upsample(cur4, output_size=(H3,W3), size=(H4,W4))\n","\t\tupsample4_2 = self.upsample(cur4, output_size=(H2,W2), size=(H4,W4))\n","\t\tdownsample2_3 = self.downsample(cur2, output_size=(H3,W3), size=(H2,W2))\n","\t\tdownsample3_4 = self.downsample(cur3, output_size=(H4,W4), size=(H3,W3))\n","\t\tdownsample2_4 = self.downsample(cur2, output_size=(H4,W4), size=(H2,W2))\n","\t\tupsample5_2 = self.upsample(cur5, output_size=(H2,W2), size=(H5,W5))\n","\t\tupsample5_3 = self.upsample(cur5, output_size=(H3,W3), size=(H5,W5))\n","\t\tdownsample3_5 = self.downsample(cur3, output_size=(H5,W5), size=(H3,W3))\n","\t\tupsample5_4 = self.upsample(cur5, output_size=(H4,W4), size=(H5,W5))\n","\t\tdownsample2_5 = self.downsample(cur2, output_size=(H5,W5), size=(H2,W2))\n","\t\tdownsample4_5 = self.downsample(cur4, output_size=(H5,W5), size=(H4,W4))\n","     \n","\n","\t\tcur2 = cur2  + upsample3_2   + upsample4_2+  upsample5_2\n","\t\tcur3 = cur3  + upsample4_3   + downsample2_3 + upsample5_3\n","\t\tcur4 = cur4  + downsample3_4 + downsample2_4 + upsample5_4 \n","\t\tcur5 = cur5  + downsample3_5 + downsample2_5 + downsample4_5 \n","\n","\n","\t\tx2 = x2 + self.drop_path(cur2)\n","\t\tx3 = x3 + self.drop_path(cur3)\n","\t\tx4 = x4 + self.drop_path(cur4)\n","\t\tx5 = x5 + self.drop_path(cur5)\n","\t\t\n","\t\t# MLP.\n","\t\tcur2 = self.norm22(x2)\n","\t\tcur3 = self.norm23(x3)\n","\t\tcur4 = self.norm24(x4)\n","\t\tcur5 = self.norm25(x5)\n","\n","\t\tcur2 = self.mlp2(cur2)\n","\t\tcur3 = self.mlp3(cur3)\n","\t\tcur4 = self.mlp4(cur4)\n","\t\tcur5 = self.mlp5(cur5)\n","\n","\t\tx2 = x2 + self.drop_path(cur2)\n","\t\tx3 = x3 + self.drop_path(cur3)\n","\t\tx4 = x4 + self.drop_path(cur4)\n","\t\tx5 = x5 + self.drop_path(cur5)\n","\t\t\n","\t\treturn x1, x2, x3, x4, x5\n","\n","\n","class PatchEmbed(nn.Module):\n","\t\"\"\" Image to Patch Embedding \"\"\"\n","\tdef __init__(self, patch_size=16, in_chans=3, embed_dim=768):\n","\t\tsuper().__init__()\n","\t\tpatch_size = to_2tuple(patch_size)\n","\t\t\n","\t\tself.patch_size = patch_size\n","\t\tself.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n","\t\tself.norm = nn.LayerNorm(embed_dim)\n","\t\n","\tdef forward(self, x):\n","\t\t_, _, H, W = x.shape\n","\t\tout_H, out_W = H // self.patch_size[0], W // self.patch_size[1]\n","\t\t\n","\t\tx = self.proj(x).flatten(2).transpose(1, 2)\n","\t\tout = self.norm(x)\n","\t\t\n","\t\treturn out, (out_H, out_W)\n","\n","\n","class CoaT(nn.Module):\n","\t\"\"\" CoaT class. \"\"\"\n","\tdef __init__(self, patch_size=16, in_chans=3, embed_dims=[0, 0, 0, 0],\n","\t             serial_depths=[0, 0, 0, 0], parallel_depth=0,\n","\t             num_heads=0, mlp_ratios=[0, 0, 0, 0], qkv_bias=True, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n","\t             drop_path_rate=0.,\n","\t             norm_layer=partial(nn.LayerNorm, eps=1e-6),\n","\t             return_interm_layers=True,\n","\t             out_features=['x1_nocls','x2_nocls','x3_nocls','x4_nocls','x5_nocls'],\n","\t             crpe_window={3:2, 5:3, 7:3},\n","\t             pretrain=None,\n","\t             out_norm = nn.Identity, #use nn.Identity, nn.BatchNorm2d, LayerNorm2d\n","\t             **kwargs):\n","\t\tsuper().__init__()\n","\t\tself.return_interm_layers = return_interm_layers\n","\t\tself.pretrain     = pretrain\n","\t\tself.embed_dims   = embed_dims\n","\t\tself.out_features = out_features\n","\t\t#self.num_classes  = num_classes\n","\t\t\n","\t\t# Patch embeddings.\n","\t\tself.patch_embed1 = PatchEmbed(patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dims[0])\n","\t\tself.patch_embed2 = PatchEmbed(patch_size=2, in_chans=embed_dims[0], embed_dim=embed_dims[1])\n","\t\tself.patch_embed3 = PatchEmbed(patch_size=2, in_chans=embed_dims[1], embed_dim=embed_dims[2])\n","\t\tself.patch_embed4 = PatchEmbed(patch_size=2, in_chans=embed_dims[2], embed_dim=embed_dims[3])\n","\t\tself.patch_embed5 = PatchEmbed(patch_size=2, in_chans=embed_dims[3], embed_dim=embed_dims[4])\n","\t\t\n","\t\t# Class tokens.\n","\t\tself.cls_token1 = nn.Parameter(torch.zeros(1, 1, embed_dims[0]))\n","\t\tself.cls_token2 = nn.Parameter(torch.zeros(1, 1, embed_dims[1]))\n","\t\tself.cls_token3 = nn.Parameter(torch.zeros(1, 1, embed_dims[2]))\n","\t\tself.cls_token4 = nn.Parameter(torch.zeros(1, 1, embed_dims[3]))\n","\t\tself.cls_token5 = nn.Parameter(torch.zeros(1, 1, embed_dims[4]))\n","\t\t\n","\t\t# Convolutional position encodings.\n","\t\tself.cpe1 = ConvPosEnc(dim=embed_dims[0], k=3)\n","\t\tself.cpe2 = ConvPosEnc(dim=embed_dims[1], k=3)\n","\t\tself.cpe3 = ConvPosEnc(dim=embed_dims[2], k=3)\n","\t\tself.cpe4 = ConvPosEnc(dim=embed_dims[3], k=3)\n","\t\tself.cpe5 = ConvPosEnc(dim=embed_dims[4], k=3)\n","\t\t\n","\t\t# Convolutional relative position encodings.\n","\t\tself.crpe1 = ConvRelPosEnc(Ch=embed_dims[0] // num_heads, h=num_heads, window=crpe_window)\n","\t\tself.crpe2 = ConvRelPosEnc(Ch=embed_dims[1] // num_heads, h=num_heads, window=crpe_window)\n","\t\tself.crpe3 = ConvRelPosEnc(Ch=embed_dims[2] // num_heads, h=num_heads, window=crpe_window)\n","\t\tself.crpe4 = ConvRelPosEnc(Ch=embed_dims[3] // num_heads, h=num_heads, window=crpe_window)\n","\t\tself.crpe5 = ConvRelPosEnc(Ch=embed_dims[4] // num_heads, h=num_heads, window=crpe_window)\n","\n","\t\t# Enable stochastic depth.\n","\t\tdpr = drop_path_rate\n","\t\t\n","\t\t# Serial blocks 1.\n","\t\tself.serial_blocks1 = nn.ModuleList([\n","\t\t\tSerialBlock(\n","\t\t\t\tdim=embed_dims[0], num_heads=num_heads, mlp_ratio=mlp_ratios[0], qkv_bias=qkv_bias, qk_scale=qk_scale,\n","\t\t\t\tdrop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr, norm_layer=norm_layer,\n","\t\t\t\tshared_cpe=self.cpe1, shared_crpe=self.crpe1\n","\t\t\t)\n","\t\t\tfor _ in range(serial_depths[0])]\n","\t\t)\n","\t\t\n","\t\t# Serial blocks 2.\n","\t\tself.serial_blocks2 = nn.ModuleList([\n","\t\t\tSerialBlock(\n","\t\t\t\tdim=embed_dims[1], num_heads=num_heads, mlp_ratio=mlp_ratios[1], qkv_bias=qkv_bias, qk_scale=qk_scale,\n","\t\t\t\tdrop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr, norm_layer=norm_layer,\n","\t\t\t\tshared_cpe=self.cpe2, shared_crpe=self.crpe2\n","\t\t\t)\n","\t\t\tfor _ in range(serial_depths[1])]\n","\t\t)\n","\t\t\n","\t\t# Serial blocks 3.\n","\t\tself.serial_blocks3 = nn.ModuleList([\n","\t\t\tSerialBlock(\n","\t\t\t\tdim=embed_dims[2], num_heads=num_heads, mlp_ratio=mlp_ratios[2], qkv_bias=qkv_bias, qk_scale=qk_scale,\n","\t\t\t\tdrop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr, norm_layer=norm_layer,\n","\t\t\t\tshared_cpe=self.cpe3, shared_crpe=self.crpe3\n","\t\t\t)\n","\t\t\tfor _ in range(serial_depths[2])]\n","\t\t)\n","\t\t\n","\t\t# Serial blocks 4.\n","\t\tself.serial_blocks4 = nn.ModuleList([\n","\t\t\tSerialBlock(\n","\t\t\t\tdim=embed_dims[3], num_heads=num_heads, mlp_ratio=mlp_ratios[3], qkv_bias=qkv_bias, qk_scale=qk_scale,\n","\t\t\t\tdrop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr, norm_layer=norm_layer,\n","\t\t\t\tshared_cpe=self.cpe4, shared_crpe=self.crpe4\n","\t\t\t)\n","\t\t\tfor _ in range(serial_depths[3])]\n","\t\t)\n","\n","\t\t# Serial blocks 5.\n","\t\tself.serial_blocks4 = nn.ModuleList([\n","\t\t\tSerialBlock(\n","\t\t\t\tdim=embed_dims[4], num_heads=num_heads, mlp_ratio=mlp_ratios[4], qkv_bias=qkv_bias, qk_scale=qk_scale,\n","\t\t\t\tdrop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr, norm_layer=norm_layer,\n","\t\t\t\tshared_cpe=self.cpe4, shared_crpe=self.crpe4\n","\t\t\t)\n","\t\t\tfor _ in range(serial_depths[4])]\n","\t\t)\n","\t\t\n","\t\t# Parallel blocks.\n","\t\tself.parallel_depth = parallel_depth\n","\t\tif self.parallel_depth > 0:\n","\t\t\tself.parallel_blocks = nn.ModuleList([\n","\t\t\t\tParallelBlock(\n","\t\t\t\t\tdims=embed_dims, num_heads=num_heads, mlp_ratios=mlp_ratios, qkv_bias=qkv_bias, qk_scale=qk_scale,\n","\t\t\t\t\tdrop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr, norm_layer=norm_layer,\n","\t\t\t\t\tshared_cpes=[self.cpe1, self.cpe2, self.cpe3, self.cpe4,  self.cpe5],\n","\t\t\t\t\tshared_crpes=[self.crpe1, self.crpe2, self.crpe3, self.crpe4,  self.cpe5]\n","\t\t\t\t)\n","\t\t\t\tfor _ in range(parallel_depth)]\n","\t\t\t)\n","\t\t\n","\t\t# Classification head(s).\n","\t\t# if not self.return_interm_layers:\n","\t\t# \tself.norm1 = norm_layer(embed_dims[0])\n","\t\t# \tself.norm2 = norm_layer(embed_dims[1])\n","\t\t# \tself.norm3 = norm_layer(embed_dims[2])\n","\t\t# \tself.norm4 = norm_layer(embed_dims[3])\n","\t\t#\n","\t\t# \tif self.parallel_depth > 0:                                  # CoaT series: Aggregate features of last three scales for classification.\n","\t\t# \t\tassert embed_dims[1] == embed_dims[2] == embed_dims[3]\n","\t\t# \t\tself.aggregate = torch.nn.Conv1d(in_channels=3, out_channels=1, kernel_size=1)\n","\t\t# \t\tself.head = nn.Linear(embed_dims[3], num_classes)\n","\t\t# \telse:\n","\t\t# \t\tself.head = nn.Linear(embed_dims[3], num_classes)        # CoaT-Lite series: Use feature of last scale for classification.\n","\t\t#---\n","\t\t# add a norm layer for each output\n","\t\tself.out_norm = nn.ModuleList(\n","\t\t\t[ out_norm(embed_dims[i]) for i in range(4)]\n","\t\t)\n","\t\t\n","\t\t# Initialize weights.\n","\t\ttrunc_normal_(self.cls_token1, std=.02)\n","\t\ttrunc_normal_(self.cls_token2, std=.02)\n","\t\ttrunc_normal_(self.cls_token3, std=.02)\n","\t\ttrunc_normal_(self.cls_token4, std=.02)\n","\t\ttrunc_normal_(self.cls_token5, std=.02)\n","\t\tself.apply(self._init_weights)\n","\t\n","\tdef _init_weights(self, m):\n","\t\tif isinstance(m, nn.Linear):\n","\t\t\ttrunc_normal_(m.weight, std=.02)\n","\t\t\tif isinstance(m, nn.Linear) and m.bias is not None:\n","\t\t\t\tnn.init.constant_(m.bias, 0)\n","\t\telif isinstance(m, nn.LayerNorm):\n","\t\t\tnn.init.constant_(m.bias, 0)\n","\t\t\tnn.init.constant_(m.weight, 1.0)\n","\t\n","\t@torch.jit.ignore\n","\tdef no_weight_decay(self):\n","\t\treturn {'cls_token1', 'cls_token2', 'cls_token3', 'cls_token4'}\n","\t\n","\t# def get_classifier(self):\n","\t# \treturn self.head\n","\t\n","\t# def reset_classifier(self, num_classes, global_pool=''):\n","\t# \tself.num_classes = num_classes\n","\t# \tself.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\t\n","\tdef insert_cls(self, x, cls_token):\n","\t\t\"\"\" Insert CLS token. \"\"\"\n","\t\tcls_tokens = cls_token.expand(x.shape[0], -1, -1)\n","\t\tx = torch.cat((cls_tokens, x), dim=1)\n","\t\treturn x\n","\t\n","\tdef remove_cls(self, x):\n","\t\t\"\"\" Remove CLS token. \"\"\"\n","\t\treturn x[:, 1:, :]\n","\t\n","\tdef forward(self, x0):\n","\t\tB = x0.shape[0]\n","\t\t\n","\t\t# Serial blocks 1.\n","\t\tx1, (H1, W1) = self.patch_embed1(x0)\n","\t\tcls = self.cls_token1#torch.zeros_like(self.cls_token1)#self.cls_token1\n","\t\tx1 = self.insert_cls(x1, cls)\n","\t\tfor blk in self.serial_blocks1:\n","\t\t\tx1 = blk(x1, size=(H1, W1))\n","\t\tx1_nocls = self.remove_cls(x1)\n","\t\tx1_nocls = x1_nocls.reshape(B, H1, W1, -1).permute(0, 3, 1, 2).contiguous()\n","\t\t\n","\t\t# Serial blocks 2.\n","\t\tx2, (H2, W2) = self.patch_embed2(x1_nocls)\n","\t\tcls = self.cls_token2# torch.zeros_like(self.cls_token2)#self.cls_token2#\n","\t\tx2 = self.insert_cls(x2,cls)\n","\t\tfor blk in self.serial_blocks2:\n","\t\t\tx2 = blk(x2, size=(H2, W2))\n","\t\tx2_nocls = self.remove_cls(x2)\n","\t\tx2_nocls = x2_nocls.reshape(B, H2, W2, -1).permute(0, 3, 1, 2).contiguous()\n","\t\t\n","\t\t# Serial blocks 3.\n","\t\tx3, (H3, W3) = self.patch_embed3(x2_nocls)\n","\t\tcls = self.cls_token3#torch.zeros_like(self.cls_token3)# self.cls_token3\n","\t\tx3 = self.insert_cls(x3, cls)\n","\t\tfor blk in self.serial_blocks3:\n","\t\t\tx3 = blk(x3, size=(H3, W3))\n","\t\tx3_nocls = self.remove_cls(x3)\n","\t\tx3_nocls = x3_nocls.reshape(B, H3, W3, -1).permute(0, 3, 1, 2).contiguous()\n","\t\t\n","\t\t# Serial blocks 4.\n","\t\tx4, (H4, W4) = self.patch_embed4(x3_nocls)\n","\t\tcls = self.cls_token5#torch.zeros_like(self.cls_token4)#self.cls_token4\n","\t\tx4 = self.insert_cls(x4, cls)\n","\t\tfor blk in self.serial_blocks4:\n","\t\t\tx4 = blk(x4, size=(H4, W4))\n","\t\tx4_nocls = self.remove_cls(x4)\n","\t\tx4_nocls = x4_nocls.reshape(B, H4, W4, -1).permute(0, 3, 1, 2).contiguous()\n","\n","\t\t# Serial blocks 5.\n","\t\tx5, (H5, W5) = self.patch_embed4(x4_nocls)\n","\t\tcls = self.cls_token4#torch.zeros_like(self.cls_token4)#self.cls_token4\n","\t\tx5 = self.insert_cls(x5, cls)\n","\t\tfor blk in self.serial_blocks4:\n","\t\t\tx5 = blk(x5, size=(H5, W5))\n","\t\tx5_nocls = self.remove_cls(x5)\n","\t\tx5_nocls = x5_nocls.reshape(B, H5, W5, -1).permute(0, 3, 1, 2).contiguous()\n","\t\t\n","\t\t# Only serial blocks: Early return.\n","\t\tif self.parallel_depth == 0:\n","\t\t\tx1_nocls = self.out_norm[0](x1_nocls)\n","\t\t\tx2_nocls = self.out_norm[1](x2_nocls)\n","\t\t\tx3_nocls = self.out_norm[2](x3_nocls)\n","\t\t\tx4_nocls = self.out_norm[3](x4_nocls)\n","\t\t\treturn [x1_nocls,x2_nocls,x3_nocls,x4_nocls]\n","\t\t \n","\t \n","\t\t\t\n","\t\t\n","\t\t# Parallel blocks.\n","\t\tfor blk in self.parallel_blocks:\n","\t\t\tx1, x2, x3, x4,x5 = blk(x1, x2, x3, x4,x5, sizes=[(H1, W1), (H2, W2), (H3, W3), (H4, W4), (H5, W5)])\n","\t\t# pdb.set_trace()\n","\t\t# remove cls and return feature for seg\n","\t\tif self.return_interm_layers:       # Return intermediate features for down-stream tasks (e.g. Deformable DETR and Detectron2).\n","\t\t\tfeat_out = {}\n","\t\t\tif 'x1_nocls' in self.out_features:\n","\t\t\t\tx1_nocls = self.remove_cls(x1)\n","\t\t\t\tx1_nocls = x1_nocls.reshape(B, H1, W1, -1).permute(0, 3, 1, 2).contiguous()\n","\t\t\t\tfeat_out['x1_nocls'] = x1_nocls\n","\t\t\tif 'x2_nocls' in self.out_features:\n","\t\t\t\tx2_nocls = self.remove_cls(x2)\n","\t\t\t\tx2_nocls = x2_nocls.reshape(B, H2, W2, -1).permute(0, 3, 1, 2).contiguous()\n","\t\t\t\tfeat_out['x2_nocls'] = x2_nocls\n","\t\t\tif 'x3_nocls' in self.out_features:\n","\t\t\t\tx3_nocls = self.remove_cls(x3)\n","\t\t\t\tx3_nocls = x3_nocls.reshape(B, H3, W3, -1).permute(0, 3, 1, 2).contiguous()\n","\t\t\t\tfeat_out['x3_nocls'] = x3_nocls\n","\t\t\tif 'x4_nocls' in self.out_features:\n","\t\t\t\tx4_nocls = self.remove_cls(x4)\n","\t\t\t\tx4_nocls = x4_nocls.reshape(B, H4, W4, -1).permute(0, 3, 1, 2).contiguous()\n","\t\t\t\tfeat_out['x4_nocls'] = x4_nocls\n","\t\t\tif 'x5_nocls' in self.out_features:\n","\t\t\t\tx5_nocls = self.remove_cls(x5)\n","\t\t\t\tx5_nocls = x5_nocls.reshape(B, H5, W5, -1).permute(0, 3, 1, 2).contiguous()\n","\t\t\t\tfeat_out['x5_nocls'] = x5_nocls\n","\t\t\tfeat_out = list(feat_out.values())\n","\t\t\treturn feat_out\n","\t\telse:\n","\t\t\tx2 = self.norm2(x2)\n","\t\t\tx3 = self.norm3(x3)\n","\t\t\tx4 = self.norm4(x4)\n","\t\t\tx2_cls = x2[:, :1]              # Shape: [B, 1, C].\n","\t\t\tx3_cls = x3[:, :1]\n","\t\t\tx4_cls = x4[:, :1]\n","\t\t\tmerged_cls = torch.cat((x2_cls, x3_cls, x4_cls), dim=1)       # Shape: [B, 3, C].\n","\t\t\tmerged_cls = self.aggregate(merged_cls).squeeze(dim=1)        # Shape: [B, C].\n","\t\t\treturn merged_cls\n","\t\n","\n","#@register_model\n","class coat_lite_small (CoaT):\n","\tdef __init__(self, **kwargs):\n","\t\tsuper(coat_lite_small, self).__init__(\n","\t        patch_size=4, embed_dims=[64, 128, 320, 512,512], serial_depths=[2, 2, 2, 2,2],\n","\t\t\tparallel_depth=6, num_heads=8, mlp_ratios=[4, 4, 4, 4,4], **kwargs)\n","\t \n","    \n","    \n","#@register_model\n","class coat_lite_medium (CoaT):\n","\tdef __init__(self, **kwargs):\n","\t\tsuper(coat_lite_medium, self).__init__(\n","\t\t\tpatch_size=4, embed_dims=[128, 256, 320, 512],\n","\t\t\tserial_depths=[3, 6, 10, 8],\n","\t\t    parallel_depth=0, num_heads=8, mlp_ratios=[4, 4, 4, 4],\n","\t\t    pretrain = 'coat_lite_medium_384x384_f9129688.pth',\n","\t\t\t**kwargs)\n","\n","#@register_model\n","class coat_parallel_small (CoaT):\n","\tdef __init__(self, **kwargs):\n","\t\tsuper(coat_parallel_small, self).__init__(\n","\t\t\tpatch_size=4, embed_dims=[152, 320, 320, 320,320],\n","\t\t\tserial_depths=[2, 2, 2, 2,2],\n","\t\t\tparallel_depth=6, num_heads=8, mlp_ratios=[4, 4, 4, 4,4], \n","\t\t\tpretrain = 'coat_lite_small_384x384_f9129688.pth',\n","\t\t\t**kwargs)\n","\n","        \n","        \n","class Net(nn.Module):\n","    \n","    def load_pretrain( self,):\n","\n","        checkpoint = torch.load('../input/coat-only-small/coat_small_7479cf9b.pth', map_location=lambda storage, loc: storage)\n","        state_dict = checkpoint['model']\n","        print(self.encoder.load_state_dict(state_dict, strict=False)) \n","\n","    def __init__(self, decoder=daformer_conv1x1, encoder_cfg={}, decoder_cfg={}):\n","        super(Net, self).__init__()\n","        decoder_dim = decoder_cfg.get('decoder_dim', 320)\n","\n","\n","        self.output_type = ['inference', 'loss']\n","\n","\n","        self.rgb = RGB()\n","\n","        self.encoder = coat_parallel_small()\n","        \n","        encoder_dim = self.encoder.embed_dims\n","        \n","        self.decoder = decoder(\n","            encoder_dim=encoder_dim,\n","            decoder_dim=decoder_dim,\n","        )\n","        self.logit = nn.Sequential(\n","            nn.Conv2d(decoder_dim, 1, kernel_size=1),\n","        )\n","\n","        self.aux = nn.ModuleList([\n","            nn.Conv2d(decoder_dim, 1, kernel_size=1, padding=0) for i in range(4)\n","        ])\n","\n","\n","    def forward(self, batch):\n","\n","        x = batch['image']\n","        x = self.rgb(x)\n","\n","        B, C, H, W = x.shape\n","        \n","        encoder = self.encoder(x)\n","\n","        last, decoder = self.decoder(encoder)\n","        logit = self.logit(last)\n","        logit = F.interpolate(logit, size=None, scale_factor=4, mode='bilinear', align_corners=False)\n","\n","        output = {}\n","        if 'loss' in self.output_type:\n","            output['bce_loss'] = F.binary_cross_entropy_with_logits(logit,batch['mask'])\n","            for i in range(4):\n","                output['aux%d_loss'%i] = criterion_aux_loss(self.aux[i](decoder[i]),batch['mask'])\n","        if 'inference' in self.output_type:\n","            probability_from_logit = torch.sigmoid(logit)\n","            output['probability'] = probability_from_logit\n","\n","        return output\n","\n","def criterion_aux_loss(logit, mask):\n","    mask = F.interpolate(mask,size=logit.shape[-2:], mode='nearest')\n","    loss = F.binary_cross_entropy_with_logits(logit,mask)\n","    return loss"]},{"cell_type":"code","execution_count":11,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-08-30T17:34:02.155637Z","iopub.status.busy":"2022-08-30T17:34:02.155263Z","iopub.status.idle":"2022-08-30T17:34:02.167593Z","shell.execute_reply":"2022-08-30T17:34:02.166622Z","shell.execute_reply.started":"2022-08-30T17:34:02.155601Z"},"papermill":{"duration":0.022542,"end_time":"2022-08-24T15:49:20.488304","exception":false,"start_time":"2022-08-24T15:49:20.465762","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def run_check_net():\n","    batch_size = 2\n","    image_size = 512\n","\n","    #---\n","    batch = {\n","        'image' : torch.from_numpy( np.random.uniform(-1,1,(batch_size,3,image_size,image_size)) ).float(),\n","        'mask'  : torch.from_numpy( np.random.choice(2,(batch_size,1,image_size,image_size)) ).float(),\n","        'organ' : torch.from_numpy( np.random.choice(5,(batch_size)) ).long(),\n","    }\n","    batch = {k:v.cuda() for k,v in batch.items()}\n","\n","    net = Net().cuda()\n","    net.load_pretrain()\n","\n","    with torch.no_grad():\n","        with torch.cuda.amp.autocast(enabled=True):\n","            output = net(batch)\n","\n","    print('batch')\n","    for k,v in batch.items():\n","        print('%32s :'%k, v.shape)\n","\n","    print('output')\n","    for k,v in output.items():\n","        if 'loss' not in k:\n","            print('%32s :'%k, v.shape)\n","    for k,v in output.items():\n","        if 'loss' in k:\n","            print('%32s :'%k, v.item())\n","            \n","#run_check_net()"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-08-30T17:34:02.169657Z","iopub.status.busy":"2022-08-30T17:34:02.169266Z","iopub.status.idle":"2022-08-30T17:34:02.181276Z","shell.execute_reply":"2022-08-30T17:34:02.180251Z","shell.execute_reply.started":"2022-08-30T17:34:02.169621Z"},"papermill":{"duration":0.020157,"end_time":"2022-08-24T15:49:20.518013","exception":false,"start_time":"2022-08-24T15:49:20.497856","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class Mlp(nn.Module):\n","    \"\"\" Multilayer perceptron.\"\"\"\n","\n","    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n","        super().__init__()\n","        out_features = out_features or in_features\n","        hidden_features = hidden_features or in_features\n","        self.fc1 = nn.Linear(in_features, hidden_features)\n","        self.act = act_layer()\n","        self.fc2 = nn.Linear(hidden_features, out_features)\n","        self.drop = nn.Dropout(drop)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.act(x)\n","        x = self.drop(x)\n","        x = self.fc2(x)\n","        x = self.drop(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.009275,"end_time":"2022-08-24T15:49:20.588396","exception":false,"start_time":"2022-08-24T15:49:20.579121","status":"completed"},"tags":[]},"source":["<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:#c3448b; background:#efe9e9; border:1px dashed #efe50b;\" role=\"tab\" aria-controls=\"folds\"><center>Folds</center></h3>"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-08-30T17:34:02.183141Z","iopub.status.busy":"2022-08-30T17:34:02.182666Z","iopub.status.idle":"2022-08-30T17:34:02.195019Z","shell.execute_reply":"2022-08-30T17:34:02.193743Z","shell.execute_reply.started":"2022-08-30T17:34:02.183105Z"},"papermill":{"duration":0.021931,"end_time":"2022-08-24T15:49:20.619806","exception":false,"start_time":"2022-08-24T15:49:20.597875","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def make_fold(fold=0):\n","    df = pd.read_csv(root_dir + '../input/hubmap-organ-segmentation/train.csv')\n","\n","    num_fold = 4\n","    skf = KFold(n_splits = num_fold, shuffle = True,random_state = 42)\n","\n","    df.loc[:,'fold']=-1\n","    for f,(t_idx, v_idx) in enumerate(skf.split(X=df['id'], y=df['organ'])):\n","        df.iloc[v_idx,-1]=f\n","\n","    #check\n","    if 0:\n","        for f in range(num_fold):\n","            train_df=df[df.fold!=f].reset_index(drop=True)\n","            valid_df=df[df.fold==f].reset_index(drop=True)\n","\n","            print('fold %d'%f)\n","            t = train_df.organ.value_counts().to_dict()\n","            v = valid_df.organ.value_counts().to_dict()\n","            for k in ['kidney', 'prostate', 'largeintestine', 'spleen', 'lung']:\n","                print('%32s %3d (%0.3f)  %3d (%0.3f)'%(k,t.get(k,0),t.get(k,0)/len(train_df),v.get(k,0),v.get(k,0)/len(valid_df)))\n","\n","            print('')\n","            zz=0\n","\n","    train_df=df[df.fold!=fold].reset_index(drop=True)\n","    valid_df=df[df.fold==fold].reset_index(drop=True)\n","    return train_df,valid_df"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.009221,"end_time":"2022-08-24T15:49:20.638545","exception":false,"start_time":"2022-08-24T15:49:20.629324","status":"completed"},"tags":[]},"source":["<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:#c3448b; background:#efe9e9; border:1px dashed #efe50b;\" role=\"tab\" aria-controls=\"dice_score\"><center>Competition Metric</center></h3>"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-08-30T17:34:02.200109Z","iopub.status.busy":"2022-08-30T17:34:02.199734Z","iopub.status.idle":"2022-08-30T17:34:02.208824Z","shell.execute_reply":"2022-08-30T17:34:02.207811Z","shell.execute_reply.started":"2022-08-30T17:34:02.200081Z"},"papermill":{"duration":0.018967,"end_time":"2022-08-24T15:49:20.666941","exception":false,"start_time":"2022-08-24T15:49:20.647974","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def compute_dice_score(probability, mask, smooth = 1):\n","    N = len(probability)\n","    p = probability.reshape(N,-1)\n","    t = mask.reshape(N,-1)\n","\n","    p = p>0.5\n","    t = t>0.5\n","    uion = p.sum(-1) + t.sum(-1)\n","    overlap = (p*t).sum(-1)\n","    dice = 2*overlap/(uion+0.0001)\n","    return dice"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.009215,"end_time":"2022-08-24T15:49:20.685602","exception":false,"start_time":"2022-08-24T15:49:20.676387","status":"completed"},"tags":[]},"source":["<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:#c3448b; background:#efe9e9; border:1px dashed #efe50b;\" role=\"tab\" aria-controls=\"validation\"><center>Validation</center></h3>"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-08-30T17:34:02.212786Z","iopub.status.busy":"2022-08-30T17:34:02.212303Z","iopub.status.idle":"2022-08-30T17:34:02.227872Z","shell.execute_reply":"2022-08-30T17:34:02.225407Z","shell.execute_reply.started":"2022-08-30T17:34:02.212760Z"},"papermill":{"duration":0.025582,"end_time":"2022-08-24T15:49:20.720735","exception":false,"start_time":"2022-08-24T15:49:20.695153","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def validate(net, valid_loader, debug = False):\n","\n","    valid_num = 0\n","    valid_probability = []\n","    valid_mask = []\n","    valid_loss = 0\n","\n","    net = net.eval()\n","    start_timer = time.time()\n","    for t, batch in enumerate(valid_loader):\n","\n","        net.output_type = ['loss', 'inference']\n","        with torch.no_grad():\n","            with amp.autocast(enabled = is_amp):\n","\n","                batch_size = len(batch['index'])\n","                batch['image'] = batch['image'].cuda()\n","                batch['mask' ] = batch['mask' ].cuda()\n","                batch['organ'] = batch['organ'].cuda()\n","\n","                output = net(batch)\n","                loss0  = output['bce_loss'].mean()\n","        #print(output['probability'])\n","        valid_probability.append(output['probability'].data.cpu().numpy())\n","        valid_mask.append(batch['mask'].data.cpu().numpy())\n","        valid_num += batch_size\n","        valid_loss += batch_size*loss0.item()\n","\n","        #debug\n","        if debug :\n","            pass\n","            organ = batch['organ'].data.cpu().numpy()\n","            image = batch['image']\n","            mask  = batch['mask']\n","            probability  = output['probability']\n","\n","            for b in range(batch_size):\n","                m = tensor_to_image(image[b])\n","                t = tensor_to_mask(mask[b,0])\n","                p = tensor_to_mask(probability[b,0])\n","                overlay = result_to_overlay(m, t, p )\n","\n","                text = label_to_organ[organ[b]]\n","                draw_shadow_text(overlay,text,(5,15),0.7,(1,1,1),1)\n","\n","                image_show_norm('overlay',overlay,min=0,max=1,resize=1)\n","                cv2.waitKey(0)\n","\n","        print('\\r %8d / %d  %s'%(valid_num, len(valid_loader.dataset),(time.time() - start_timer)),end='',flush=True)\n","\n","    assert(valid_num == len(valid_loader.dataset))\n","\n","    probability = np.concatenate(valid_probability)\n","    mask = np.concatenate(valid_mask)\n","\n","    loss = valid_loss/valid_num\n","\n","    dice = compute_dice_score(probability, mask)\n","    dice = dice.mean()\n","    \n","    return [dice, loss,  0, 0]"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.009216,"end_time":"2022-08-24T15:49:20.739390","exception":false,"start_time":"2022-08-24T15:49:20.730174","status":"completed"},"tags":[]},"source":["<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:#c3448b; background:#efe9e9; border:1px dashed #efe50b;\" role=\"tab\" aria-controls=\"init\"><center>Initialization</center></h3>"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-08-30T17:34:02.230282Z","iopub.status.busy":"2022-08-30T17:34:02.229723Z","iopub.status.idle":"2022-08-30T17:34:02.238528Z","shell.execute_reply":"2022-08-30T17:34:02.237566Z","shell.execute_reply.started":"2022-08-30T17:34:02.230063Z"},"papermill":{"duration":0.017608,"end_time":"2022-08-24T15:49:20.766300","exception":false,"start_time":"2022-08-24T15:49:20.748692","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def get_learning_rate(optimizer):\n","    return optimizer.param_groups[0]['lr']"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-08-30T17:34:02.240662Z","iopub.status.busy":"2022-08-30T17:34:02.240183Z","iopub.status.idle":"2022-08-30T17:34:05.311140Z","shell.execute_reply":"2022-08-30T17:34:05.309946Z","shell.execute_reply.started":"2022-08-30T17:34:02.240615Z"},"papermill":{"duration":6.521756,"end_time":"2022-08-24T15:49:27.297286","exception":false,"start_time":"2022-08-24T15:49:20.775530","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["fold = 3\n","\n","out_dir = root_dir + '/result/coatnet-small-aux5-768/fold-%d' % (fold)\n","initial_checkpoint = None\n","#initial_checkpoint = '../input/swin-10k/result/upernet-swin-v1-tiny-aux5-768/fold-3/checkpoint/00009975.model.pth'\n","#initial_checkpoint = '../input/heng-hubmap-fold0-10k/result/upernet-swin-v1-tiny-aux5-768/fold-0/checkpoint/00009975.model.pth'\n","\n","start_lr   = 5e-5 #0.0001\n","batch_size = 8 #32 #32\n","\n","\n","## setup  ----------------------------------------\n","for f in ['checkpoint','train','valid','backup'] : os.makedirs(out_dir +'/'+f, exist_ok=True)\n","\n","    \n","log = open(out_dir+'/log.train.txt',mode='a')\n","log.write('\\n--- [START %s] %s\\n\\n' % ('Swin', '-' * 64))\n","log.write('\\n')\n","\n","\n","## dataset ----------------------------------------\n","log.write('** dataset setting **\\n')\n","\n","train_df, valid_df = make_fold(fold)\n","\n","train_dataset = HubmapDataset(train_df, train_augment5b)\n","valid_dataset = HubmapDataset(valid_df, valid_augment5)\n","\n","train_loader  = DataLoader(\n","    train_dataset,\n","    sampler = RandomSampler(train_dataset),\n","    batch_size  = batch_size,\n","    drop_last   = True,\n","    num_workers = 8,\n","    pin_memory  = False,\n","    worker_init_fn = lambda id: np.random.seed(torch.initial_seed() // 2 ** 32 + id),\n","    collate_fn = null_collate,\n",")\n","\n","valid_loader = DataLoader(\n","    valid_dataset,\n","    sampler = SequentialSampler(valid_dataset),\n","    batch_size  = 8,\n","    drop_last   = False,\n","    num_workers = 4,\n","    pin_memory  = False,\n","    collate_fn = null_collate,\n",")\n","\n","\n","log.write('fold = %s\\n'%str(fold))\n","log.write('train_dataset : \\n%s\\n'%(train_dataset))\n","log.write('valid_dataset : \\n%s\\n'%(valid_dataset))\n","log.write('\\n')\n","\n","\n","## net ----------------------------------------\n","log.write('** net setting **\\n')\n","\n","scaler = amp.GradScaler(enabled = is_amp)\n","net = Net().cuda()\n","\n","if initial_checkpoint is not None:\n","    f = torch.load(initial_checkpoint, map_location=lambda storage, loc: storage)\n","    start_iteration = f['iteration']\n","    start_epoch = f['epoch']\n","    state_dict  = f['state_dict']\n","    net.load_state_dict(state_dict,strict=False)  #True\n","else:\n","    start_iteration = 0\n","    start_epoch = 0\n","    net.load_pretrain()\n","\n","\n","log.write('\\tinitial_checkpoint = %s\\n' % initial_checkpoint)\n","log.write('\\n')\n","\n","\n","## optimiser ----------------------------------\n","if 0: ##freeze\n","    for p in net.stem.parameters():   p.requires_grad = False\n","    pass\n","\n","def freeze_bn(net):\n","    for m in net.modules():\n","        if isinstance(m, nn.BatchNorm2d):\n","            m.eval()\n","            m.weight.requires_grad = False\n","            m.bias.requires_grad = False\n","            \n","#freeze_bn(net)\n","\n","#-----------------------------------------------\n","\n","optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, net.parameters()),lr=start_lr)\n","\n","log.write('optimizer\\n  %s\\n'%(optimizer))\n","log.write('\\n')\n","\n","#num_iteration = 1000*len(train_loader)\n","num_iteration = 7000\n","iter_log   = len(train_loader)*3 #479\n","iter_valid = iter_log\n","iter_save  = iter_log"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.009392,"end_time":"2022-08-24T15:49:27.317076","exception":false,"start_time":"2022-08-24T15:49:27.307684","status":"completed"},"tags":[]},"source":["<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:#c3448b; background:#efe9e9; border:1px dashed #efe50b;\" role=\"tab\" aria-controls=\"training\"><center>Training</center></h3>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-30T17:34:05.313159Z","iopub.status.busy":"2022-08-30T17:34:05.312762Z"},"papermill":{"duration":31959.59981,"end_time":"2022-08-25T00:42:06.926466","exception":false,"start_time":"2022-08-24T15:49:27.326656","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["log.write('** start training here! **\\n')\n","log.write('   batch_size = %d \\n'%(batch_size))\n","log.write('                     |-------------- VALID---------|---- TRAIN/BATCH ----------------\\n')\n","log.write('rate     iter  epoch | dice   loss   tp     tn     | loss           | time           \\n')\n","log.write('-------------------------------------------------------------------------------------\\n')\n","\n","valid_loss = np.zeros(4,np.float32)\n","train_loss = np.zeros(2,np.float32)\n","batch_loss = np.zeros_like(train_loss)\n","sum_train_loss = np.zeros_like(train_loss)\n","sum_train = 0\n","\n","start_timer = time.time()\n","iteration = start_iteration\n","epoch = start_epoch\n","rate = 0\n","\n","while iteration < num_iteration:\n","    for t, batch in enumerate(train_loader):\n","\n","        if iteration%iter_save==0:\n","            if iteration != start_iteration:\n","                torch.save({\n","                    'state_dict': net.state_dict(),\n","                    'iteration': iteration,\n","                    'epoch': epoch,\n","                }, out_dir + '/checkpoint/%08d.model.pth' %  (iteration))\n","                pass\n","\n","\n","        if (iteration%iter_valid==0):\n","            valid_loss = validate(net, valid_loader)\n","            pass\n","\n","\n","        if (iteration%iter_log==0) or (iteration%iter_valid==0):\n","            print('\\r', end='', flush=True)\n","            log.write(message(mode='log') + '\\n')\n","\n","\n","        # learning rate schduler ------------\n","        rate = get_learning_rate(optimizer)\n","\n","        # one iteration update  -------------\n","        batch_size = len(batch['index'])\n","        \n","        #print(batch_size, iteration, epoch)\n","        batch['image'] = batch['image'].half().cuda()\n","        batch['mask' ] = batch['mask' ].half().cuda()\n","        batch['organ'] = batch['organ'].cuda()\n","\n","\n","        net.train()\n","        net.output_type = ['loss']\n","        if 1:\n","            with amp.autocast(enabled = is_amp):\n","                output = net(batch)\n","                loss0  = output['bce_loss'].mean()\n","                loss1  = output['aux2_loss'].mean()\n","\n","            optimizer.zero_grad()\n","            scaler.scale(loss0+0.2*loss1).backward()\n","\n","            scaler.unscale_(optimizer)\n","            scaler.step(optimizer)\n","            scaler.update()\n","\n","\n","        # print statistics  --------\n","        batch_loss[:2] = [loss0.item(),loss1.item()]\n","        sum_train_loss = batch_loss\n","        sum_train += 1\n","        if t % 100 == 0:\n","            train_loss = sum_train_loss / (sum_train + 1e-12)\n","            sum_train_loss[...] = 0\n","            sum_train = 0\n","\n","        print('\\r', end='', flush=True)\n","        print(message(mode='print'), end='', flush=True)\n","        epoch += 1 / len(train_loader)\n","        iteration += 1\n","        \n","    torch.cuda.empty_cache()\n","    \n","log.write('\\n')\n","log.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.856564,"end_time":"2022-08-25T00:42:08.520783","exception":false,"start_time":"2022-08-25T00:42:07.664219","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# def image_to_tensor(image, mode='bgr'): #image mode\n","#     if mode=='bgr':\n","#         image = image[:,:,::-1]\n","#     x = image\n","#     x = x.transpose(2,0,1)\n","#     x = np.ascontiguousarray(x)\n","#     x = torch.tensor(x, dtype=torch.float)\n","#     return x\n","\n","# def tensor_to_image(x, mode='bgr'):\n","#     image = x.data.cpu().numpy()\n","#     image = image.transpose(1,2,0)\n","#     if mode=='bgr':\n","#         image = image[:,:,::-1]\n","#     image = np.ascontiguousarray(image)\n","#     image = image.astype(np.float32)\n","#     return image\n","\n","# def mask_to_tensor(mask):\n","#     x = mask\n","#     x = torch.tensor(x, dtype=torch.float)\n","#     return x\n","\n","# def tensor_to_mask(x):\n","#     mask = x.data.cpu().numpy()\n","#     mask = mask.astype(np.float32)\n","#     return mask"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.753979,"end_time":"2022-08-25T00:42:10.017427","exception":false,"start_time":"2022-08-25T00:42:09.263448","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# valid_num = 0\n","# valid_probability = []\n","# valid_mask = []\n","# valid_loss = 0\n","\n","# net = net.eval()\n","# start_timer = time.time()\n","# for t, batch in enumerate(valid_loader):\n","\n","#     net.output_type = ['loss', 'inference']\n","#     with torch.no_grad():\n","#         with amp.autocast(enabled = is_amp):\n","\n","#             batch_size = len(batch['index'])\n","#             batch['image'] = batch['image'].cuda()\n","#             batch['mask' ] = batch['mask' ].cuda()\n","#             batch['organ'] = batch['organ'].cuda()\n","\n","#             output = net(batch)\n","#             loss0  = output['bce_loss'].mean()\n","\n","#     valid_probability.append(output['probability'].data.cpu().numpy())\n","#     valid_mask.append(batch['mask'].data.cpu().numpy())\n","#     valid_num += batch_size\n","#     valid_loss += batch_size*loss0.item()\n","\n","#     organ = batch['organ'].data.cpu().numpy()\n","#     image = batch['image']\n","#     mask  = batch['mask']\n","#     probability  = output['probability']\n","\n","#     for b in range(batch_size):\n","#         m = tensor_to_image(image[b])\n","#         t = tensor_to_mask(mask[b,0])\n","#         p = tensor_to_mask(probability[b,0])"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.851532,"end_time":"2022-08-25T00:42:11.607636","exception":false,"start_time":"2022-08-25T00:42:10.756104","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# import matplotlib.pyplot as plt\n","\n","# plt.imshow(m), m.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.800372,"end_time":"2022-08-25T00:42:13.367804","exception":false,"start_time":"2022-08-25T00:42:12.567432","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# plt.imshow(t), t.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.849843,"end_time":"2022-08-25T00:42:14.956797","exception":false,"start_time":"2022-08-25T00:42:14.106954","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# plt.imshow(p), p.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.880937,"end_time":"2022-08-25T00:42:16.904682","exception":false,"start_time":"2022-08-25T00:42:16.023745","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.7.0 ('.venv': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"vscode":{"interpreter":{"hash":"21a2a557654fc1676068684031cf9bb9dfda94e124d3623f4e9c9ed764d794ac"}}},"nbformat":4,"nbformat_minor":4}
