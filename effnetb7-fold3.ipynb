{"cells":[{"cell_type":"markdown","metadata":{"papermill":{"duration":0.007246,"end_time":"2022-08-16T19:02:52.302567","exception":false,"start_time":"2022-08-16T19:02:52.295321","status":"completed"},"tags":[]},"source":["## Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-08-17T06:18:41.221301Z","iopub.status.busy":"2022-08-17T06:18:41.220595Z","iopub.status.idle":"2022-08-17T06:18:45.749538Z","shell.execute_reply":"2022-08-17T06:18:45.748207Z","shell.execute_reply.started":"2022-08-17T06:18:41.221215Z"},"papermill":{"duration":5.945852,"end_time":"2022-08-16T19:02:58.254572","exception":false,"start_time":"2022-08-16T19:02:52.308720","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/lakshita/somusan/hubmap_kaggle/.venv/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import cv2\n","import matplotlib.pyplot as plt\n","import time\n","import random\n","\n","import torch\n","from torch import nn\n","import torch.cuda.amp as amp\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torch.utils.data import RandomSampler \n","from torch.utils.data import SequentialSampler\n","import torch.nn.functional as F\n","# from torchmetrics.functional import dice_score\n","from torch.optim.lr_scheduler import StepLR\n","import tifffile\n","from fastai.vision.all import *\n","\n","from collections import defaultdict\n","\n","import torch\n","from torch.optim.optimizer import Optimizer\n","import itertools as it\n","\n","is_amp = True\n","import logging\n","\n","from sklearn.model_selection import KFold\n","\n","\n","from itertools import repeat\n","import collections.abc\n","import math\n","import torch\n","from torch import Tensor\n","from torch.optim.optimizer import Optimizer, required\n","from collections import defaultdict\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","import sys\n","sys.path.append('/home/lakshita/somusan/hubmap_kaggle/nbs/timm-pytorch-image-models/pytorch-image-models-master')\n","import timm\n","\n","sys.path.append('/home/lakshita/somusan/hubmap_kaggle/nbs/efficientnet-pytorch/EfficientNet-PyTorch/EfficientNet-PyTorch-master')\n","\n","from efficientnet_pytorch import EfficientNet\n","from efficientnet_pytorch.utils import url_map, url_map_advprop, get_model_params"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.005742,"end_time":"2022-08-16T19:02:58.266495","exception":false,"start_time":"2022-08-16T19:02:58.260753","status":"completed"},"tags":[]},"source":["## Directory"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-08-17T06:18:45.760015Z","iopub.status.busy":"2022-08-17T06:18:45.758472Z","iopub.status.idle":"2022-08-17T06:18:48.126151Z","shell.execute_reply":"2022-08-17T06:18:48.124647Z","shell.execute_reply.started":"2022-08-17T06:18:45.759971Z"},"papermill":{"duration":2.014493,"end_time":"2022-08-16T19:03:00.286680","exception":false,"start_time":"2022-08-16T19:02:58.272187","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["mkdir: cannot create directory ‘/home/lakshita/somusan/hubmap_kaggle/nbs/result’: File exists\n","mkdir: cannot create directory ‘/home/lakshita/somusan/hubmap_kaggle/nbs/checkpoint’: File exists\n"]}],"source":["!mkdir /home/lakshita/somusan/hubmap_kaggle/nbs/result\n","!mkdir /home/lakshita/somusan/hubmap_kaggle/nbs/checkpoint\n","\n","root_dir = '/home/lakshita/somusan/hubmap_kaggle/nbs'\n","#pretrain_dir = '/kaggle/input/swin-tiny-small-22k-pretrained/'\n","\n","TRAIN = '/home/lakshita/somusan/hubmap_kaggle/hubmap_data/hubmap-organ-segmentation/train_images'\n","LABELS = '/home/lakshita/somusan/hubmap_kaggle/hubmap_data/hubmap-organ-segmentation/train.csv'"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.005637,"end_time":"2022-08-16T19:03:00.299522","exception":false,"start_time":"2022-08-16T19:03:00.293885","status":"completed"},"tags":[]},"source":["## Utility"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-08-17T06:18:48.134400Z","iopub.status.busy":"2022-08-17T06:18:48.131958Z","iopub.status.idle":"2022-08-17T06:18:48.204636Z","shell.execute_reply":"2022-08-17T06:18:48.203398Z","shell.execute_reply.started":"2022-08-17T06:18:48.134356Z"},"papermill":{"duration":0.053647,"end_time":"2022-08-16T19:03:00.359131","exception":false,"start_time":"2022-08-16T19:03:00.305484","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def image_to_tensor(image, mode='bgr'): #image mode\n","    if mode=='bgr':\n","        image = image[:,:,::-1]\n","    x = image\n","    x = x.transpose(2,0,1)\n","    x = np.ascontiguousarray(x)\n","    x = torch.tensor(x, dtype = torch.float)\n","    return x\n","\n","def mask_to_tensor(mask):\n","    x = mask\n","    #x = x.transpose(2, 0, 1)\n","    x = torch.tensor(x, dtype = torch.float)\n","    return x\n","\n","\n","class RGB(nn.Module):\n","    IMAGE_RGB_MEAN = [0.485, 0.456, 0.406] #[0.5, 0.5, 0.5]\n","    IMAGE_RGB_STD  = [0.229, 0.224, 0.225] #[0.5, 0.5, 0.5]\n","\n","    def __init__(self,):\n","        super(RGB, self).__init__()\n","        self.register_buffer('mean', torch.zeros(1,3,1,1))\n","        self.register_buffer('std', torch.ones(1,3,1,1))\n","        self.mean.data = torch.FloatTensor(self.IMAGE_RGB_MEAN).view(self.mean.shape)\n","        self.std.data = torch.FloatTensor(self.IMAGE_RGB_STD).view(self.std.shape)\n","\n","    def forward(self, x):\n","        x = (x-self.mean)/self.std\n","        return x\n","\n","def message(mode='print'):\n","    asterisk = ' '\n","    if mode==('print'):\n","        loss = batch_loss\n","    if mode==('log'):\n","        loss = train_loss\n","        if (iteration % iter_save == 0): asterisk = '*'\n","\n","    text = \\\n","        ('%0.2e   %08d%s %6.2f | '%(rate, iteration, asterisk, epoch,)).replace('e-0','e-').replace('e+0','e+') + \\\n","        '%4.3f  %4.3f   | '%(*valid_loss,) + \\\n","        '%4.3f   | '%(loss) + \\\n","        '%s' % ((time.time() - start_timer))\n","\n","    return text\n","\n","def rle_decode(mask_rle, shape):\n","    '''\n","    mask_rle: run-length as string formated (start length)\n","    shape: (height,width) of array to return \n","    Returns numpy array, 1 - mask, 0 - background\n","\n","    '''\n","    s = mask_rle.split()\n","    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n","    starts -= 1\n","    ends = starts + lengths\n","    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n","    for lo, hi in zip(starts, ends):\n","        img[lo:hi] = 1\n","    return img.reshape(shape).T\n","\n","\n","# ref.: https://www.kaggle.com/stainsby/fast-tested-rle\n","def rle_encode(img):\n","    '''\n","    img: numpy array, 1 - mask, 0 - background\n","    Returns run length as string formated\n","    '''\n","    pixels = img.flatten()\n","    pixels = np.concatenate([[0], pixels, [0]])\n","    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n","    runs[1::2] -= runs[::2]\n","    return ' '.join(str(x) for x in runs)\n","\n","# class Lookahead(Optimizer):\n","#     def __init__(self, optimizer, alpha=0.5, k=6):\n","\n","#         if not 0.0 <= alpha <= 1.0:\n","#             raise ValueError(f'Invalid slow update rate: {alpha}')\n","#         if not 1 <= k:\n","#             raise ValueError(f'Invalid lookahead steps: {k}')\n","\n","#         self.optimizer = optimizer\n","#         self.param_groups = self.optimizer.param_groups\n","#         self.alpha = alpha\n","#         self.k = k\n","#         for group in self.param_groups:\n","#             group[\"step_counter\"] = 0\n","\n","#         self.slow_weights = [\n","#                 [p.clone().detach() for p in group['params']]\n","#             for group in self.param_groups]\n","\n","#         for w in it.chain(*self.slow_weights):\n","#             w.requires_grad = False\n","#         self.state = optimizer.state\n","\n","#     def step(self, closure=None):\n","#         loss = None\n","#         if closure is not None:\n","#             loss = closure()\n","#         loss = self.optimizer.step()\n","\n","#         for group,slow_weights in zip(self.param_groups,self.slow_weights):\n","#             group['step_counter'] += 1\n","#             if group['step_counter'] % self.k != 0:\n","#                 continue\n","#             for p,q in zip(group['params'],slow_weights):\n","#                 if p.grad is None:\n","#                     continue\n","#                 q.data.add_(p.data - q.data, alpha=self.alpha )\n","#                 p.data.copy_(q.data)\n","#         return loss\n","\n","# class RAdam(Optimizer):\n","\n","#     def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n","#         defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n","#         self.buffer = [[None, None, None] for ind in range(10)]\n","#         super(RAdam, self).__init__(params, defaults)\n","\n","#     def __setstate__(self, state):\n","#         super(RAdam, self).__setstate__(state)\n","\n","#     def step(self, closure=None):\n","\n","#         loss = None\n","#         if closure is not None:\n","#             loss = closure()\n","\n","#         for group in self.param_groups:\n","\n","#             for p in group['params']:\n","#                 if p.grad is None:\n","#                     continue\n","#                 grad = p.grad.data.float()\n","#                 if grad.is_sparse:\n","#                     raise RuntimeError('RAdam does not support sparse gradients')\n","\n","#                 p_data_fp32 = p.data.float()\n","\n","#                 state = self.state[p]\n","\n","#                 if len(state) == 0:\n","#                     state['step'] = 0\n","#                     state['exp_avg'] = torch.zeros_like(p_data_fp32)\n","#                     state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n","#                 else:\n","#                     state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n","#                     state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n","\n","#                 exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n","#                 beta1, beta2 = group['betas']\n","\n","#                 exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value = 1 - beta2)\n","#                 exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n","\n","#                 state['step'] += 1\n","#                 buffered = self.buffer[int(state['step'] % 10)]\n","#                 if state['step'] == buffered[0]:\n","#                     N_sma, step_size = buffered[1], buffered[2]\n","#                 else:\n","#                     buffered[0] = state['step']\n","#                     beta2_t = beta2 ** state['step']\n","#                     N_sma_max = 2 / (1 - beta2) - 1\n","#                     N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n","#                     buffered[1] = N_sma\n","\n","#                     # more conservative since it's an approximated value\n","#                     if N_sma >= 5:\n","#                         step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n","#                     else:\n","#                         step_size = 1.0 / (1 - beta1 ** state['step'])\n","#                     buffered[2] = step_size\n","\n","#                 if group['weight_decay'] != 0:\n","#                     p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n","\n","#                 # more conservative since it's an approximated value\n","#                 if N_sma >= 5:\n","#                     denom = exp_avg_sq.sqrt().add_(group['eps'])\n","#                     p_data_fp32.addcdiv_(exp_avg, denom, value=-step_size * group['lr'])\n","#                 else:\n","#                     p_data_fp32.add_(exp_avg, alpha=-step_size * group['lr'])\n","\n","#                 p.data.copy_(p_data_fp32)\n","\n","#         return loss\n","\n","# class PlainRAdam(Optimizer):\n","\n","#     def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n","#         defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n","\n","#         super(PlainRAdam, self).__init__(params, defaults)\n","\n","#     def __setstate__(self, state):\n","#         super(PlainRAdam, self).__setstate__(state)\n","\n","#     def step(self, closure=None):\n","\n","#         loss = None\n","#         if closure is not None:\n","#             loss = closure()\n","\n","#         for group in self.param_groups:\n","\n","#             for p in group['params']:\n","#                 if p.grad is None:\n","#                     continue\n","#                 grad = p.grad.data.float()\n","#                 if grad.is_sparse:\n","#                     raise RuntimeError('RAdam does not support sparse gradients')\n","\n","#                 p_data_fp32 = p.data.float()\n","\n","#                 state = self.state[p]\n","\n","#                 if len(state) == 0:\n","#                     state['step'] = 0\n","#                     state['exp_avg'] = torch.zeros_like(p_data_fp32)\n","#                     state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n","#                 else:\n","#                     state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n","#                     state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n","\n","#                 exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n","#                 beta1, beta2 = group['betas']\n","\n","#                 exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n","#                 exp_avg.mul_(beta1).add_( grad, alpha= 1 - beta1)\n","\n","#                 state['step'] += 1\n","#                 beta2_t = beta2 ** state['step']\n","#                 N_sma_max = 2 / (1 - beta2) - 1\n","#                 N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n","\n","#                 if group['weight_decay'] != 0:\n","#                     p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n","\n","#                 # more conservative since it's an approximated value\n","#                 if N_sma >= 5:\n","#                     step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n","#                     denom = exp_avg_sq.sqrt().add_(group['eps'])\n","#                     p_data_fp32.addcdiv_(exp_avg, denom, value=-step_size)\n","#                 else:\n","#                     step_size = group['lr'] / (1 - beta1 ** state['step'])\n","#                     p_data_fp32.add_(exp_avg, alpha=-step_size )\n","\n","#                 p.data.copy_(p_data_fp32)\n","\n","#         return loss\n","\n","\n","def get_learning_rate(optimizer):\n","    return optimizer.param_groups[0]['lr']"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.005519,"end_time":"2022-08-16T19:03:00.370356","exception":false,"start_time":"2022-08-16T19:03:00.364837","status":"completed"},"tags":[]},"source":["## Augments"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-08-17T06:18:48.213072Z","iopub.status.busy":"2022-08-17T06:18:48.210690Z","iopub.status.idle":"2022-08-17T06:18:48.224027Z","shell.execute_reply":"2022-08-17T06:18:48.223000Z","shell.execute_reply.started":"2022-08-17T06:18:48.213032Z"},"papermill":{"duration":0.016754,"end_time":"2022-08-16T19:03:00.392910","exception":false,"start_time":"2022-08-16T19:03:00.376156","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def valid_augment5(image, mask, organ):\n","    #image, mask  = do_crop(image, mask, image_size, xy=(None,None))\n","    return image, mask\n","\n","def train_augment5b(image, mask, organ):\n","    image, mask = do_random_flip(image, mask)\n","    image, mask = do_random_rot90(image, mask)\n","\n","    for fn in np.random.choice([\n","        lambda image, mask: (image, mask),\n","        #lambda image, mask: do_random_noise(image, mask, mag=0.1),\n","        #lambda image, mask: do_random_contast(image, mask, mag=0.40),\n","        lambda image, mask: do_random_hsv(image, mask, mag=[0.40, 0.40, 0])\n","    ], 2): image, mask = fn(image, mask)\n","\n","    for fn in np.random.choice([\n","        lambda image, mask: (image, mask),\n","        lambda image, mask: do_random_rotate_scale(image, mask, angle=45, scale=[0.50, 2.0]),\n","    ], 1): image, mask = fn(image, mask)\n","\n","    return image, mask"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-08-17T06:18:48.229868Z","iopub.status.busy":"2022-08-17T06:18:48.229470Z","iopub.status.idle":"2022-08-17T06:18:48.258454Z","shell.execute_reply":"2022-08-17T06:18:48.257422Z","shell.execute_reply.started":"2022-08-17T06:18:48.229830Z"},"papermill":{"duration":0.024978,"end_time":"2022-08-16T19:03:00.423769","exception":false,"start_time":"2022-08-16T19:03:00.398791","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def do_random_flip(image, mask):\n","    if np.random.rand()>0.5:\n","        image = cv2.flip(image,0)\n","        mask = cv2.flip(mask,0)\n","    if np.random.rand()>0.5:\n","        image = cv2.flip(image,1)\n","        mask = cv2.flip(mask,1)\n","    if np.random.rand()>0.5:\n","        image = image.transpose(1,0,2)\n","        mask = mask.transpose(1,0)\n","    \n","    image = np.ascontiguousarray(image)\n","    mask = np.ascontiguousarray(mask)\n","    return image, mask\n","\n","def do_random_rot90(image, mask):\n","    r = np.random.choice([\n","        0,\n","        cv2.ROTATE_90_CLOCKWISE,\n","        cv2.ROTATE_90_COUNTERCLOCKWISE,\n","        cv2.ROTATE_180,\n","    ])\n","    if r==0:\n","        return image, mask\n","    else:\n","        image = cv2.rotate(image, r)\n","        mask = cv2.rotate(mask, r)\n","        return image, mask\n","    \n","def do_random_contast(image, mask, mag=0.3): #this thing kills the image and sets all pixels to 1\n","    alpha = 1 + random.uniform(-1,1)*mag\n","    image = image * alpha\n","    image = np.clip(image,0,1)\n","    return image, mask\n","\n","def do_random_hsv(image, mask, mag=[0.15,0.25,0.25]):\n","    image = (image*255).astype(np.uint8)\n","    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n","\n","    h = hsv[:, :, 0].astype(np.float32)  # hue\n","    s = hsv[:, :, 1].astype(np.float32)  # saturation\n","    v = hsv[:, :, 2].astype(np.float32)  # value\n","    h = (h*(1 + random.uniform(-1,1)*mag[0]))%180\n","    s =  s*(1 + random.uniform(-1,1)*mag[1])\n","    v =  v*(1 + random.uniform(-1,1)*mag[2])\n","\n","    hsv[:, :, 0] = np.clip(h,0,180).astype(np.uint8)\n","    hsv[:, :, 1] = np.clip(s,0,255).astype(np.uint8)\n","    hsv[:, :, 2] = np.clip(v,0,255).astype(np.uint8)\n","    image = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n","    image = image.astype(np.float32)/255\n","    return image, mask\n","\n","def do_random_noise(image, mask, mag=0.1): #also seems to kill the image and set to 1\n","    height, width = image.shape[:2]\n","    noise = np.random.uniform(-1,1, (height, width,1))*mag\n","    image = image + noise\n","    image = np.clip(image,0,1)\n","    return image, mask\n","\n","def do_random_rotate_scale(image, mask, angle=30, scale=[0.8,1.2] ):\n","    angle = np.random.uniform(-angle, angle)\n","    scale = np.random.uniform(*scale) if scale is not None else 1\n","    \n","    height, width = image.shape[:2]\n","    center = (height // 2, width // 2)\n","    \n","    transform = cv2.getRotationMatrix2D(center, angle, scale)\n","    image = cv2.warpAffine( image, transform, (width, height), flags=cv2.INTER_LINEAR,\n","                            borderMode=cv2.BORDER_CONSTANT, borderValue=(0,0,0))\n","    mask  = cv2.warpAffine( mask, transform, (width, height), flags=cv2.INTER_LINEAR,\n","                            borderMode=cv2.BORDER_CONSTANT, borderValue=0)\n","    return image, mask"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-08-17T06:18:48.266794Z","iopub.status.busy":"2022-08-17T06:18:48.263553Z","iopub.status.idle":"2022-08-17T06:18:48.273504Z","shell.execute_reply":"2022-08-17T06:18:48.272295Z","shell.execute_reply.started":"2022-08-17T06:18:48.266744Z"},"papermill":{"duration":0.012714,"end_time":"2022-08-16T19:03:00.442141","exception":false,"start_time":"2022-08-16T19:03:00.429427","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# dummy_mask = np.zeros( (2023, 2023, 1) )\n","# dummy_mask[ dummy_mask.shape[0] // 2 : dummy_mask.shape[0] // 2 + 100, \n","#            dummy_mask.shape[0] // 2 : dummy_mask.shape[0] // 2 + 100] = 1\n","# # dummy_mask[150 : 170, 220 : 240] = 1\n","# # dummy_mask[300 : 320, 150 : 170] = 1\n","# dummy_mask[1000 : 1100, 450 : 550] = 1\n","# dummy_mask[1300 : 1400, 500 : 600] = 1\n","# dummy_mask[1100 : 1200, 700 : 800] = 1\n","# dummy_mask[800 : 900, 1000 : 1100] = 1\n","# dummy_mask[1350 : 1450, 1250 : 1350] = 1\n","\n","# plt.imshow(dummy_mask)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.005493,"end_time":"2022-08-16T19:03:00.454279","exception":false,"start_time":"2022-08-16T19:03:00.448786","status":"completed"},"tags":[]},"source":["## Dataset"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-08-17T06:18:48.281438Z","iopub.status.busy":"2022-08-17T06:18:48.278764Z","iopub.status.idle":"2022-08-17T06:18:48.300202Z","shell.execute_reply":"2022-08-17T06:18:48.298987Z","shell.execute_reply.started":"2022-08-17T06:18:48.281400Z"},"papermill":{"duration":0.02051,"end_time":"2022-08-16T19:03:00.480870","exception":false,"start_time":"2022-08-16T19:03:00.460360","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["image_size = 512\n","\n","class HubmapDataset(Dataset):\n","    def __init__(self, df, augment=None):\n","\n","        self.df = df\n","        self.augment = augment\n","        self.length = len(self.df)\n","        self.organ_to_label = {'kidney' : 0,\n","                               'prostate' : 1,\n","                               'largeintestine' : 2,\n","                               'spleen' : 3,\n","                               'lung' : 4}\n","\n","    def __str__(self):\n","        string = ''\n","        string += '\\tlen = %d\\n' % len(self)\n","\n","        d = self.df.organ.value_counts().to_dict()\n","        for k in ['kidney', 'prostate', 'largeintestine', 'spleen', 'lung']:\n","            string +=  '%24s %3d (%0.3f) \\n'%(k, d.get(k,0), d.get(k,0)/len(self.df))\n","        return string\n","\n","    def __len__(self):\n","        return self.length\n","\n","    def __getitem__(self, index):\n","        d = self.df.iloc[index]\n","        img_height = self.df.loc[index, 'img_height']\n","        img_width = self.df.loc[index, 'img_width']\n","        organ = self.organ_to_label[d.organ]\n","\n","        image = cv2.cvtColor(tifffile.imread(os.path.join(TRAIN, f'{d.id}.tiff')), cv2.COLOR_BGR2RGB)\n","        \n","        rle_mask = self.df.loc[index, 'rle']\n","        mask = rle_decode(rle_mask, (img_height, img_width))\n","        #mask = cv2.cvtColor(mask, cv2.IMREAD_GRAYSCALE)\n","        #mask = cv2.imread(os.path.join(MASKS,fname),cv2.IMREAD_GRAYSCALE)\n","        mask = np.expand_dims(mask, axis = 2)\n","        #print(mask.shape)\n","        \n","        image = image.astype(np.float32)/255\n","        #mask  = mask.astype(np.float32)/255\n","        mask = mask.astype(np.float32)\n","\n","        s = d.pixel_size/0.4 * (image_size/3000)\n","        image = cv2.resize(image,dsize=(image_size,image_size),interpolation=cv2.INTER_LINEAR)\n","        mask  = cv2.resize(mask, dsize=(image_size,image_size),interpolation=cv2.INTER_LINEAR)\n","\n","        if self.augment is not None:\n","            image, mask = self.augment(image, mask, organ)\n","\n","\n","        r ={}\n","        r['index']= index\n","        r['organ'] = torch.tensor([organ], dtype=torch.long)\n","        r['image'] = image_to_tensor(image)\n","        r['mask' ] = mask_to_tensor(mask)\n","        return r"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-08-17T06:18:48.307774Z","iopub.status.busy":"2022-08-17T06:18:48.305096Z","iopub.status.idle":"2022-08-17T06:18:48.316642Z","shell.execute_reply":"2022-08-17T06:18:48.315454Z","shell.execute_reply.started":"2022-08-17T06:18:48.307736Z"},"papermill":{"duration":0.01215,"end_time":"2022-08-16T19:03:00.498755","exception":false,"start_time":"2022-08-16T19:03:00.486605","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# df = pd.read_csv('../input/hubmap-organ-segmentation/train.csv')\n","# ds = HubmapDataset(df)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.00545,"end_time":"2022-08-16T19:03:00.509765","exception":false,"start_time":"2022-08-16T19:03:00.504315","status":"completed"},"tags":[]},"source":["## Model"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-08-17T06:18:48.324964Z","iopub.status.busy":"2022-08-17T06:18:48.321812Z","iopub.status.idle":"2022-08-17T06:18:48.389832Z","shell.execute_reply":"2022-08-17T06:18:48.388678Z","shell.execute_reply.started":"2022-08-17T06:18:48.324925Z"},"papermill":{"duration":0.049669,"end_time":"2022-08-16T19:03:00.565172","exception":false,"start_time":"2022-08-16T19:03:00.515503","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class FPN(nn.Module):\n","    def __init__(self, input_channels:list, output_channels:list):\n","        super().__init__()\n","        self.convs = nn.ModuleList(\n","            [nn.Sequential(nn.Conv2d(in_ch, out_ch*2, kernel_size=3, padding=1),\n","             nn.ReLU(inplace=True), nn.BatchNorm2d(out_ch*2),\n","             nn.Conv2d(out_ch*2, out_ch, kernel_size=3, padding=1))\n","            for in_ch, out_ch in zip(input_channels, output_channels)])\n","        \n","    def forward(self, xs:list, last_layer):\n","        hcs = [F.interpolate(c(x),scale_factor=2**(len(self.convs)-i),mode='bilinear') \n","               for i,(c,x) in enumerate(zip(self.convs, xs))]\n","        hcs.append(last_layer)\n","        return torch.cat(hcs, dim=1)\n","\n","class UnetBlock(nn.Module):\n","    def __init__(self, up_in_c:int, x_in_c:int, nf:int=None, blur:bool=False,\n","                 self_attention:bool=False, **kwargs):\n","        super().__init__()\n","        self.shuf = PixelShuffle_ICNR(up_in_c, up_in_c//2, blur=blur, **kwargs)\n","        self.bn = nn.BatchNorm2d(x_in_c)\n","        ni = up_in_c//2 + x_in_c\n","        nf = nf if nf is not None else max(up_in_c//2,32)\n","        self.conv1 = ConvLayer(ni, nf, norm_type=None, **kwargs)\n","        self.conv2 = ConvLayer(nf, nf, norm_type=None,\n","            xtra=SelfAttention(nf) if self_attention else None, **kwargs)\n","        self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, up_in:Tensor, left_in:Tensor) -> Tensor:\n","        s = left_in\n","        up_out = self.shuf(up_in)\n","        cat_x = self.relu(torch.cat([up_out, self.bn(s)], dim=1))\n","        return self.conv2(self.conv1(cat_x))\n","        \n","class _ASPPModule(nn.Module):\n","    def __init__(self, inplanes, planes, kernel_size, padding, dilation, groups=1):\n","        super().__init__()\n","        self.atrous_conv = nn.Conv2d(inplanes, planes, kernel_size=kernel_size,\n","                stride=1, padding=padding, dilation=dilation, bias=False, groups=groups)\n","        self.bn = nn.BatchNorm2d(planes)\n","        self.relu = nn.ReLU()\n","\n","        self._init_weight()\n","\n","    def forward(self, x):\n","        x = self.atrous_conv(x)\n","        x = self.bn(x)\n","\n","        return self.relu(x)\n","\n","    def _init_weight(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                torch.nn.init.kaiming_normal_(m.weight)\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","class ASPP(nn.Module):\n","    def __init__(self, inplanes=512, mid_c=256, dilations=[6, 12, 18, 24], out_c=None):\n","        super().__init__()\n","        self.aspps = [_ASPPModule(inplanes, mid_c, 1, padding=0, dilation=1)] + \\\n","            [_ASPPModule(inplanes, mid_c, 3, padding=d, dilation=d,groups=4) for d in dilations]\n","        self.aspps = nn.ModuleList(self.aspps)\n","        self.global_pool = nn.Sequential(nn.AdaptiveMaxPool2d((1, 1)),\n","                        nn.Conv2d(inplanes, mid_c, 1, stride=1, bias=False),\n","                        nn.BatchNorm2d(mid_c), nn.ReLU())\n","        out_c = out_c if out_c is not None else mid_c\n","        self.out_conv = nn.Sequential(nn.Conv2d(mid_c*(2+len(dilations)), out_c, 1, bias=False),\n","                                    nn.BatchNorm2d(out_c), nn.ReLU(inplace=True))\n","        self.conv1 = nn.Conv2d(mid_c*(2+len(dilations)), out_c, 1, bias=False)\n","        self._init_weight()\n","\n","    def forward(self, x):\n","        x0 = self.global_pool(x)\n","        xs = [aspp(x) for aspp in self.aspps]\n","        x0 = F.interpolate(x0, size=xs[0].size()[2:], mode='bilinear', align_corners=True)\n","        x = torch.cat([x0] + xs, dim=1)\n","        return self.out_conv(x)\n","    \n","    def _init_weight(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                torch.nn.init.kaiming_normal_(m.weight)\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","\n","class config:\n","    pretrained_root = '../input/efficientnet-pytorch/'\n","    efficient_net_encoders = {\n","        \"efficientnet-b0\": {\n","            \"out_channels\": (3, 32, 24, 40, 112, 320),\n","            \"stage_idxs\": (3, 5, 9, 16),\n","            \"weight_path\": pretrained_root + \"efficientnet-b0-08094119.pth\"\n","        },\n","        \"efficientnet-b1\": {\n","            \"out_channels\": (3, 32, 24, 40, 112, 320),\n","            \"stage_idxs\": (5, 8, 16, 23),\n","            \"weight_path\": pretrained_root + \"efficientnet-b1-dbc7070a.pth\"\n","        },\n","        \"efficientnet-b2\": {\n","            \"out_channels\": (3, 32, 24, 48, 120, 352),\n","            \"stage_idxs\": (5, 8, 16, 23),\n","            \"weight_path\": pretrained_root + \"efficientnet-b2-27687264.pth\"\n","        },\n","        \"efficientnet-b3\": {\n","            \"out_channels\": (3, 40, 32, 48, 136, 384),\n","            \"stage_idxs\": (5, 8, 18, 26),\n","            \"weight_path\": pretrained_root + \"efficientnet-b3-c8376fa2.pth\"\n","        },\n","        \"efficientnet-b4\": {\n","            \"out_channels\": (3, 48, 32, 56, 160, 448),\n","            \"stage_idxs\": (6, 10, 22, 32),\n","            \"weight_path\": pretrained_root + \"efficientnet-b4-e116e8b3.pth\"\n","        },\n","        \"efficientnet-b5\": {\n","            \"out_channels\": (3, 48, 40, 64, 176, 512),\n","            \"stage_idxs\": (8, 13, 27, 39),\n","            \"weight_path\": pretrained_root + \"efficientnet-b5-586e6cc6.pth\"\n","        },\n","        \"efficientnet-b6\": {\n","            \"out_channels\": (3, 56, 40, 72, 200, 576),\n","            \"stage_idxs\": (9, 15, 31, 45),\n","            \"weight_path\": pretrained_root + \"efficientnet-b6-c76e70fd.pth\"\n","        },\n","        \"efficientnet-b7\": {\n","            \"out_channels\": (3, 64, 48, 80, 224, 640),\n","            \"stage_idxs\": (11, 18, 38, 55),\n","            \"weight_path\": pretrained_root + \"efficientnet-b7-dcc49843.pth\"\n","        }\n","    }\n","    model = 'efficientnet-b7'\n","    \n","class EfficientNetEncoder(EfficientNet):\n","    def __init__(self, stage_idxs, out_channels, model_name, depth=5):\n","\n","        blocks_args, global_params = get_model_params(model_name, override_params=None)\n","        super().__init__(blocks_args, global_params)\n","        \n","        cfg = config.efficient_net_encoders[model_name]\n","\n","        self._stage_idxs = stage_idxs\n","        self._out_channels = out_channels\n","        self._depth = depth\n","        self._in_channels = 3\n","\n","        del self._fc\n","        self.load_state_dict(torch.load(cfg['weight_path']))\n","\n","    def get_stages(self):\n","        return [\n","            nn.Identity(),\n","            nn.Sequential(self._conv_stem, self._bn0, self._swish),\n","            self._blocks[:self._stage_idxs[0]],\n","            self._blocks[self._stage_idxs[0]:self._stage_idxs[1]],\n","            self._blocks[self._stage_idxs[1]:self._stage_idxs[2]],\n","            self._blocks[self._stage_idxs[2]:],\n","        ]\n","\n","    def forward(self, x):\n","        stages = self.get_stages()\n","\n","        block_number = 0.\n","        drop_connect_rate = self._global_params.drop_connect_rate\n","\n","        features = []\n","        for i in range(self._depth + 1):\n","\n","            # Identity and Sequential stages\n","            if i < 2:\n","                x = stages[i](x)\n","\n","            # Block stages need drop_connect rate\n","            else:\n","                for module in stages[i]:\n","                    drop_connect = drop_connect_rate * block_number / len(self._blocks)\n","                    block_number += 1.\n","                    x = module(x, drop_connect)\n","\n","            features.append(x)\n","\n","        return features\n","\n","    def load_state_dict(self, state_dict, **kwargs):\n","        state_dict.pop(\"_fc.bias\")\n","        state_dict.pop(\"_fc.weight\")\n","        super().load_state_dict(state_dict, **kwargs)  \n","        \n","\n","class EffUnet(nn.Module):\n","    def __init__(self, model_name, stride=1):\n","        super().__init__()\n","        \n","        cfg = config.efficient_net_encoders[model_name]\n","        stage_idxs = cfg['stage_idxs']\n","        out_channels = cfg['out_channels']\n","        \n","        self.encoder = EfficientNetEncoder(stage_idxs, out_channels, model_name)\n","\n","        #aspp with customized dilatations\n","        self.aspp = ASPP(out_channels[-1], 256, out_c=384, \n","                         dilations=[stride*1, stride*2, stride*3, stride*4])\n","        self.drop_aspp = nn.Dropout2d(0.5)\n","        #decoder\n","        self.dec4 = UnetBlock(384, out_channels[-2], 256)\n","        self.dec3 = UnetBlock(256, out_channels[-3], 128)\n","        self.dec2 = UnetBlock(128, out_channels[-4], 64)\n","        self.dec1 = UnetBlock(64, out_channels[-5], 32)\n","        self.fpn = FPN([384, 256, 128, 64], [16]*4)\n","        self.drop = nn.Dropout2d(0.1)\n","        self.final_conv = ConvLayer(32+16*4, 1, ks=1, norm_type=None, act_cls=None)\n","        \n","        self.rgb = RGB()\n","        \n","    def forward(self, batch):\n","        x = batch['image']\n","        B, C, H, W = x.shape\n","        x = self.rgb(x)\n","        enc0, enc1, enc2, enc3, enc4 = self.encoder(x)[-5:]\n","        enc5 = self.aspp(enc4)\n","        dec3 = self.dec4(self.drop_aspp(enc5), enc3)\n","        dec2 = self.dec3(dec3,enc2)\n","        dec1 = self.dec2(dec2,enc1)\n","        dec0 = self.dec1(dec1,enc0)\n","        x = self.fpn([enc5, dec3, dec2, dec1], dec0)\n","        x = self.final_conv(self.drop(x))\n","        x = F.interpolate(x, size = 512, mode = 'bilinear')\n","        return x"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-08-17T06:18:48.410006Z","iopub.status.busy":"2022-08-17T06:18:48.407591Z","iopub.status.idle":"2022-08-17T06:18:48.421723Z","shell.execute_reply":"2022-08-17T06:18:48.420446Z","shell.execute_reply.started":"2022-08-17T06:18:48.409968Z"},"papermill":{"duration":0.016373,"end_time":"2022-08-16T19:03:00.606286","exception":false,"start_time":"2022-08-16T19:03:00.589913","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def run_check_net():\n","    batch_size = 2\n","    image_size = 256\n","\n","    #---\n","    batch = {\n","        'image' : torch.from_numpy( np.random.uniform(-1,1,(batch_size,3,image_size,image_size)) ).float(),\n","        'mask'  : torch.from_numpy( np.random.choice(2,(batch_size,1,image_size,image_size)) ).float(),\n","        'organ' : torch.from_numpy( np.random.choice(5,(batch_size)) ).long(),\n","    }\n","    batch = {k:v.cuda() for k,v in batch.items()}\n","\n","    net = EffUnet(config.model).cuda()\n","\n","    with torch.no_grad():\n","        with torch.cuda.amp.autocast(enabled=True):\n","            output = net(batch)\n","\n","    print('batch')\n","    for k,v in batch.items():\n","        print('%32s :'%k, v.shape)\n","\n","    print('output')\n","    print( (' '*35 + f'{output.shape}'))\n","\n","#run_check_net()"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.00547,"end_time":"2022-08-16T19:03:00.617292","exception":false,"start_time":"2022-08-16T19:03:00.611822","status":"completed"},"tags":[]},"source":["## Folds"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-08-17T06:18:48.429843Z","iopub.status.busy":"2022-08-17T06:18:48.426734Z","iopub.status.idle":"2022-08-17T06:18:48.443398Z","shell.execute_reply":"2022-08-17T06:18:48.442495Z","shell.execute_reply.started":"2022-08-17T06:18:48.429803Z"},"papermill":{"duration":0.017484,"end_time":"2022-08-16T19:03:00.640381","exception":false,"start_time":"2022-08-16T19:03:00.622897","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def make_fold(fold = 3):\n","    df = pd.read_csv(LABELS)\n","\n","    num_fold = 4\n","    skf = KFold(n_splits = num_fold, shuffle = True,random_state = 42)\n","\n","    df.loc[:,'fold']=-1\n","    for f,(t_idx, v_idx) in enumerate(skf.split(X=df['id'], y=df['organ'])):\n","        df.iloc[v_idx,-1]=f\n","\n","    #check\n","    if 0:\n","        for f in range(num_fold):\n","            train_df=df[df.fold!=f].reset_index(drop=True)\n","            valid_df=df[df.fold==f].reset_index(drop=True)\n","\n","            print('fold %d'%f)\n","            t = train_df.organ.value_counts().to_dict()\n","            v = valid_df.organ.value_counts().to_dict()\n","            for k in ['kidney', 'prostate', 'largeintestine', 'spleen', 'lung']:\n","                print('%32s %3d (%0.3f)  %3d (%0.3f)'%(k,t.get(k,0),t.get(k,0)/len(train_df),v.get(k,0),v.get(k,0)/len(valid_df)))\n","\n","            print('')\n","            zz=0\n","\n","    train_df=df[df.fold!=fold].reset_index(drop=True)\n","    valid_df=df[df.fold==fold].reset_index(drop=True)\n","    return train_df,valid_df"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.005895,"end_time":"2022-08-16T19:03:00.651958","exception":false,"start_time":"2022-08-16T19:03:00.646063","status":"completed"},"tags":[]},"source":["## Competition metric"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-08-17T06:18:48.454933Z","iopub.status.busy":"2022-08-17T06:18:48.452286Z","iopub.status.idle":"2022-08-17T06:18:48.463652Z","shell.execute_reply":"2022-08-17T06:18:48.462634Z","shell.execute_reply.started":"2022-08-17T06:18:48.454894Z"},"papermill":{"duration":0.013577,"end_time":"2022-08-16T19:03:00.671364","exception":false,"start_time":"2022-08-16T19:03:00.657787","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def compute_dice_score(probability, mask, smooth = 1):\n","    N = len(probability)\n","    p = probability.reshape(N,-1)\n","    t = mask.reshape(N,-1)\n","\n","    p = p>0.5\n","    t = t>0.5\n","    uion = p.sum(-1) + t.sum(-1)\n","    overlap = (p*t).sum(-1)\n","    dice = 2*overlap/(uion+0.0001)\n","    return dice"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.005378,"end_time":"2022-08-16T19:03:00.682366","exception":false,"start_time":"2022-08-16T19:03:00.676988","status":"completed"},"tags":[]},"source":["## Validation"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-08-17T06:18:48.471856Z","iopub.status.busy":"2022-08-17T06:18:48.468907Z","iopub.status.idle":"2022-08-17T06:18:48.486188Z","shell.execute_reply":"2022-08-17T06:18:48.485069Z","shell.execute_reply.started":"2022-08-17T06:18:48.471796Z"},"papermill":{"duration":0.01812,"end_time":"2022-08-16T19:03:00.706195","exception":false,"start_time":"2022-08-16T19:03:00.688075","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def validate(net, valid_loader, debug = False):\n","    \n","    valid_num = 0\n","    valid_probability = []\n","    valid_mask = []\n","    valid_loss = 0\n","    \n","    criterion = nn.BCEWithLogitsLoss()\n","    \n","    net = net.eval()\n","    start_timer = time.time()\n","    \n","    for t, batch in enumerate(valid_loader):\n","        \n","        with torch.no_grad():\n","            with amp.autocast(enabled = is_amp):\n","                \n","                batch_size = len(batch['index'])\n","                batch['image'] = batch['image'].cuda()\n","                batch['mask' ] = batch['mask' ].cuda()\n","                batch['organ'] = batch['organ'].cuda()\n","\n","                output = net(batch)\n","                \n","                batch['mask'] = batch['mask'].unsqueeze(1)\n","                loss = criterion(output, batch['mask'])\n","        \n","        valid_probability.append(output.data.cpu().numpy())\n","        valid_mask.append(batch['mask'].data.cpu().numpy())\n","        valid_num += batch_size\n","        valid_loss += batch_size * loss.item()\n","        \n","        print('\\r %8d / %d  %s'%(valid_num, len(valid_loader.dataset),(time.time() - start_timer)),end='',flush=True)\n","    \n","    assert(valid_num == len(valid_loader.dataset))\n","\n","    probability = np.concatenate(valid_probability)\n","    mask = np.concatenate(valid_mask)\n","\n","    loss = valid_loss/valid_num\n","\n","    dice = compute_dice_score(probability, mask)\n","    dice = dice.mean()\n","    \n","    return [dice, loss]"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.00551,"end_time":"2022-08-16T19:03:00.717363","exception":false,"start_time":"2022-08-16T19:03:00.711853","status":"completed"},"tags":[]},"source":["## Initialize"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-08-17T06:18:48.494186Z","iopub.status.busy":"2022-08-17T06:18:48.491148Z","iopub.status.idle":"2022-08-17T06:18:57.748016Z","shell.execute_reply":"2022-08-17T06:18:57.746853Z","shell.execute_reply.started":"2022-08-17T06:18:48.494147Z"},"papermill":{"duration":8.210001,"end_time":"2022-08-16T19:03:08.932978","exception":false,"start_time":"2022-08-16T19:03:00.722977","status":"completed"},"tags":[],"trusted":true},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '../input/efficientnet-pytorch/efficientnet-b7-dcc49843.pth'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_16296/417261218.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menabled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_amp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEffUnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minitial_checkpoint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_16296/2528474493.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name, stride)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mout_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'out_channels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEfficientNetEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;31m#aspp with customized dilatations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_16296/2528474493.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, stage_idxs, out_channels, model_name, depth)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_stages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/somusan/hubmap_kaggle/.venv/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/somusan/hubmap_kaggle/.venv/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/somusan/hubmap_kaggle/.venv/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/efficientnet-pytorch/efficientnet-b7-dcc49843.pth'"]}],"source":["criterion = nn.BCEWithLogitsLoss()\n","\n","fold = 3\n","\n","out_dir = root_dir + '/result/effnetb7/fold-%d' % (fold)\n","initial_checkpoint = None\n","\n","start_lr = 5e-5\n","batch_size = 8\n","\n","## setup  ----------------------------------------\n","for f in ['checkpoint','train','valid','backup'] : os.makedirs(out_dir +'/'+f, exist_ok=True)\n","\n","\n","log = open(out_dir+'/log.train.txt',mode='a')\n","log.write('\\n--- [START %s] %s\\n\\n' % ('EfficientNet-b7', '-' * 64))\n","log.write('\\n')\n","\n","## dataset ----------------------------------------\n","log.write('** dataset setting **\\n')\n","\n","train_df, valid_df = make_fold(fold)\n","\n","train_dataset = HubmapDataset(train_df, train_augment5b)\n","valid_dataset = HubmapDataset(valid_df, valid_augment5)\n","\n","train_loader  = DataLoader(\n","    train_dataset,\n","    sampler = RandomSampler(train_dataset),\n","    batch_size  = batch_size,\n","    drop_last   = True,\n","    num_workers = 8,\n","    pin_memory  = False,\n","    worker_init_fn = lambda id: np.random.seed(torch.initial_seed() // 2 ** 32 + id)\n",")\n","\n","valid_loader = DataLoader(\n","    valid_dataset,\n","    sampler = SequentialSampler(valid_dataset),\n","    batch_size  = 8,\n","    drop_last   = False,\n","    num_workers = 4,\n","    pin_memory  = False\n",")\n","\n","\n","log.write('fold = %s\\n'%str(fold))\n","log.write('train_dataset : \\n%s\\n'%(train_dataset))\n","log.write('valid_dataset : \\n%s\\n'%(valid_dataset))\n","log.write('\\n')\n","\n","\n","## net ----------------------------------------\n","log.write('** net setting **\\n')\n","\n","scaler = amp.GradScaler(enabled = is_amp)\n","net = EffUnet(config.model).cuda()\n","\n","if initial_checkpoint is not None:\n","    f = torch.load(initial_checkpoint, map_location=lambda storage, loc: storage)\n","    start_iteration = f['iteration']\n","    start_epoch = f['epoch']\n","    state_dict  = f['state_dict']\n","    net.load_state_dict(state_dict,strict=False)  #True\n","else:\n","    start_iteration = 0\n","    start_epoch = 0\n","\n","\n","log.write('\\tinitial_checkpoint = %s\\n' % initial_checkpoint)\n","log.write('\\n')\n","\n","\n","## optimiser ----------------------------------\n","if 0: ##freeze\n","    for p in net.stem.parameters():   p.requires_grad = False\n","    pass\n","\n","def freeze_bn(net):\n","    for m in net.modules():\n","        if isinstance(m, nn.BatchNorm2d):\n","            m.eval()\n","            m.weight.requires_grad = False\n","            m.bias.requires_grad = False\n","            \n","#freeze_bn(net)\n","\n","#-----------------------------------------------\n","\n","optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, net.parameters()),lr=start_lr)\n","#optimizer = Lookahead(RAdam(filter(lambda p: p.requires_grad, net.parameters()),lr=start_lr), alpha=0.5, k=5)\n","\n","log.write('optimizer\\n  %s\\n'%(optimizer))\n","log.write('\\n')\n","\n","#num_iteration = 1000*len(train_loader)\n","num_iteration = 9000\n","iter_log   = len(train_loader)*3 #479\n","iter_valid = iter_log\n","iter_save  = iter_log"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([8, 3, 256, 256])"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["next(iter(valid_loader))[\"image\"].shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["items = next(iter(train_loader))\n","imgs = items[\"image\"].permute((0, 2, 3, 1))\n","msks = items[\"mask\"].permute((0, 2, 3, 1))\n","print(imgs.size(), msks.size())\n","\n","# torch.unique(msks)\n","\n","import matplotlib.pyplot as plt\n","def plot_batch(imgs, msks, size=3):\n","    for idx in range(size):\n","        plt.figure(figsize=(4*3, 5))\n","\n","        plt.subplot(1, 3, 1); plt.imshow(imgs[idx])\n","        plt.title('image', fontsize=15)\n","        plt.axis('OFF')\n","\n","        plt.subplot(1, 3, 2); plt.imshow(msks[idx])\n","        plt.title('mask', fontsize=15)\n","        plt.axis('OFF')\n","            \n","        plt.subplot(1, 3, 3); plt.imshow(imgs[idx]); plt.imshow(msks[idx], alpha=0.3)\n","        plt.title('overlay', fontsize=15)\n","        plt.axis('OFF')\n","        \n","        plt.tight_layout()\n","        plt.show()\n","\n","plot_batch(imgs, msks, size=3)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.006146,"end_time":"2022-08-16T19:03:08.945358","exception":false,"start_time":"2022-08-16T19:03:08.939212","status":"completed"},"tags":[]},"source":["## Training"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-08-17T06:18:57.755373Z","iopub.status.busy":"2022-08-17T06:18:57.752991Z"},"papermill":{"duration":2131.600859,"end_time":"2022-08-16T19:38:40.552279","exception":false,"start_time":"2022-08-16T19:03:08.951420","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["5.00e-5   00009008  272.97 | 0.709  0.121   | 0.017   | 4390.27815175056552   | 0.695   | 8.9550008773803715.00e-5   00000099    3.00 | 0.177  0.264   | 0.211   | 82.18287611007695.00e-5   00000198    6.00 | 0.221  0.198   | 0.181   | 128.342260599136355.00e-5   00000297    9.00 | 0.317  0.179   | 0.186   | 174.466472625732425.00e-5   00000396   12.00 | 0.459  0.171   | 0.182   | 220.651890516281135.00e-5   00000495   15.00 | 0.527  0.163   | 0.160   | 266.91068577766425.00e-5   00000594   18.00 | 0.540  0.157   | 0.155   | 313.07668781280525.00e-5   00000693   21.00 | 0.523  0.149   | 0.127   | 359.601087570190435.00e-5   00000792   24.00 | 0.554  0.143   | 0.148   | 406.287989377975465.00e-5   00000891   27.00 | 0.587  0.149   | 0.165   | 452.78152513504035.00e-5   00000990   30.00 | 0.588  0.137   | 0.120   | 499.68253636360175.00e-5   00001089   33.00 | 0.598  0.137   | 0.127   | 545.90124464035035.00e-5   00001188   36.00 | 0.610  0.132   | 0.107   | 591.92883563041695.00e-5   00001287   39.00 | 0.617  0.148   | 0.133   | 638.31198382377625.00e-5   00001386   42.00 | 0.626  0.133   | 0.146   | 685.00628852844245.00e-5   00001485   45.00 | 0.634  0.134   | 0.104   | 731.2481477260595.00e-5   00001584   48.00 | 0.644  0.126   | 0.116   | 777.43241310119635.00e-5   00001683   51.00 | 0.653  0.123   | 0.078   | 823.66479897499085.00e-5   00001782   54.00 | 0.645  0.122   | 0.113   | 871.28082919120795.00e-5   00001881   57.00 | 0.656  0.125   | 0.109   | 918.90296554565435.00e-5   00001980   60.00 | 0.662  0.121   | 0.090   | 965.92687320709235.00e-5   00002079   63.00 | 0.659  0.121   | 0.064   | 1012.96863770484925.00e-5   00002178   66.00 | 0.665  0.122   | 0.089   | 1059.59197139745.00e-5   00002277   69.00 | 0.669  0.119   | 0.080   | 1106.34682965278635.00e-5   00002376   72.00 | 0.668  0.117   | 0.075   | 1153.66076707839975.00e-5   00002475   75.00 | 0.674  0.127   | 0.081   | 1200.79668402671815.00e-5   00002574   78.00 | 0.669  0.116   | 0.097   | 1248.59892940521245.00e-5   00002673   81.00 | 0.680  0.116   | 0.089   | 1298.0831708908085.00e-5   00002772   84.00 | 0.688  0.120   | 0.080   | 1347.65966081619265.00e-5   00002871   87.00 | 0.689  0.112   | 0.088   | 1397.04258847236635.00e-5   00002970   90.00 | 0.695  0.116   | 0.103   | 1446.32665967941285.00e-5   00003069   93.00 | 0.695  0.109   | 0.062   | 1493.55625796318055.00e-5   00003168   96.00 | 0.696  0.119   | 0.098   | 1540.6679279804235.00e-5   00003267   99.00 | 0.691  0.109   | 0.058   | 1587.5089335441595.00e-5   00003366  102.00 | 0.677  0.111   | 0.083   | 1634.38529777526865.00e-5   00003465  105.00 | 0.703  0.121   | 0.095   | 1681.32117819786075.00e-5   00003564  108.00 | 0.697  0.117   | 0.088   | 1728.34667539596565.00e-5   00003663  111.00 | 0.702  0.117   | 0.071   | 1775.50996828079225.00e-5   00003762  114.00 | 0.692  0.118   | 0.051   | 1822.58903217315675.00e-5   00003861  117.00 | 0.704  0.120   | 0.053   | 1870.33408641815195.00e-5   00003960  120.00 | 0.704  0.116   | 0.055   | 1918.51061391830445.00e-5   00004059  123.00 | 0.698  0.113   | 0.068   | 1965.99078583717355.00e-5   00004158  126.00 | 0.703  0.115   | 0.070   | 2013.64474940299995.00e-5   00004257  129.00 | 0.703  0.119   | 0.054   | 2061.89567470550545.00e-5   00004356  132.00 | 0.694  0.113   | 0.049   | 2109.3945970535285.00e-5   00004455  135.00 | 0.706  0.109   | 0.056   | 2156.92536306381235.00e-5   00004554  138.00 | 0.705  0.114   | 0.055   | 2204.59447312355045.00e-5   00004653  141.00 | 0.702  0.121   | 0.077   | 2252.7238254547125.00e-5   00004752  144.00 | 0.715  0.114   | 0.056   | 2301.09735107421885.00e-5   00004851  147.00 | 0.706  0.109   | 0.062   | 2349.62907075881965.00e-5   00004950  150.00 | 0.711  0.111   | 0.044   | 2397.8921980857855.00e-5   00005049  153.00 | 0.713  0.116   | 0.058   | 2445.93685579299935.00e-5   00005148  156.00 | 0.696  0.111   | 0.061   | 2494.7900838851935.00e-5   00005247  159.00 | 0.707  0.117   | 0.048   | 2542.99905419349675.00e-5   00005346  162.00 | 0.695  0.113   | 0.044   | 2591.1767346858985.00e-5   00005445  165.00 | 0.704  0.111   | 0.047   | 2639.14523458480835.00e-5   00005544  168.00 | 0.714  0.115   | 0.044   | 2687.953218936925.00e-5   00005643  171.00 | 0.711  0.112   | 0.059   | 2737.79493641853335.00e-5   00005742  174.00 | 0.711  0.114   | 0.060   | 2786.792520761495.00e-5   00005841  177.00 | 0.716  0.116   | 0.057   | 2836.06172585487375.00e-5   00005940  180.00 | 0.712  0.116   | 0.053   | 2883.9565317630775.00e-5   00006039  183.00 | 0.710  0.122   | 0.040   | 2931.05557203292855.00e-5   00006138  186.00 | 0.708  0.118   | 0.056   | 2978.8022391796115.00e-5   00006237  189.00 | 0.719  0.117   | 0.046   | 3027.01836180686955.00e-5   00006336  192.00 | 0.706  0.114   | 0.052   | 3075.6297860145575.00e-5   00006435  195.00 | 0.711  0.118   | 0.044   | 3124.0389940738685.00e-5   00006534  198.00 | 0.715  0.124   | 0.076   | 3176.9157612323765.00e-5   00006633  201.00 | 0.717  0.118   | 0.072   | 3224.8314280509955.00e-5   00006732  204.00 | 0.708  0.116   | 0.045   | 3273.135827779775.00e-5   00006831  207.00 | 0.713  0.123   | 0.072   | 3320.6141211986545.00e-5   00006930  210.00 | 0.717  0.116   | 0.048   | 3368.17510962486275.00e-5   00007029  213.00 | 0.714  0.112   | 0.054   | 3416.2909979820255.00e-5   00007128  216.00 | 0.714  0.114   | 0.049   | 3466.6084408760075.00e-5   00007227  219.00 | 0.721  0.118   | 0.044   | 3514.8012690544135.00e-5   00007326  222.00 | 0.721  0.113   | 0.039   | 3563.28249645233155.00e-5   00007425  225.00 | 0.719  0.117   | 0.040   | 3611.1171262264255.00e-5   00007524  228.00 | 0.713  0.117   | 0.050   | 3659.0296185016635.00e-5   00007623  231.00 | 0.721  0.118   | 0.042   | 3712.74652290344245.00e-5   00007722  234.00 | 0.721  0.121   | 0.045   | 3761.30601620674135.00e-5   00007821  237.00 | 0.703  0.125   | 0.046   | 3809.02959895133975.00e-5   00007920  240.00 | 0.704  0.124   | 0.051   | 3858.30163002014165.00e-5   00008019  243.00 | 0.713  0.129   | 0.032   | 3907.970919609075.00e-5   00008118  246.00 | 0.712  0.119   | 0.040   | 3957.4756193161015.00e-5   00008217  249.00 | 0.708  0.119   | 0.041   | 4006.22687149047855.00e-5   00008316  252.00 | 0.717  0.115   | 0.041   | 4055.502793788915.00e-5   00008415  255.00 | 0.723  0.124   | 0.047   | 4104.0773787498475.00e-5   00008514  258.00 | 0.714  0.125   | 0.036   | 4152.40796208381655.00e-5   00008613  261.00 | 0.710  0.121   | 0.043   | 4200.3848021030435.00e-5   00008712  264.00 | 0.713  0.129   | 0.038   | 4248.5821225643165.00e-5   00008811  267.00 | 0.707  0.130   | 0.058   | 4297.7988998889925.00e-5   00008910  270.00 | 0.709  0.121   | 0.049   | 4350.969886541367"]}],"source":["log.write('** start training here! **\\n')\n","log.write('   batch_size = %d \\n'%(batch_size))\n","log.write('                     |-------------- VALID---------|---- TRAIN/BATCH ----------------\\n')\n","log.write('rate     iter  epoch | dice   loss   tp     tn     | loss           | time           \\n')\n","log.write('-------------------------------------------------------------------------------------\\n')\n","\n","valid_loss = np.zeros(2,np.float32)\n","train_loss = 0\n","batch_loss = 0\n","sum_train_loss = 0\n","sum_train = 0\n","\n","start_timer = time.time()\n","iteration = start_iteration\n","epoch = start_epoch\n","rate = 0\n","\n","while iteration < num_iteration:\n","    for t, batch in enumerate(train_loader):\n","\n","        if iteration%iter_save==0:\n","            if iteration != start_iteration:\n","                torch.save({\n","                    'state_dict': net.state_dict(),\n","                    'iteration': iteration,\n","                    'epoch': epoch,\n","                }, out_dir + '/checkpoint/%08d.model.pth' %  (iteration))\n","                pass\n","\n","\n","        if (iteration%iter_valid==0):\n","            valid_loss = validate(net, valid_loader)\n","            pass\n","\n","\n","        if (iteration%iter_log==0) or (iteration%iter_valid==0):\n","            print('\\r', end='', flush=True)\n","            log.write(message(mode='log') + '\\n')\n","\n","\n","        # learning rate schduler ------------\n","        rate = get_learning_rate(optimizer)\n","\n","        # one iteration update  -------------\n","        batch_size = len(batch['index'])\n","        \n","        #print(batch_size, iteration, epoch)\n","        batch['image'] = batch['image'].half().cuda()\n","        batch['mask' ] = batch['mask' ].half().cuda()\n","        batch['organ'] = batch['organ'].cuda()\n","\n","\n","        net.train()\n","        net.output_type = ['loss']\n","        if 1:\n","            with amp.autocast(enabled = is_amp):\n","                output = net(batch)\n","                batch['mask'] = batch['mask'].unsqueeze(1)\n","                loss = criterion(output, batch['mask'])\n","\n","            optimizer.zero_grad()\n","            scaler.scale(loss).backward()\n","\n","            scaler.unscale_(optimizer)\n","            scaler.step(optimizer)\n","            scaler.update()\n","\n","\n","        # print statistics  --------\n","        batch_loss = loss.item()\n","        sum_train_loss += batch_loss\n","        sum_train += 1\n","        if t % 100 == 0:\n","            train_loss = sum_train_loss / (sum_train + 1e-12)\n","            sum_train_loss = 0\n","            sum_train = 0\n","\n","        print('\\r', end='', flush=True)\n","        print(message(mode='print'), end='', flush=True)\n","        epoch += 1 / len(train_loader)\n","        iteration += 1\n","        \n","    torch.cuda.empty_cache()\n","    \n","log.write('\\n')\n","log.close()"]},{"cell_type":"code","execution_count":17,"metadata":{"papermill":{"duration":0.110188,"end_time":"2022-08-16T19:38:40.786715","exception":false,"start_time":"2022-08-16T19:38:40.676527","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Fri Aug 19 12:25:55 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.141.03   Driver Version: 470.141.03   CUDA Version: 11.4     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\n","| 56%   69C    P2    70W / 250W |   2978MiB / 11016MiB |     10%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|    0   N/A  N/A      2800      G   /usr/lib/xorg/Xorg                 18MiB |\n","|    0   N/A  N/A      3090      G   /usr/bin/gnome-shell               70MiB |\n","|    0   N/A  N/A      5666      G   /usr/lib/xorg/Xorg                150MiB |\n","|    0   N/A  N/A      5803      G   /usr/bin/gnome-shell               31MiB |\n","|    0   N/A  N/A      6687      G   ...RendererForSitePerProcess       61MiB |\n","|    0   N/A  N/A      7299      G   ...isable-features=EventPath       92MiB |\n","|    0   N/A  N/A     12793      C   ...p_kaggle/.venv/bin/python     2547MiB |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.7.0 ('.venv': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"vscode":{"interpreter":{"hash":"21a2a557654fc1676068684031cf9bb9dfda94e124d3623f4e9c9ed764d794ac"}}},"nbformat":4,"nbformat_minor":4}
