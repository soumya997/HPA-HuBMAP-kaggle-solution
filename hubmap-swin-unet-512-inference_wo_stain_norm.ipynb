{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7409926a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-08-25T15:34:26.709314Z",
     "iopub.status.busy": "2022-08-25T15:34:26.708591Z",
     "iopub.status.idle": "2022-08-25T15:34:30.723179Z",
     "shell.execute_reply": "2022-08-25T15:34:30.721980Z"
    },
    "papermill": {
     "duration": 4.028429,
     "end_time": "2022-08-25T15:34:30.726047",
     "exception": false,
     "start_time": "2022-08-25T15:34:26.697618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lakshita/somusan/hubmap_kaggle/.venv/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.cuda.amp as amp\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import RandomSampler \n",
    "from torch.utils.data import SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "import albumentations as A\n",
    "import gc\n",
    "\n",
    "# from torchmetrics.functional import dice_score\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import tifffile\n",
    "is_amp = True\n",
    "import logging\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import numpy as np\n",
    "from itertools import repeat\n",
    "import collections.abc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73c0e932",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T15:34:30.754033Z",
     "iopub.status.busy": "2022-08-25T15:34:30.753540Z",
     "iopub.status.idle": "2022-08-25T15:34:30.778682Z",
     "shell.execute_reply": "2022-08-25T15:34:30.777484Z"
    },
    "papermill": {
     "duration": 0.0419,
     "end_time": "2022-08-25T15:34:30.781520",
     "exception": false,
     "start_time": "2022-08-25T15:34:30.739620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# root_dir = '/kaggle/working/'\n",
    "# pretrain_dir = '/kaggle/input/swin-tiny-small-22k-pretrained/'\n",
    "\n",
    "# TRAIN = '../input/hubmap-2022-256x256/train/'\n",
    "# MASKS = '../input/hubmap-2022-256x256/masks/'\n",
    "# LABELS = '../input/hubmap-organ-segmentation/train.csv'\n",
    "\n",
    "df = pd.read_csv(\"/home/lakshita/somusan/hubmap_kaggle/hubmap_data/hubmap-organ-segmentation/train.csv\")\n",
    "\n",
    "# !mkdir /kaggle/working/result\n",
    "# !mkdir /kaggle/working/checkpoint\n",
    "\n",
    "root_dir = '.'\n",
    "pretrain_dir = '/home/lakshita/somusan/hubmap_kaggle/nbs/swin-tiny-small-22k-pretrained'\n",
    "\n",
    "TRAIN = '/home/lakshita/somusan/hubmap_kaggle/hubmap_data/hubmap-22-aug-pixel-size'\n",
    "MASKS = '/home/lakshita/somusan/hubmap_kaggle/hubmap_data/mask_png/train_binary_masks'\n",
    "LABELS = '/home/lakshita/somusan/hubmap_kaggle/hubmap_data/hubmap-organ-segmentation/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2fc36cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T15:34:30.815425Z",
     "iopub.status.busy": "2022-08-25T15:34:30.814929Z",
     "iopub.status.idle": "2022-08-25T15:34:30.844612Z",
     "shell.execute_reply": "2022-08-25T15:34:30.843590Z"
    },
    "papermill": {
     "duration": 0.047807,
     "end_time": "2022-08-25T15:34:30.847418",
     "exception": false,
     "start_time": "2022-08-25T15:34:30.799611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def image_to_tensor(image, mode='bgr'): #image mode\n",
    "    if mode=='bgr':\n",
    "        image = image[:,:,::-1]\n",
    "    x = image\n",
    "    x = x.transpose(2,0,1)\n",
    "    x = np.ascontiguousarray(x)\n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    return x\n",
    "\n",
    "\n",
    "def mask_to_tensor(mask):\n",
    "    x = mask\n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    return x\n",
    "\n",
    "\n",
    "tensor_list = ['mask', 'image', 'organ']\n",
    "\n",
    "def null_collate(batch):\n",
    "    d = {}\n",
    "    key = batch[0].keys()\n",
    "    for k in key:\n",
    "        v = [b[k] for b in batch]\n",
    "        if k in tensor_list:\n",
    "            v = torch.stack(v)\n",
    "        d[k] = v\n",
    "\n",
    "    d['mask'] = d['mask'].unsqueeze(1)\n",
    "    d['organ'] = d['organ'].reshape(-1)\n",
    "    return d\n",
    "\n",
    "\n",
    "def _ntuple(n):\n",
    "    def parse(x):\n",
    "        if isinstance(x, collections.abc.Iterable):\n",
    "            return x\n",
    "        return tuple(repeat(x, n))\n",
    "    return parse\n",
    "\n",
    "\n",
    "to_2tuple = _ntuple(2)\n",
    "\n",
    "\n",
    "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
    "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
    "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
    "    def norm_cdf(x):\n",
    "        # Computes standard normal cumulative distribution function\n",
    "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
    "\n",
    "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
    "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
    "                      \"The distribution of values may be incorrect.\",\n",
    "                      stacklevel=2)\n",
    "\n",
    "        \n",
    "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
    "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
    "\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n",
    "    if keep_prob > 0.0 and scale_by_keep:\n",
    "        random_tensor.div_(keep_prob)\n",
    "    return x * random_tensor\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob: float = 0., scale_by_keep: bool = True):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.scale_by_keep = scale_by_keep\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f'drop_prob={round(self.drop_prob,3):0.3f}'\n",
    "    \n",
    "    \n",
    "class RGB(nn.Module):\n",
    "    IMAGE_RGB_MEAN = [0.485, 0.456, 0.406] #[0.5, 0.5, 0.5]\n",
    "    IMAGE_RGB_STD  = [0.229, 0.224, 0.225] #[0.5, 0.5, 0.5]\n",
    "\n",
    "    def __init__(self,):\n",
    "        super(RGB, self).__init__()\n",
    "        self.register_buffer('mean', torch.zeros(1,3,1,1))\n",
    "        self.register_buffer('std', torch.ones(1,3,1,1))\n",
    "        self.mean.data = torch.FloatTensor(self.IMAGE_RGB_MEAN).view(self.mean.shape)\n",
    "        self.std.data = torch.FloatTensor(self.IMAGE_RGB_STD).view(self.std.shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x-self.mean)/self.std\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def message(mode='print'):\n",
    "    asterisk = ' '\n",
    "    if mode==('print'):\n",
    "        loss = batch_loss\n",
    "    if mode==('log'):\n",
    "        loss = train_loss\n",
    "        if (iteration % iter_save == 0): asterisk = '*'\n",
    "\n",
    "    text = \\\n",
    "        ('%0.2e   %08d%s %6.2f | '%(rate, iteration, asterisk, epoch,)).replace('e-0','e-').replace('e+0','e+') + \\\n",
    "        '%4.3f  %4.3f  %4.4f  %4.3f   | '%(*valid_loss,) + \\\n",
    "        '%4.3f  %4.3f   | '%(*loss,) + \\\n",
    "        '%s' % ((time.time() - start_timer))\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4afac788",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-08-25T15:34:30.872190Z",
     "iopub.status.busy": "2022-08-25T15:34:30.868311Z",
     "iopub.status.idle": "2022-08-25T15:34:30.896129Z",
     "shell.execute_reply": "2022-08-25T15:34:30.893141Z"
    },
    "papermill": {
     "duration": 0.042435,
     "end_time": "2022-08-25T15:34:30.899244",
     "exception": false,
     "start_time": "2022-08-25T15:34:30.856809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rle_encode(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = img.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def image_to_tensor(image, mode='bgr'): #image mode\n",
    "    if mode=='bgr':\n",
    "        image = image[:,:,::-1]\n",
    "    x = image\n",
    "    x = x.transpose(2,0,1)\n",
    "    x = np.ascontiguousarray(x)\n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    return x\n",
    "\n",
    "\n",
    "def mask_to_tensor(mask):\n",
    "    x = mask\n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    return x\n",
    "\n",
    "    \n",
    "    \n",
    "class RGB(nn.Module):\n",
    "    IMAGE_RGB_MEAN = [0.485, 0.456, 0.406] #[0.5, 0.5, 0.5]\n",
    "    IMAGE_RGB_STD  = [0.229, 0.224, 0.225] #[0.5, 0.5, 0.5]\n",
    "\n",
    "    def __init__(self,):\n",
    "        super(RGB, self).__init__()\n",
    "        self.register_buffer('mean', torch.zeros(1,3,1,1))\n",
    "        self.register_buffer('std', torch.ones(1,3,1,1))\n",
    "        self.mean.data = torch.FloatTensor(self.IMAGE_RGB_MEAN).view(self.mean.shape)\n",
    "        self.std.data = torch.FloatTensor(self.IMAGE_RGB_STD).view(self.std.shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x-self.mean)/self.std\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def message(mode='print'):\n",
    "    asterisk = ' '\n",
    "    if mode==('print'):\n",
    "        loss = batch_loss\n",
    "    if mode==('log'):\n",
    "        loss = train_loss\n",
    "        if (iteration % iter_save == 0): asterisk = '*'\n",
    "\n",
    "    text = \\\n",
    "        ('%0.2e   %08d%s %6.2f | '%(rate, iteration, asterisk, epoch,)).replace('e-0','e-').replace('e+0','e+') + \\\n",
    "        '%4.3f  %4.3f  %4.4f  %4.3f   | '%(*valid_loss,) + \\\n",
    "        '%4.3f  %4.3f   | '%(*loss,) + \\\n",
    "        '%s' % ((time.time() - start_timer))\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a1575c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T15:34:30.927287Z",
     "iopub.status.busy": "2022-08-25T15:34:30.926924Z",
     "iopub.status.idle": "2022-08-25T15:34:30.941317Z",
     "shell.execute_reply": "2022-08-25T15:34:30.940456Z"
    },
    "papermill": {
     "duration": 0.029913,
     "end_time": "2022-08-25T15:34:30.943844",
     "exception": false,
     "start_time": "2022-08-25T15:34:30.913931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def valid_augment5(image, mask, organ):\n",
    "    #image, mask  = do_crop(image, mask, image_size, xy=(None,None))\n",
    "    return image, mask\n",
    "\n",
    "def train_augment5b(image, mask, organ):\n",
    "    image, mask = do_random_flip(image, mask)\n",
    "    image, mask = do_random_rot90(image, mask)\n",
    "\n",
    "    for fn in np.random.choice([\n",
    "        lambda image, mask: (image, mask),\n",
    "        lambda image, mask: do_random_noise(image, mask, mag=0.1),\n",
    "        lambda image, mask: do_random_contast(image, mask, mag=0.40),\n",
    "        lambda image, mask: do_random_hsv(image, mask, mag=[0.40, 0.40, 0])\n",
    "    ], 2): image, mask = fn(image, mask)\n",
    "\n",
    "    for fn in np.random.choice([\n",
    "        lambda image, mask: (image, mask),\n",
    "        lambda image, mask: do_random_rotate_scale(image, mask, angle=45, scale=[0.50, 2.0]),\n",
    "    ], 1): image, mask = fn(image, mask)\n",
    "\n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559ae168",
   "metadata": {
    "papermill": {
     "duration": 0.011695,
     "end_time": "2022-08-25T15:34:30.967549",
     "exception": false,
     "start_time": "2022-08-25T15:34:30.955854",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:#c3448b; background:#efe9e9; border:1px dashed #efe50b;\" role=\"tab\" aria-controls=\"augmentations\"><center>Augmentations</center></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9bdcfb4",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-08-25T15:34:30.992827Z",
     "iopub.status.busy": "2022-08-25T15:34:30.992507Z",
     "iopub.status.idle": "2022-08-25T15:34:31.018176Z",
     "shell.execute_reply": "2022-08-25T15:34:31.017398Z"
    },
    "papermill": {
     "duration": 0.041216,
     "end_time": "2022-08-25T15:34:31.020637",
     "exception": false,
     "start_time": "2022-08-25T15:34:30.979421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def do_random_flip(image, mask):\n",
    "    if np.random.rand()>0.5:\n",
    "        image = cv2.flip(image,0)\n",
    "        mask = cv2.flip(mask,0)\n",
    "    if np.random.rand()>0.5:\n",
    "        image = cv2.flip(image,1)\n",
    "        mask = cv2.flip(mask,1)\n",
    "    if np.random.rand()>0.5:\n",
    "        image = image.transpose(1,0,2)\n",
    "        mask = mask.transpose(1,0)\n",
    "    \n",
    "    image = np.ascontiguousarray(image)\n",
    "    mask = np.ascontiguousarray(mask)\n",
    "    return image, mask\n",
    "\n",
    "def do_random_rot90(image, mask):\n",
    "    r = np.random.choice([\n",
    "        0,\n",
    "        cv2.ROTATE_90_CLOCKWISE,\n",
    "        cv2.ROTATE_90_COUNTERCLOCKWISE,\n",
    "        cv2.ROTATE_180,\n",
    "    ])\n",
    "    if r==0:\n",
    "        return image, mask\n",
    "    else:\n",
    "        image = cv2.rotate(image, r)\n",
    "        mask = cv2.rotate(mask, r)\n",
    "        return image, mask\n",
    "    \n",
    "def do_random_contast(image, mask, mag=0.3):\n",
    "    alpha = 1 + random.uniform(-1,1)*mag\n",
    "    image = image * alpha\n",
    "    image = np.clip(image,0,1)\n",
    "    return image, mask\n",
    "\n",
    "def do_random_hsv(image, mask, mag=[0.15,0.25,0.25]):\n",
    "    image = (image*255).astype(np.uint8)\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    h = hsv[:, :, 0].astype(np.float32)  # hue\n",
    "    s = hsv[:, :, 1].astype(np.float32)  # saturation\n",
    "    v = hsv[:, :, 2].astype(np.float32)  # value\n",
    "    h = (h*(1 + random.uniform(-1,1)*mag[0]))%180\n",
    "    s =  s*(1 + random.uniform(-1,1)*mag[1])\n",
    "    v =  v*(1 + random.uniform(-1,1)*mag[2])\n",
    "\n",
    "    hsv[:, :, 0] = np.clip(h,0,180).astype(np.uint8)\n",
    "    hsv[:, :, 1] = np.clip(s,0,255).astype(np.uint8)\n",
    "    hsv[:, :, 2] = np.clip(v,0,255).astype(np.uint8)\n",
    "    image = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "    image = image.astype(np.float32)/255\n",
    "    return image, mask\n",
    "\n",
    "def do_random_noise(image, mask, mag=0.1):\n",
    "    height, width = image.shape[:2]\n",
    "    noise = np.random.uniform(-1,1, (height, width,1))*mag\n",
    "    image = image + noise\n",
    "    image = np.clip(image,0,1)\n",
    "    return image, mask\n",
    "\n",
    "def do_random_rotate_scale(image, mask, angle=30, scale=[0.8,1.2] ):\n",
    "    angle = np.random.uniform(-angle, angle)\n",
    "    scale = np.random.uniform(*scale) if scale is not None else 1\n",
    "    \n",
    "    height, width = image.shape[:2]\n",
    "    center = (height // 2, width // 2)\n",
    "    \n",
    "    transform = cv2.getRotationMatrix2D(center, angle, scale)\n",
    "    image = cv2.warpAffine( image, transform, (width, height), flags=cv2.INTER_LINEAR,\n",
    "                            borderMode=cv2.BORDER_CONSTANT, borderValue=(0,0,0))\n",
    "    mask  = cv2.warpAffine( mask, transform, (width, height), flags=cv2.INTER_LINEAR,\n",
    "                            borderMode=cv2.BORDER_CONSTANT, borderValue=0)\n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128c9311",
   "metadata": {
    "papermill": {
     "duration": 0.012206,
     "end_time": "2022-08-25T15:34:31.046149",
     "exception": false,
     "start_time": "2022-08-25T15:34:31.033943",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:#c3448b; background:#efe9e9; border:1px dashed #efe50b;\" role=\"tab\" aria-controls=\"dataset\"><center>Dataset</center></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed05e7ca",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-08-25T15:34:31.070728Z",
     "iopub.status.busy": "2022-08-25T15:34:31.070424Z",
     "iopub.status.idle": "2022-08-25T15:34:31.088415Z",
     "shell.execute_reply": "2022-08-25T15:34:31.087626Z"
    },
    "papermill": {
     "duration": 0.033235,
     "end_time": "2022-08-25T15:34:31.091055",
     "exception": false,
     "start_time": "2022-08-25T15:34:31.057820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_size = 512\n",
    "\n",
    "class HubmapDataset(Dataset):\n",
    "    def __init__(self, df, augment=None):\n",
    "\n",
    "        self.df = df\n",
    "        self.augment = augment\n",
    "        self.length = len(self.df)\n",
    "        self.organ_to_label = {'kidney' : 0,\n",
    "                               'prostate' : 1,\n",
    "                               'largeintestine' : 2,\n",
    "                               'spleen' : 3,\n",
    "                               'lung' : 4}\n",
    "\n",
    "    def __str__(self):\n",
    "        string = ''\n",
    "        string += '\\tlen = %d\\n' % len(self)\n",
    "\n",
    "        d = self.df.organ.value_counts().to_dict()\n",
    "        for k in ['kidney', 'prostate', 'largeintestine', 'spleen', 'lung']:\n",
    "            string +=  '%24s %3d (%0.3f) \\n'%(k, d.get(k,0), d.get(k,0)/len(self.df))\n",
    "        return string\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        d = self.df.iloc[index]\n",
    "        img_height = self.df.loc[index, 'img_height']\n",
    "        img_width = self.df.loc[index, 'img_width']\n",
    "        organ = self.organ_to_label[d.organ]\n",
    "\n",
    "        image = cv2.cvtColor(tifffile.imread(os.path.join(TRAIN, f'{d.id}.tiff')), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        rle_mask = self.df.loc[index, 'rle']\n",
    "        mask = rle_decode(rle_mask, (img_height, img_width))\n",
    "        #mask = cv2.cvtColor(mask, cv2.IMREAD_GRAYSCALE)\n",
    "        #mask = cv2.imread(os.path.join(MASKS,fname),cv2.IMREAD_GRAYSCALE)\n",
    "        mask = np.expand_dims(mask, axis = 2)\n",
    "        #print(mask.shape)\n",
    "        \n",
    "        image = image.astype(np.float32)/255\n",
    "        #mask  = mask.astype(np.float32)/255\n",
    "        mask = mask.astype(np.float32)\n",
    "\n",
    "        s = d.pixel_size/0.4 * (image_size/3000)\n",
    "        image = cv2.resize(image,dsize=(image_size,image_size),interpolation=cv2.INTER_LINEAR)\n",
    "        mask  = cv2.resize(mask, dsize=(image_size,image_size),interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        if self.augment is not None:\n",
    "            image, mask = self.augment(image, mask, organ)\n",
    "\n",
    "\n",
    "        r ={}\n",
    "        r['index']= index\n",
    "        r['organ'] = torch.tensor([organ], dtype=torch.long)\n",
    "        r['image'] = image_to_tensor(image)\n",
    "        r['mask' ] = mask_to_tensor(mask)\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ebcccbb",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-08-25T15:34:31.117002Z",
     "iopub.status.busy": "2022-08-25T15:34:31.116682Z",
     "iopub.status.idle": "2022-08-25T15:34:31.133931Z",
     "shell.execute_reply": "2022-08-25T15:34:31.133182Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.033436,
     "end_time": "2022-08-25T15:34:31.136427",
     "exception": false,
     "start_time": "2022-08-25T15:34:31.102991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class FPN(nn.Module):\n",
    "#     def __init__(self, input_channels:list, output_channels:list):\n",
    "#         super().__init__()\n",
    "#         self.convs = nn.ModuleList(\n",
    "#             [nn.Sequential(nn.Conv2d(in_ch, out_ch*2, kernel_size=3, padding=1),\n",
    "#              nn.ReLU(inplace=True), nn.BatchNorm2d(out_ch*2),\n",
    "#              nn.Conv2d(out_ch*2, out_ch, kernel_size=3, padding=1))\n",
    "#             for in_ch, out_ch in zip(input_channels, output_channels)])\n",
    "        \n",
    "#     def forward(self, xs:list, last_layer):\n",
    "#         hcs = [F.interpolate(c(x),scale_factor=2**(len(self.convs)-i),mode='bilinear') \n",
    "#                for i,(c,x) in enumerate(zip(self.convs, xs))]\n",
    "#         hcs.append(last_layer)\n",
    "#         return torch.cat(hcs, dim=1)\n",
    "\n",
    "# class UnetBlock(nn.Module):\n",
    "#     def __init__(self, up_in_c:int, x_in_c:int, nf:int=None, blur:bool=False,\n",
    "#                  self_attention:bool=False, **kwargs):\n",
    "#         super().__init__()\n",
    "#         self.shuf = PixelShuffle_ICNR(up_in_c, up_in_c//2, blur=blur, **kwargs)\n",
    "#         self.bn = nn.BatchNorm2d(x_in_c)\n",
    "#         ni = up_in_c//2 + x_in_c\n",
    "#         nf = nf if nf is not None else max(up_in_c//2,32)\n",
    "#         self.conv1 = ConvLayer(ni, nf, norm_type=None, **kwargs)\n",
    "#         self.conv2 = ConvLayer(nf, nf, norm_type=None,\n",
    "#             xtra=SelfAttention(nf) if self_attention else None, **kwargs)\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "#     def forward(self, up_in:Tensor, left_in:Tensor) -> Tensor:\n",
    "#         s = left_in\n",
    "#         up_out = self.shuf(up_in)\n",
    "#         cat_x = self.relu(torch.cat([up_out, self.bn(s)], dim=1))\n",
    "#         return self.conv2(self.conv1(cat_x))\n",
    "        \n",
    "# class _ASPPModule(nn.Module):\n",
    "#     def __init__(self, inplanes, planes, kernel_size, padding, dilation, groups=1):\n",
    "#         super().__init__()\n",
    "#         self.atrous_conv = nn.Conv2d(inplanes, planes, kernel_size=kernel_size,\n",
    "#                 stride=1, padding=padding, dilation=dilation, bias=False, groups=groups)\n",
    "#         self.bn = nn.BatchNorm2d(planes)\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#         self._init_weight()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.atrous_conv(x)\n",
    "#         x = self.bn(x)\n",
    "\n",
    "#         return self.relu(x)\n",
    "\n",
    "#     def _init_weight(self):\n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m, nn.Conv2d):\n",
    "#                 torch.nn.init.kaiming_normal_(m.weight)\n",
    "#             elif isinstance(m, nn.BatchNorm2d):\n",
    "#                 m.weight.data.fill_(1)\n",
    "#                 m.bias.data.zero_()\n",
    "\n",
    "# class ASPP(nn.Module):\n",
    "#     def __init__(self, inplanes=512, mid_c=256, dilations=[6, 12, 18, 24], out_c=None):\n",
    "#         super().__init__()\n",
    "#         self.aspps = [_ASPPModule(inplanes, mid_c, 1, padding=0, dilation=1)] + \\\n",
    "#             [_ASPPModule(inplanes, mid_c, 3, padding=d, dilation=d,groups=4) for d in dilations]\n",
    "#         self.aspps = nn.ModuleList(self.aspps)\n",
    "#         self.global_pool = nn.Sequential(nn.AdaptiveMaxPool2d((1, 1)),\n",
    "#                         nn.Conv2d(inplanes, mid_c, 1, stride=1, bias=False),\n",
    "#                         nn.BatchNorm2d(mid_c), nn.ReLU())\n",
    "#         out_c = out_c if out_c is not None else mid_c\n",
    "#         self.out_conv = nn.Sequential(nn.Conv2d(mid_c*(2+len(dilations)), out_c, 1, bias=False),\n",
    "#                                     nn.BatchNorm2d(out_c), nn.ReLU(inplace=True))\n",
    "#         self.conv1 = nn.Conv2d(mid_c*(2+len(dilations)), out_c, 1, bias=False)\n",
    "#         self._init_weight()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x0 = self.global_pool(x)\n",
    "#         xs = [aspp(x) for aspp in self.aspps]\n",
    "#         x0 = F.interpolate(x0, size=xs[0].size()[2:], mode='bilinear', align_corners=True)\n",
    "#         x = torch.cat([x0] + xs, dim=1)\n",
    "#         return self.out_conv(x)\n",
    "    \n",
    "#     def _init_weight(self):\n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m, nn.Conv2d):\n",
    "#                 torch.nn.init.kaiming_normal_(m.weight)\n",
    "#             elif isinstance(m, nn.BatchNorm2d):\n",
    "#                 m.weight.data.fill_(1)\n",
    "#                 m.bias.data.zero_()\n",
    "\n",
    "\n",
    "# class config:\n",
    "#     pretrained_root = '../input/efficientnet-pytorch/'\n",
    "#     efficient_net_encoders = {\n",
    "#         \"efficientnet-b0\": {\n",
    "#             \"out_channels\": (3, 32, 24, 40, 112, 320),\n",
    "#             \"stage_idxs\": (3, 5, 9, 16),\n",
    "#             \"weight_path\": pretrained_root + \"efficientnet-b0-08094119.pth\"\n",
    "#         },\n",
    "#         \"efficientnet-b1\": {\n",
    "#             \"out_channels\": (3, 32, 24, 40, 112, 320),\n",
    "#             \"stage_idxs\": (5, 8, 16, 23),\n",
    "#             \"weight_path\": pretrained_root + \"efficientnet-b1-dbc7070a.pth\"\n",
    "#         },\n",
    "#         \"efficientnet-b2\": {\n",
    "#             \"out_channels\": (3, 32, 24, 48, 120, 352),\n",
    "#             \"stage_idxs\": (5, 8, 16, 23),\n",
    "#             \"weight_path\": pretrained_root + \"efficientnet-b2-27687264.pth\"\n",
    "#         },\n",
    "#         \"efficientnet-b3\": {\n",
    "#             \"out_channels\": (3, 40, 32, 48, 136, 384),\n",
    "#             \"stage_idxs\": (5, 8, 18, 26),\n",
    "#             \"weight_path\": pretrained_root + \"efficientnet-b3-c8376fa2.pth\"\n",
    "#         },\n",
    "#         \"efficientnet-b4\": {\n",
    "#             \"out_channels\": (3, 48, 32, 56, 160, 448),\n",
    "#             \"stage_idxs\": (6, 10, 22, 32),\n",
    "#             \"weight_path\": pretrained_root + \"efficientnet-b4-e116e8b3.pth\"\n",
    "#         },\n",
    "#         \"efficientnet-b5\": {\n",
    "#             \"out_channels\": (3, 48, 40, 64, 176, 512),\n",
    "#             \"stage_idxs\": (8, 13, 27, 39),\n",
    "#             \"weight_path\": pretrained_root + \"efficientnet-b5-586e6cc6.pth\"\n",
    "#         },\n",
    "#         \"efficientnet-b6\": {\n",
    "#             \"out_channels\": (3, 56, 40, 72, 200, 576),\n",
    "#             \"stage_idxs\": (9, 15, 31, 45),\n",
    "#             \"weight_path\": pretrained_root + \"efficientnet-b6-c76e70fd.pth\"\n",
    "#         },\n",
    "#         \"efficientnet-b7\": {\n",
    "#             \"out_channels\": (3, 64, 48, 80, 224, 640),\n",
    "#             \"stage_idxs\": (11, 18, 38, 55),\n",
    "#             \"weight_path\": pretrained_root + \"efficientnet-b7-dcc49843.pth\"\n",
    "#         }\n",
    "#     }\n",
    "#     model = 'efficientnet-b7'\n",
    "    \n",
    "# class EfficientNetEncoder(EfficientNet):\n",
    "#     def __init__(self, stage_idxs, out_channels, model_name, depth=5):\n",
    "\n",
    "#         blocks_args, global_params = get_model_params(model_name, override_params=None)\n",
    "#         super().__init__(blocks_args, global_params)\n",
    "        \n",
    "#         cfg = config.efficient_net_encoders[model_name]\n",
    "\n",
    "#         self._stage_idxs = stage_idxs\n",
    "#         self._out_channels = out_channels\n",
    "#         self._depth = depth\n",
    "#         self._in_channels = 3\n",
    "\n",
    "#         del self._fc\n",
    "#         self.load_state_dict(torch.load(cfg['weight_path']))\n",
    "\n",
    "#     def get_stages(self):\n",
    "#         return [\n",
    "#             nn.Identity(),\n",
    "#             nn.Sequential(self._conv_stem, self._bn0, self._swish),\n",
    "#             self._blocks[:self._stage_idxs[0]],\n",
    "#             self._blocks[self._stage_idxs[0]:self._stage_idxs[1]],\n",
    "#             self._blocks[self._stage_idxs[1]:self._stage_idxs[2]],\n",
    "#             self._blocks[self._stage_idxs[2]:],\n",
    "#         ]\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         stages = self.get_stages()\n",
    "\n",
    "#         block_number = 0.\n",
    "#         drop_connect_rate = self._global_params.drop_connect_rate\n",
    "\n",
    "#         features = []\n",
    "#         for i in range(self._depth + 1):\n",
    "\n",
    "#             # Identity and Sequential stages\n",
    "#             if i < 2:\n",
    "#                 x = stages[i](x)\n",
    "\n",
    "#             # Block stages need drop_connect rate\n",
    "#             else:\n",
    "#                 for module in stages[i]:\n",
    "#                     drop_connect = drop_connect_rate * block_number / len(self._blocks)\n",
    "#                     block_number += 1.\n",
    "#                     x = module(x, drop_connect)\n",
    "\n",
    "#             features.append(x)\n",
    "\n",
    "#         return features\n",
    "\n",
    "#     def load_state_dict(self, state_dict, **kwargs):\n",
    "#         state_dict.pop(\"_fc.bias\")\n",
    "#         state_dict.pop(\"_fc.weight\")\n",
    "#         super().load_state_dict(state_dict, **kwargs)  \n",
    "        \n",
    "\n",
    "# class EffUnet(nn.Module):\n",
    "#     def __init__(self, model_name, stride=1):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         cfg = config.efficient_net_encoders[model_name]\n",
    "#         stage_idxs = cfg['stage_idxs']\n",
    "#         out_channels = cfg['out_channels']\n",
    "        \n",
    "#         self.encoder = EfficientNetEncoder(stage_idxs, out_channels, model_name)\n",
    "\n",
    "#         #aspp with customized dilatations\n",
    "#         self.aspp = ASPP(out_channels[-1], 256, out_c=384, \n",
    "#                          dilations=[stride*1, stride*2, stride*3, stride*4])\n",
    "#         self.drop_aspp = nn.Dropout2d(0.5)\n",
    "#         #decoder\n",
    "#         self.dec4 = UnetBlock(384, out_channels[-2], 256)\n",
    "#         self.dec3 = UnetBlock(256, out_channels[-3], 128)\n",
    "#         self.dec2 = UnetBlock(128, out_channels[-4], 64)\n",
    "#         self.dec1 = UnetBlock(64, out_channels[-5], 32)\n",
    "#         self.fpn = FPN([384, 256, 128, 64], [16]*4)\n",
    "#         self.drop = nn.Dropout2d(0.1)\n",
    "#         self.final_conv = ConvLayer(32+16*4, 1, ks=1, norm_type=None, act_cls=None)\n",
    "        \n",
    "#         self.rgb = RGB()\n",
    "        \n",
    "#     def forward(self, batch):\n",
    "#         x = batch['image']\n",
    "#         B, C, H, W = x.shape\n",
    "#         x = self.rgb(x)\n",
    "#         enc0, enc1, enc2, enc3, enc4 = self.encoder(x)[-5:]\n",
    "#         enc5 = self.aspp(enc4)\n",
    "#         dec3 = self.dec4(self.drop_aspp(enc5), enc3)\n",
    "#         dec2 = self.dec3(dec3,enc2)\n",
    "#         dec1 = self.dec2(dec2,enc1)\n",
    "#         dec0 = self.dec1(dec1,enc0)\n",
    "#         x = self.fpn([enc5, dec3, dec2, dec1], dec0)\n",
    "#         x = self.final_conv(self.drop(x))\n",
    "#         x = F.interpolate(x, size = 512, mode = 'bilinear')\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "854eee24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T15:34:31.162625Z",
     "iopub.status.busy": "2022-08-25T15:34:31.162321Z",
     "iopub.status.idle": "2022-08-25T15:34:31.302279Z",
     "shell.execute_reply": "2022-08-25T15:34:31.301382Z"
    },
    "papermill": {
     "duration": 0.156249,
     "end_time": "2022-08-25T15:34:31.305084",
     "exception": false,
     "start_time": "2022-08-25T15:34:31.148835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    r\"\"\" Image to Patch Embedding\n",
    "\n",
    "    Args:\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 patch_size=4,\n",
    "                 in_chans=3,\n",
    "                 embed_dim=96,\n",
    "                 norm_layer=None\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        # padding\n",
    "        if W % self.patch_size[1] != 0:\n",
    "            x = F.pad(x, (0, self.patch_size[1] - W % self.patch_size[1]))\n",
    "        if H % self.patch_size[0] != 0:\n",
    "            x = F.pad(x, (0, 0, 0, self.patch_size[0] - H % self.patch_size[0]))\n",
    "\n",
    "\n",
    "        x = self.proj(x)  # B C Wh Ww\n",
    "        if self.norm is not None:\n",
    "            Wh, Ww = x.size(2), x.size(3)\n",
    "            x = x.flatten(2).transpose(1, 2)\n",
    "            x = self.norm(x)\n",
    "            x = x.transpose(1, 2).view(-1, self.embed_dim, Wh, Ww)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class PatchMerging(nn.Module):\n",
    "    r\"\"\" Patch Merging Layer.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input feature, tensor size (B, H*W, C).\n",
    "            H, W: Spatial resolution of the input feature.\n",
    "        \"\"\"\n",
    "\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "        # padding\n",
    "        pad_input = (H % 2 == 1) or (W % 2 == 1)\n",
    "        if pad_input:\n",
    "            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B, H/2, W/2, 4*C\n",
    "        x = x.view(B, -1, 4 * C)  # B, H/2*W/2, 4*C\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class BasicLayer(nn.Module):\n",
    "    \"\"\" A basic Swin Transformer layer for one stage.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Local window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "        fused_window_process (bool, optional): If True, use one kernel to fused window shift & window partition for acceleration, similar for the reversed part. Default: False\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        dim,\n",
    "        depth,\n",
    "        num_heads,\n",
    "        window_size,\n",
    "        mlp_ratio=4.,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop=0.,\n",
    "        attn_drop=0.,\n",
    "        drop_path=0.,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        downsample=None,\n",
    "        #use_checkpoint=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = window_size // 2\n",
    "        self.depth = depth\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SwinTransformerBlock(\n",
    "                dim=dim,\n",
    "                num_heads=num_heads,\n",
    "                window_size=window_size,\n",
    "                shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale,\n",
    "                drop=drop,\n",
    "                attn_drop=attn_drop,\n",
    "                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                norm_layer=norm_layer,\n",
    "            )\n",
    "            for i in range(depth)\n",
    "        ])\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input feature, tensor size (B, H*W, C).\n",
    "            H, W: Spatial resolution of the input feature.\n",
    "        \"\"\"\n",
    "\n",
    "        # calculate attention mask for SW-MSA ----\n",
    "        Hp = int(np.ceil(H / self.window_size)) * self.window_size\n",
    "        Wp = int(np.ceil(W / self.window_size)) * self.window_size\n",
    "        img_mask = torch.zeros((1, Hp, Wp, 1), device=x.device)  # 1 Hp Wp 1\n",
    "        h_slices = (slice(0, -self.window_size),\n",
    "                    slice(-self.window_size, -self.shift_size),\n",
    "                    slice(-self.shift_size, None))\n",
    "        w_slices = (slice(0, -self.window_size),\n",
    "                    slice(-self.window_size, -self.shift_size),\n",
    "                    slice(-self.shift_size, None))\n",
    "        cnt = 0\n",
    "        for h in h_slices:\n",
    "            for w in w_slices:\n",
    "                img_mask[:, h, w, :] = cnt\n",
    "                cnt += 1\n",
    "\n",
    "        mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "        #------\n",
    "\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, H, W, attn_mask)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            x_down = self.downsample(x, H, W)\n",
    "            Wh, Ww = (H + 1) // 2, (W + 1) // 2\n",
    "            return x, H, W, x_down, Wh, Ww\n",
    "        else:\n",
    "            return x, H, W, x, H, W\n",
    "        \n",
    "        \n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    r\"\"\" Swin Transformer Block.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resulotion.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        shift_size (int): Shift size for SW-MSA.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "        fused_window_process (bool, optional): If True, use one kernel to fused window shift & window partition for acceleration, similar for the reversed part. Default: False\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        dim,\n",
    "        num_heads,\n",
    "        window_size=7,\n",
    "        shift_size=0,\n",
    "        mlp_ratio=4.,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop=0.,\n",
    "        attn_drop=0.,\n",
    "        drop_path=0.,\n",
    "        act_layer=nn.GELU,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim,\n",
    "            window_size=to_2tuple(self.window_size),\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop,\n",
    "        )\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "\n",
    "    def forward(self, x, H, W, mask_matrix):\n",
    "\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # pad feature maps to multiples of window size\n",
    "        pad_l = pad_t = 0\n",
    "        pad_r = (self.window_size - W % self.window_size) % self.window_size\n",
    "        pad_b = (self.window_size - H % self.window_size) % self.window_size\n",
    "        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n",
    "        _, Hp, Wp, _ = x.shape\n",
    "\n",
    "\n",
    "        # cyclic shift ---\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "            attn_mask = mask_matrix\n",
    "        else:\n",
    "            shifted_x = x\n",
    "            attn_mask = None\n",
    "\n",
    "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n",
    "        attn_windows = self.attn(x_windows, mask=attn_mask)  # nW*B, window_size*window_size, C\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "\n",
    "\n",
    "        # reverse cyclic shift ---\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, Hp, Wp)  # B H' W' C\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "\n",
    "        if pad_r > 0 or pad_b > 0:\n",
    "            x = x[:, :H, :W, :].contiguous()\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, num_heads={self.num_heads}, \" \\\n",
    "               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n",
    "\n",
    "    \n",
    "class SwinTransformerV1(nn.Module):\n",
    "    def __init__(self,\n",
    "        pretrain_img_size=224,\n",
    "        patch_size=4,\n",
    "        in_chans=3,\n",
    "        embed_dim=96,\n",
    "        depths=[2, 2, 6, 2],\n",
    "        num_heads=[3, 6, 12, 24],\n",
    "        window_size=7,\n",
    "        mlp_ratio=4.,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.,\n",
    "        attn_drop_rate=0.,\n",
    "        drop_path_rate=0.1,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        patch_norm=True,\n",
    "        out_norm = nn.Identity, #use nn.Identity, nn.BatchNorm2d, LayerNorm2d\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pretrain_img_size = pretrain_img_size\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if patch_norm else None\n",
    "        )\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = np.linspace(0, drop_path_rate, sum(depths)).tolist() # stochastic depth decay rule\n",
    "\n",
    "        # build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(self.num_layers):\n",
    "            layer = BasicLayer(\n",
    "                dim=int(embed_dim * 2 ** i),\n",
    "                depth=depths[i],\n",
    "                num_heads=num_heads[i],\n",
    "                window_size=window_size,\n",
    "                mlp_ratio=self.mlp_ratio,\n",
    "                qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[sum(depths[:i]):sum(depths[:i + 1])],\n",
    "                norm_layer=norm_layer,\n",
    "                downsample=PatchMerging if (i < self.num_layers - 1) else None,\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        #---\n",
    "        # add a norm layer for each output\n",
    "        self.out_norm = nn.ModuleList(\n",
    "            [ out_norm(int(embed_dim * 2 ** i)) for i in range(self.num_layers)]\n",
    "        )\n",
    "\n",
    "        #---\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        Wh, Ww = x.size(2), x.size(3)\n",
    "\n",
    "        #positional encode?\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        outs = []\n",
    "        for i in range(self.num_layers):\n",
    "            x_out, H, W, x, Wh, Ww = self.layers[i](x, Wh, Ww)\n",
    "            out = x_out.view(-1, H, W, int(self.embed_dim * 2 ** i)).permute(0, 3, 1, 2).contiguous()\n",
    "            out = self.out_norm[i](out)\n",
    "            outs.append(out)\n",
    "\n",
    "        return outs\n",
    "class WindowAttention(nn.Module):\n",
    "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** (-0.5)\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        relative_position_bias = \\\n",
    "            self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], self.num_heads)\n",
    "            # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n",
    "    \n",
    "    \n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (int): window size\n",
    "\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (int): Window size\n",
    "        H (int): Height of image\n",
    "        W (int): Width of image\n",
    "\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "\n",
    "def conv3x3_bn_relu(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution + BN + relu\"\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "        nn.BatchNorm2d(out_planes),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "\n",
    "class UPerDecoder(nn.Module):\n",
    "    def __init__(self,\n",
    "        in_dim=[256, 512, 1024, 2048],\n",
    "        ppm_pool_scale=[1, 2, 3, 6],\n",
    "        ppm_dim=512,\n",
    "        fpn_out_dim=256\n",
    "    ):\n",
    "        super(UPerDecoder, self).__init__()\n",
    "\n",
    "        # PPM ----\n",
    "        dim = in_dim[-1]\n",
    "        ppm_pooling = []\n",
    "        ppm_conv = []\n",
    "\n",
    "        for scale in ppm_pool_scale:\n",
    "            ppm_pooling.append(\n",
    "                nn.AdaptiveAvgPool2d(scale)\n",
    "            )\n",
    "            ppm_conv.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(dim, ppm_dim, kernel_size=1, bias=False),\n",
    "                    nn.BatchNorm2d(ppm_dim),\n",
    "                    nn.ReLU(inplace=True)\n",
    "                )\n",
    "            )\n",
    "        self.ppm_pooling   = nn.ModuleList(ppm_pooling)\n",
    "        self.ppm_conv      = nn.ModuleList(ppm_conv)\n",
    "        self.ppm_out = conv3x3_bn_relu(dim + len(ppm_pool_scale)*ppm_dim, fpn_out_dim, 1)\n",
    "\n",
    "        # FPN ----\n",
    "        fpn_in = []\n",
    "        for i in range(0, len(in_dim)-1):  # skip the top layer\n",
    "            fpn_in.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_dim[i], fpn_out_dim, kernel_size=1, bias=False),\n",
    "                    nn.BatchNorm2d(fpn_out_dim),\n",
    "                    nn.ReLU(inplace=True)\n",
    "                )\n",
    "            )\n",
    "        self.fpn_in = nn.ModuleList(fpn_in)\n",
    "\n",
    "        fpn_out = []\n",
    "        for i in range(len(in_dim) - 1):  # skip the top layer\n",
    "            fpn_out.append(\n",
    "                conv3x3_bn_relu(fpn_out_dim, fpn_out_dim, 1),\n",
    "            )\n",
    "        self.fpn_out = nn.ModuleList(fpn_out)\n",
    "\n",
    "        self.fpn_fuse = nn.Sequential(\n",
    "            conv3x3_bn_relu(len(in_dim) * fpn_out_dim, fpn_out_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, feature):\n",
    "        f = feature[-1]\n",
    "        pool_shape = f.shape[2:]\n",
    "\n",
    "        ppm_out = [f]\n",
    "        for pool, conv in zip(self.ppm_pooling, self.ppm_conv):\n",
    "            p = pool(f)\n",
    "            p = F.interpolate(p, size=pool_shape, mode='bilinear', align_corners=False)\n",
    "            p = conv(p)\n",
    "            ppm_out.append(p)\n",
    "        ppm_out = torch.cat(ppm_out, 1)\n",
    "        down = self.ppm_out(ppm_out)\n",
    "\n",
    "        fpn_out = [down]\n",
    "        for i in reversed(range(len(feature) - 1)):\n",
    "            lateral = feature[i]\n",
    "            lateral = self.fpn_in[i](lateral) # lateral branch\n",
    "            down = F.interpolate(down, size=lateral.shape[2:], mode='bilinear', align_corners=False) # top-down branch\n",
    "            down = down + lateral\n",
    "            fpn_out.append(self.fpn_out[i](down))\n",
    "\n",
    "        fpn_out.reverse() # [P2 - P5]\n",
    "        fusion_shape = fpn_out[0].shape[2:]\n",
    "        fusion = [fpn_out[0]]\n",
    "        for i in range(1, len(fpn_out)):\n",
    "            fusion.append(\n",
    "                F.interpolate( fpn_out[i], fusion_shape, mode='bilinear', align_corners=False)\n",
    "            )\n",
    "        x = self.fpn_fuse( torch.cat(fusion, 1))\n",
    "\n",
    "        return x, fusion\n",
    "    \n",
    "    \n",
    "    \n",
    "class LayerNorm2d(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.bias = nn.Parameter(torch.zeros(dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.mean(1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.eps)\n",
    "        x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "def criterion_aux_loss(logit, mask):\n",
    "    mask = F.interpolate(mask,size=logit.shape[-2:], mode='nearest')\n",
    "    loss = F.binary_cross_entropy_with_logits(logit,mask)\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def load_pretrain( self,):\n",
    "\n",
    "        checkpoint = cfg[self.arch]['checkpoint']\n",
    "        print('loading %s ...'%checkpoint)\n",
    "        checkpoint = torch.load(checkpoint, map_location=lambda storage, loc: storage)['model']\n",
    "        if 0:\n",
    "            skip = ['relative_coords_table','relative_position_index']\n",
    "            filtered={}\n",
    "            for k,v in checkpoint.items():\n",
    "                if any([s in k for s in skip ]): continue\n",
    "                filtered[k]=v\n",
    "            checkpoint = filtered\n",
    "        print(self.encoder.load_state_dict(checkpoint,strict=False))  #True\n",
    "\n",
    "\n",
    "    def __init__( self,):\n",
    "        super(Net, self).__init__()\n",
    "        self.output_type = ['inference']\n",
    "\n",
    "        self.rgb = RGB()\n",
    "        self.arch = 'swin_tiny_patch4_window7_224'\n",
    "\n",
    "        self.encoder = SwinTransformerV1(\n",
    "            ** {**cfg['basic']['swin'], **cfg[self.arch]['swin'],\n",
    "                **{'out_norm' : LayerNorm2d} }\n",
    "        )\n",
    "        encoder_dim =cfg[self.arch]['upernet']['in_channels']\n",
    "        #[96, 192, 384, 768]\n",
    "\n",
    "        self.decoder = UPerDecoder(\n",
    "            in_dim=encoder_dim,\n",
    "            ppm_pool_scale=[1, 2, 3, 6],\n",
    "            ppm_dim=512,\n",
    "            fpn_out_dim=256\n",
    "        )\n",
    "\n",
    "        self.logit = nn.Sequential(\n",
    "            nn.Conv2d(256, 1, kernel_size=1)\n",
    "        )\n",
    "        self.aux = nn.ModuleList([\n",
    "            nn.Conv2d(256, 1, kernel_size=1, padding=0) for i in range(4)\n",
    "        ])\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x = batch['image']\n",
    "        B,C,H,W = x.shape\n",
    "        x = self.rgb(x)\n",
    "        encoder = self.encoder(x)\n",
    "        last, decoder = self.decoder(encoder)\n",
    "        logit = self.logit(last)\n",
    "        logit = F.interpolate(logit, size=None, scale_factor=4, mode='bilinear', align_corners=False)\n",
    "\n",
    "        output = {}\n",
    "        if 'loss' in self.output_type:\n",
    "            output['bce_loss'] = F.binary_cross_entropy_with_logits(logit,batch['mask'])\n",
    "            for i in range(4):\n",
    "                output['aux%d_loss'%i] = criterion_aux_loss(self.aux[i](decoder[i]),batch['mask'])\n",
    "\n",
    "        if 'inference' in self.output_type:\n",
    "            output['probability'] = torch.sigmoid(logit)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ca3ea9",
   "metadata": {
    "papermill": {
     "duration": 0.011793,
     "end_time": "2022-08-25T15:34:31.330323",
     "exception": false,
     "start_time": "2022-08-25T15:34:31.318530",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:#c3448b; background:#efe9e9; border:1px dashed #efe50b;\" role=\"tab\" aria-controls=\"folds\"><center>Folds</center></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43c217e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T15:34:31.356077Z",
     "iopub.status.busy": "2022-08-25T15:34:31.355696Z",
     "iopub.status.idle": "2022-08-25T15:34:31.373555Z",
     "shell.execute_reply": "2022-08-25T15:34:31.372787Z"
    },
    "papermill": {
     "duration": 0.033733,
     "end_time": "2022-08-25T15:34:31.376039",
     "exception": false,
     "start_time": "2022-08-25T15:34:31.342306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    \"\"\" Multilayer perceptron.\"\"\"\n",
    "\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "cfg = dict(\n",
    "\n",
    "        #configs/_base_/models/upernet_swin.py\n",
    "        basic = dict(\n",
    "            swin=dict(\n",
    "                embed_dim=96,\n",
    "                depths=[2, 2, 6, 2],\n",
    "                num_heads=[3, 6, 12, 24],\n",
    "                window_size=7,\n",
    "                mlp_ratio=4.,\n",
    "                qkv_bias=True,\n",
    "                qk_scale=None,\n",
    "                drop_rate=0.,\n",
    "                attn_drop_rate=0.,\n",
    "                drop_path_rate=0.3,\n",
    "                ape=False,\n",
    "                patch_norm=True,\n",
    "                out_indices=(0, 1, 2, 3),\n",
    "                use_checkpoint=False\n",
    "            ),\n",
    "\n",
    "        ),\n",
    "\n",
    "        #configs/swin/upernet_swin_tiny_patch4_window7_512x512_160k_ade20k.py\n",
    "        swin_tiny_patch4_window7_224=dict(\n",
    "            checkpoint = pretrain_dir+'/swin_tiny_patch4_window7_224_22k.pth',\n",
    "\n",
    "            swin = dict(\n",
    "                embed_dim=96,\n",
    "                depths=[2, 2, 6, 2],\n",
    "                num_heads=[3, 6, 12, 24],\n",
    "                window_size=7,\n",
    "                ape=False,\n",
    "                drop_path_rate=0.3,\n",
    "                patch_norm=True,\n",
    "                use_checkpoint=False,\n",
    "            ),\n",
    "            upernet=dict(\n",
    "                in_channels=[96, 192, 384, 768],\n",
    "            ),\n",
    "        ),\n",
    "\n",
    "        #/configs/swin/upernet_swin_small_patch4_window7_512x512_160k_ade20k.py\n",
    "        swin_small_patch4_window7_224_22k=dict(\n",
    "            checkpoint = pretrain_dir+'/swin_small_patch4_window7_224_22k.pth',\n",
    "\n",
    "            swin = dict(\n",
    "                embed_dim=96,\n",
    "                depths=[2, 2, 18, 2],\n",
    "                num_heads=[3, 6, 12, 24],\n",
    "                window_size=7,\n",
    "                ape=False,\n",
    "                drop_path_rate=0.3,\n",
    "                patch_norm=True,\n",
    "                use_checkpoint=False\n",
    "            ),\n",
    "            upernet=dict(\n",
    "                in_channels=[96, 192, 384, 768],\n",
    "            ),\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cc99f8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T15:34:31.401537Z",
     "iopub.status.busy": "2022-08-25T15:34:31.401232Z",
     "iopub.status.idle": "2022-08-25T15:34:31.414274Z",
     "shell.execute_reply": "2022-08-25T15:34:31.413499Z"
    },
    "papermill": {
     "duration": 0.028009,
     "end_time": "2022-08-25T15:34:31.416685",
     "exception": false,
     "start_time": "2022-08-25T15:34:31.388676",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_fold(fold=3):\n",
    "    df = pd.read_csv('../input/hubmap-organ-segmentation/train.csv')\n",
    "\n",
    "    num_fold = 5\n",
    "    skf = KFold(n_splits=num_fold, shuffle=True,random_state=42)\n",
    "\n",
    "    df.loc[:,'fold']=-1\n",
    "    for f,(t_idx, v_idx) in enumerate(skf.split(X=df['id'], y=df['organ'])):\n",
    "        df.iloc[v_idx,-1]=f\n",
    "\n",
    "    #check\n",
    "    if 0:\n",
    "        for f in range(num_fold):\n",
    "            train_df=df[df.fold!=f].reset_index(drop=True)\n",
    "            valid_df=df[df.fold==f].reset_index(drop=True)\n",
    "\n",
    "            print('fold %d'%f)\n",
    "            t = train_df.organ.value_counts().to_dict()\n",
    "            v = valid_df.organ.value_counts().to_dict()\n",
    "            for k in ['kidney', 'prostate', 'largeintestine', 'spleen', 'lung']:\n",
    "                print('%32s %3d (%0.3f)  %3d (%0.3f)'%(k,t.get(k,0),t.get(k,0)/len(train_df),v.get(k,0),v.get(k,0)/len(valid_df)))\n",
    "\n",
    "            print('')\n",
    "            zz=0\n",
    "\n",
    "    train_df=df[df.fold!=fold].reset_index(drop=True)\n",
    "    valid_df=df[df.fold==fold].reset_index(drop=True)\n",
    "    return train_df,valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f951024",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T15:34:31.442512Z",
     "iopub.status.busy": "2022-08-25T15:34:31.442194Z",
     "iopub.status.idle": "2022-08-25T15:34:31.448259Z",
     "shell.execute_reply": "2022-08-25T15:34:31.447467Z"
    },
    "papermill": {
     "duration": 0.023126,
     "end_time": "2022-08-25T15:34:31.452080",
     "exception": false,
     "start_time": "2022-08-25T15:34:31.428954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_dice_score(probability, mask, smooth = 1):\n",
    "    N = len(probability)\n",
    "    p = probability.reshape(N,-1)\n",
    "    t = mask.reshape(N,-1)\n",
    "\n",
    "    p = p>0.5\n",
    "    t = t>0.5\n",
    "    uion = p.sum(-1) + t.sum(-1)\n",
    "    overlap = (p*t).sum(-1)\n",
    "    dice = 2*overlap/(uion+0.0001)\n",
    "    return dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17aed414",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T15:34:31.476843Z",
     "iopub.status.busy": "2022-08-25T15:34:31.476538Z",
     "iopub.status.idle": "2022-08-25T15:34:31.480624Z",
     "shell.execute_reply": "2022-08-25T15:34:31.479843Z"
    },
    "papermill": {
     "duration": 0.020437,
     "end_time": "2022-08-25T15:34:31.484424",
     "exception": false,
     "start_time": "2022-08-25T15:34:31.463987",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_learning_rate(optimizer):\n",
    "    return optimizer.param_groups[0]['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "010826e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T15:34:31.509573Z",
     "iopub.status.busy": "2022-08-25T15:34:31.509253Z",
     "iopub.status.idle": "2022-08-25T15:34:31.514690Z",
     "shell.execute_reply": "2022-08-25T15:34:31.513907Z"
    },
    "papermill": {
     "duration": 0.022473,
     "end_time": "2022-08-25T15:34:31.518687",
     "exception": false,
     "start_time": "2022-08-25T15:34:31.496214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# organ_threshold = {\n",
    "#     'kidney': 0.50,\n",
    "#     'prostate': 0.50,\n",
    "#     'largeintestine': 0.50,\n",
    "#     'spleen': 0.50,\n",
    "#     'lung': 0.20,\n",
    "# }   \n",
    "\n",
    "organ_threshold = {\n",
    "    'Hubmap': {\n",
    "        'kidney'        : 0.40,\n",
    "        'prostate'      : 0.40,\n",
    "        'largeintestine': 0.40,\n",
    "        'spleen'        : 0.40,\n",
    "        'lung'          : 0.10,\n",
    "    },\n",
    "    'HPA': {\n",
    "        'kidney'        : 0.50,\n",
    "        'prostate'      : 0.50,\n",
    "        'largeintestine': 0.50,\n",
    "        'spleen'        : 0.50,\n",
    "        'lung'          : 0.10,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05848475",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T15:34:31.543471Z",
     "iopub.status.busy": "2022-08-25T15:34:31.543179Z",
     "iopub.status.idle": "2022-08-25T15:34:31.552351Z",
     "shell.execute_reply": "2022-08-25T15:34:31.551562Z"
    },
    "papermill": {
     "duration": 0.024427,
     "end_time": "2022-08-25T15:34:31.554814",
     "exception": false,
     "start_time": "2022-08-25T15:34:31.530387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stochastic Weight Averaging\n",
    "# https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/\n",
    "def do_swa(checkpoint):\n",
    "    skip = ['relative_position_index', 'num_batches_tracked']\n",
    "    \n",
    "    K = len(checkpoint)\n",
    "    swa = None\n",
    "    \n",
    "    for k in range(K):\n",
    "        state_dict = torch.load(checkpoint[k], map_location=lambda storage, loc: storage)['state_dict']\n",
    "        if swa is None:\n",
    "            swa = state_dict\n",
    "        else:\n",
    "            for k, v in state_dict.items():\n",
    "                #print(k)\n",
    "                if any(s in k for s in skip): continue\n",
    "                swa[k] += v\n",
    "    \n",
    "    for k, v in swa.items():\n",
    "        if any(s in k for s in skip): continue\n",
    "        swa[k] /= K\n",
    "    \n",
    "    return swa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0c469a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T15:34:31.579531Z",
     "iopub.status.busy": "2022-08-25T15:34:31.579238Z",
     "iopub.status.idle": "2022-08-25T15:34:31.587549Z",
     "shell.execute_reply": "2022-08-25T15:34:31.586781Z"
    },
    "papermill": {
     "duration": 0.023446,
     "end_time": "2022-08-25T15:34:31.590003",
     "exception": false,
     "start_time": "2022-08-25T15:34:31.566557",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rle_decode(mask_rle, shape, color=1):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.float32)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo : hi] = color\n",
    "    return img.reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "249854f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T15:34:31.614606Z",
     "iopub.status.busy": "2022-08-25T15:34:31.614313Z",
     "iopub.status.idle": "2022-08-25T15:34:31.618206Z",
     "shell.execute_reply": "2022-08-25T15:34:31.617386Z"
    },
    "papermill": {
     "duration": 0.020181,
     "end_time": "2022-08-25T15:34:31.622036",
     "exception": false,
     "start_time": "2022-08-25T15:34:31.601855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#root_dir = '../input/swin-10k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29bd9bf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T15:34:31.649265Z",
     "iopub.status.busy": "2022-08-25T15:34:31.648932Z",
     "iopub.status.idle": "2022-08-25T15:34:31.653248Z",
     "shell.execute_reply": "2022-08-25T15:34:31.652264Z"
    },
    "papermill": {
     "duration": 0.021267,
     "end_time": "2022-08-25T15:34:31.655473",
     "exception": false,
     "start_time": "2022-08-25T15:34:31.634206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4928d44c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T15:34:31.672447Z",
     "iopub.status.busy": "2022-08-25T15:34:31.672122Z",
     "iopub.status.idle": "2022-08-25T15:34:31.678505Z",
     "shell.execute_reply": "2022-08-25T15:34:31.677635Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.01689,
     "end_time": "2022-08-25T15:34:31.680513",
     "exception": false,
     "start_time": "2022-08-25T15:34:31.663623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #out_dir = root_dir + '/result/upernet-swin-v1-tiny-aux5-768'\n",
    "# all_df = []\n",
    "# valid = {\n",
    "#     3: [\n",
    "#         '../input/kaggle-hubmap-model-weights/00008514.model.pth',\n",
    "#         '../input/kaggle-hubmap-model-weights/00008712.model.pth',\n",
    "#         '../input/kaggle-hubmap-model-weights/00008910.model.pth',\n",
    "#         ],\n",
    "# }\n",
    "\n",
    "# all_score_df = []\n",
    "# for f, checkpoint in valid.items():\n",
    "#     if len(checkpoint)==0: \n",
    "#         continue\n",
    "\n",
    "#     #project_name = out_dir.split('/')[-1]\n",
    "#     #fold_dir = out_dir + '/fold-%d' % f\n",
    "\n",
    "#     #checkpoint = [fold_dir + '/checkpoint' + c for c in checkpoint]\n",
    "#     checkpoint = [c for c in checkpoint]\n",
    "#     swa = do_swa(checkpoint)\n",
    "#     torch.save({\n",
    "#         'state_dict': swa,\n",
    "#         'swa': [c.split('/')[-1] for c in checkpoint],\n",
    "#     }, \n",
    "#         f'./fold-{f}-swa.pth')\n",
    "#     net = EffUnet(config.model).cuda()\n",
    "# #     net = Net().cuda()\n",
    "#     state_dict = swa\n",
    "#     net.load_state_dict(state_dict, strict=False)  # True\n",
    "\n",
    "#     net = net.eval()\n",
    "#     net.output_type = ['inference']\n",
    "\n",
    "#     result = {\n",
    "#         'id': [],\n",
    "#         'probability': [],\n",
    "#         'rle': [],\n",
    "#     }\n",
    "    \n",
    "# #     df = pd.read_csv(\"../input/hubmap-organ-segmentation/train.csv\").iloc[0:10]\n",
    "#     start_timer = time.time()\n",
    "#     for t, d in tqdm(df.iterrows()):\n",
    "        \n",
    "#         fname = d['id']\n",
    "#         image = tifffile.imread(f'../input/hubmap-organ-segmentation/test_images/{fname}.tiff')\n",
    "#         image = image.astype(np.float32)/255\n",
    "        \n",
    "#         H, W, _ = image.shape\n",
    "\n",
    "#         s = d.pixel_size / 0.4 * (image_size / 3000)\n",
    "#         image = cv2.resize(image, dsize=(image_size,image_size), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "#         image = image_to_tensor(image)\n",
    "#         image = image.cuda()\n",
    "#         batch = {\n",
    "#             'image':\n",
    "#                 torch.stack([\n",
    "#                     image,\n",
    "#                     torch.flip(image, [1]),\n",
    "#                     torch.flip(image, [2]),\n",
    "#                 ]),  # simple TTA\n",
    "#         }\n",
    "        \n",
    "#         probability = 0\n",
    "#         with torch.no_grad():\n",
    "#             with amp.autocast(enabled=is_amp):\n",
    "#                 output = net(batch) \n",
    "\n",
    "#                 probability += F.interpolate(\n",
    "#                     output, size=(H, W), mode='bilinear', align_corners=False, antialias=True)\n",
    "\n",
    "#         # undo TTA\n",
    "#         probability[1] = torch.flip(probability[1], [1])\n",
    "#         probability[2] = torch.flip(probability[2], [2])\n",
    "#         probability = probability.float().data.cpu().numpy().mean(0)[0]\n",
    "#         #p = probability > organ_threshold[d.organ] \n",
    "#         p = probability > organ_threshold[d.data_source][d.organ] \n",
    "#         p = p.T\n",
    "#         rle = rle_encode(p)\n",
    "#         #rle = rle\n",
    "\n",
    "#         result['rle'].append(rle)\n",
    "#         result['probability'].append(p)\n",
    "#         result['id'].append(fname)\n",
    "#         print('\\r', t, end='', flush=True)\n",
    "#     print('')\n",
    "#     submit_df = pd.DataFrame({'id': result['id'], 'rle': result['rle']})\n",
    "#     print(submit_df)\n",
    "#     all_df.append(submit_df)\n",
    "#     print('submit_df ok!')\n",
    "#     print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ababb96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T15:34:31.696794Z",
     "iopub.status.busy": "2022-08-25T15:34:31.696546Z",
     "iopub.status.idle": "2022-08-25T15:34:31.700863Z",
     "shell.execute_reply": "2022-08-25T15:34:31.699854Z"
    },
    "papermill": {
     "duration": 0.014773,
     "end_time": "2022-08-25T15:34:31.703071",
     "exception": false,
     "start_time": "2022-08-25T15:34:31.688298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# submit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e60e7702",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T15:34:31.726045Z",
     "iopub.status.busy": "2022-08-25T15:34:31.725692Z",
     "iopub.status.idle": "2022-08-25T15:34:31.730201Z",
     "shell.execute_reply": "2022-08-25T15:34:31.729011Z"
    },
    "papermill": {
     "duration": 0.021821,
     "end_time": "2022-08-25T15:34:31.732548",
     "exception": false,
     "start_time": "2022-08-25T15:34:31.710727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plt.imshow(rle_decode(submit_df[\"rle\"].iloc[0], (2023,2023)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "282c88d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T15:34:31.749718Z",
     "iopub.status.busy": "2022-08-25T15:34:31.749455Z",
     "iopub.status.idle": "2022-08-25T15:34:31.754737Z",
     "shell.execute_reply": "2022-08-25T15:34:31.753671Z"
    },
    "papermill": {
     "duration": 0.016771,
     "end_time": "2022-08-25T15:34:31.757130",
     "exception": false,
     "start_time": "2022-08-25T15:34:31.740359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plt.imshow(result['probability'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d38d04b",
   "metadata": {
    "papermill": {
     "duration": 0.008093,
     "end_time": "2022-08-25T15:34:31.773412",
     "exception": false,
     "start_time": "2022-08-25T15:34:31.765319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e0c9eba3",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-08-25T15:34:31.790350Z",
     "iopub.status.busy": "2022-08-25T15:34:31.790091Z",
     "iopub.status.idle": "2022-08-25T15:34:43.875514Z",
     "shell.execute_reply": "2022-08-25T15:34:43.874337Z"
    },
    "papermill": {
     "duration": 12.09755,
     "end_time": "2022-08-25T15:34:43.878699",
     "exception": false,
     "start_time": "2022-08-25T15:34:31.781149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0\n",
      "      id                                                rle\n",
      "0  10044  4588770 4 4591766 11 4594765 14 4597763 19 459...\n",
      "submit_df ok!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_df = []\n",
    "# valid = {\n",
    "# #     0: [\n",
    "# #         '../input/fold-0/result/upernet-swin-v1-tiny-aux5-768/fold-0/checkpoint/00004992.model.pth',\n",
    "# #         '../input/fold-0/result/upernet-swin-v1-tiny-aux5-768/fold-0/checkpoint/00007776.model.pth',\n",
    "# #         '../input/fold-0/result/upernet-swin-v1-tiny-aux5-768/fold-0/checkpoint/00009216.model.pth',\n",
    "# #         ],\n",
    "# #     1: [\n",
    "# #         '../input/fold-1/result/upernet-swin-v1-tiny-aux5-768/fold-1/checkpoint/00007296.model.pth',\n",
    "# #         '../input/fold-1/result/upernet-swin-v1-tiny-aux5-768/fold-1/checkpoint/00008736.model.pth',\n",
    "# #         '../input/fold-1/result/upernet-swin-v1-tiny-aux5-768/fold-1/checkpoint/00009120.model.pth',\n",
    "# #         ],\n",
    "# #     2: [\n",
    "# #         '../input/fold-2/result/upernet-swin-v1-tiny-aux5-768/fold-2/checkpoint/00005376.model.pth',\n",
    "# #         '../input/fold-2/result/upernet-swin-v1-tiny-aux5-768/fold-2/checkpoint/00008832.model.pth',\n",
    "# #         '../input/fold-2/result/upernet-swin-v1-tiny-aux5-768/fold-2/checkpoint/00009888.model.pth',\n",
    "# #         ],\n",
    "#     3: [\n",
    "#         '../input/fold-3/result/upernet-swin-v1-tiny-aux5-768/fold-3/checkpoint/00008019.model.pth',\n",
    "#         '../input/fold-3/result/upernet-swin-v1-tiny-aux5-768/fold-3/checkpoint/00008910.model.pth',\n",
    "#         '../input/fold-3/result/upernet-swin-v1-tiny-aux5-768/fold-3/checkpoint/00009900.model.pth',\n",
    "#         ],\n",
    "# }\n",
    "\n",
    "\n",
    "result = {\n",
    "        'id': [],\n",
    "        'probability': [],\n",
    "        'rle': [],\n",
    "    }\n",
    "df = pd.read_csv(\"/home/lakshita/somusan/hubmap_kaggle/hubmap_data/hubmap-organ-segmentation/train.csv\")\n",
    "start_timer = time.time()\n",
    "for t, d in df.iterrows():\n",
    "\n",
    "    fname = d['id']\n",
    "    # image = tifffile.imread(f'../input/hubmap-organ-segmentation/test_images/{fname}.tiff')\n",
    "    image = cv2.imread(\"/home/lakshita/somusan/hubmap_kaggle/hubmap_data/hubmap-22-aug-pixel-size/127.png\")\n",
    "\n",
    "    image = image.astype(np.float32)/255\n",
    "\n",
    "    H, W, _ = image.shape\n",
    "\n",
    "    s = d.pixel_size / 0.4 * (image_size / 3000)\n",
    "    image = cv2.resize(image, dsize=(image_size,image_size), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    image = image_to_tensor(image)\n",
    "    image = image.cuda()\n",
    "    batch = {\n",
    "        'image':\n",
    "            torch.stack([\n",
    "                image,\n",
    "                torch.flip(image, [1]),\n",
    "                torch.flip(image, [2]),\n",
    "            ]),  # simple TTA\n",
    "    }\n",
    "    \n",
    "    use = 0\n",
    "    probability = 0\n",
    "    with torch.no_grad():\n",
    "        with amp.autocast(enabled = True):\n",
    "            \n",
    "            net = Net().cuda()\n",
    "            state_dict = torch.load('/home/lakshita/somusan/hubmap_kaggle/nbs/result/upernet-swin-v1-tiny-aux5-768/fold-3/checkpoint_cutout/00005445.model.pth',\n",
    "                        map_location=torch.device('cpu'))\n",
    "            net.load_state_dict(state_dict['state_dict'])\n",
    "#             state_dict = swa\n",
    "#             net.load_state_dict(state_dict, strict=False)  # True\n",
    "\n",
    "            net = net.eval()\n",
    "#             net.output_type = ['inference']\n",
    "            use += 1\n",
    "\n",
    "            output = net(batch)\n",
    "            #print(output['probability'].shape)\n",
    "            probability += F.interpolate(\n",
    "                output['probability'], size=(H, W), mode='bilinear', align_corners=False, antialias=True)\n",
    "\n",
    "        #probability = probability / 4\n",
    "        probability[0] = probability[0]   \n",
    "        probability[1] = torch.flip(probability[1], [1])\n",
    "        probability[2] = torch.flip(probability[2], [2])\n",
    "        probability = probability.mean(0, keepdims=True)\n",
    "        probability = probability[0,0].float()\n",
    "        #probability = probability.float().data.cpu().numpy().mean(0)[0]\n",
    "    \n",
    "    probability = probability.data.cpu().numpy()\n",
    "    #p = probability > organ_threshold[d.organ]\n",
    "    p = probability > organ_threshold[d.data_source][d.organ]\n",
    "    p = p.T\n",
    "\n",
    "    rle = rle_encode(p)\n",
    "    \n",
    "    result['rle'].append(rle)\n",
    "    result['probability'].append(p)\n",
    "    result['id'].append(fname)\n",
    "    print('\\r', t, end='', flush=True)\n",
    "    print('')\n",
    "    break\n",
    "    \n",
    "\n",
    "submit_df = pd.DataFrame({'id': result['id'], 'rle': result['rle']})\n",
    "print(submit_df)\n",
    "print('submit_df ok!')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "76d365c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T15:34:43.896092Z",
     "iopub.status.busy": "2022-08-25T15:34:43.895769Z",
     "iopub.status.idle": "2022-08-25T15:34:43.903022Z",
     "shell.execute_reply": "2022-08-25T15:34:43.901970Z"
    },
    "papermill": {
     "duration": 0.019627,
     "end_time": "2022-08-25T15:34:43.906493",
     "exception": false,
     "start_time": "2022-08-25T15:34:43.886866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f50513963c8>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(result['probability'][0].reshape(3000,3000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e1562dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T15:34:43.923916Z",
     "iopub.status.busy": "2022-08-25T15:34:43.923637Z",
     "iopub.status.idle": "2022-08-25T15:34:43.927712Z",
     "shell.execute_reply": "2022-08-25T15:34:43.926746Z"
    },
    "papermill": {
     "duration": 0.015072,
     "end_time": "2022-08-25T15:34:43.929637",
     "exception": false,
     "start_time": "2022-08-25T15:34:43.914565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# plt.imshow(rle_decode(submit_df[\"rle\"].iloc[0], (2023,2023)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38c9f5ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T15:34:43.948320Z",
     "iopub.status.busy": "2022-08-25T15:34:43.946705Z",
     "iopub.status.idle": "2022-08-25T15:34:43.951490Z",
     "shell.execute_reply": "2022-08-25T15:34:43.950640Z"
    },
    "papermill": {
     "duration": 0.015839,
     "end_time": "2022-08-25T15:34:43.953510",
     "exception": false,
     "start_time": "2022-08-25T15:34:43.937671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# type(submit_df[\"rle\"].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d87402b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T15:34:43.972140Z",
     "iopub.status.busy": "2022-08-25T15:34:43.970550Z",
     "iopub.status.idle": "2022-08-25T15:34:43.975829Z",
     "shell.execute_reply": "2022-08-25T15:34:43.974999Z"
    },
    "papermill": {
     "duration": 0.016523,
     "end_time": "2022-08-25T15:34:43.977851",
     "exception": false,
     "start_time": "2022-08-25T15:34:43.961328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.imshow(p)\n",
    "# print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8fac9a82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T15:34:43.994415Z",
     "iopub.status.busy": "2022-08-25T15:34:43.994159Z",
     "iopub.status.idle": "2022-08-25T15:34:44.144194Z",
     "shell.execute_reply": "2022-08-25T15:34:44.143205Z"
    },
    "papermill": {
     "duration": 0.160754,
     "end_time": "2022-08-25T15:34:44.146319",
     "exception": false,
     "start_time": "2022-08-25T15:34:43.985565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "508c261e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T15:34:44.164627Z",
     "iopub.status.busy": "2022-08-25T15:34:44.164348Z",
     "iopub.status.idle": "2022-08-25T15:34:44.172010Z",
     "shell.execute_reply": "2022-08-25T15:34:44.171101Z"
    },
    "papermill": {
     "duration": 0.018796,
     "end_time": "2022-08-25T15:34:44.174159",
     "exception": false,
     "start_time": "2022-08-25T15:34:44.155363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submit_df.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be5ca6d",
   "metadata": {
    "papermill": {
     "duration": 0.00912,
     "end_time": "2022-08-25T15:34:44.192310",
     "exception": false,
     "start_time": "2022-08-25T15:34:44.183190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 26.801291,
   "end_time": "2022-08-25T15:34:45.625264",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-25T15:34:18.823973",
   "version": "2.3.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "21a2a557654fc1676068684031cf9bb9dfda94e124d3623f4e9c9ed764d794ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
