{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-09-20T19:13:10.094486Z","iopub.status.busy":"2022-09-20T19:13:10.094026Z","iopub.status.idle":"2022-09-20T19:13:10.118370Z","shell.execute_reply":"2022-09-20T19:13:10.117498Z","shell.execute_reply.started":"2022-09-20T19:13:10.094366Z"},"trusted":true},"outputs":[],"source":["# !pip install /kaggle/input/staintools-offline/spams-2.6.5.4-cp37-cp37m-linux_x86_64.whl\n","# !pip install /kaggle/input/staintools-offline/staintools-2.1.2-py3-none-any.whl"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-09-20T19:13:10.121147Z","iopub.status.busy":"2022-09-20T19:13:10.120502Z","iopub.status.idle":"2022-09-20T19:13:10.125926Z","shell.execute_reply":"2022-09-20T19:13:10.124968Z","shell.execute_reply.started":"2022-09-20T19:13:10.121111Z"},"trusted":true},"outputs":[],"source":["# import staintools\n","\n","# def get_target(organ_type):\n","#     if organ_type == \"kidney\":\n","#         target = staintools.read_image(\"/kaggle/input/hubmap-organ-segmentation/train_images/15005.tiff\")\n","#         return target\n","#     elif organ_type == \"largeintestine\":\n","#         target = staintools.read_image(\"/kaggle/input/hubmap-organ-segmentation/train_images/9777.tiff\")\n","#         return target\n","#     elif organ_type == \"lung\":\n","#         target = staintools.read_image(\"/kaggle/input/hubmap-organ-segmentation/train_images/4301.tiff\")\n","#         return target\n","#     elif organ_type == \"prostate\":\n","#         target = staintools.read_image(\"/kaggle/input/hubmap-organ-segmentation/train_images/12244.tiff\")\n","#         return target\n","#     elif organ_type == \"spleen\":\n","#         target = staintools.read_image(\"/kaggle/input/hubmap-organ-segmentation/train_images/1123.tiff\")\n","#         return target"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-09-20T19:13:10.128113Z","iopub.status.busy":"2022-09-20T19:13:10.127430Z","iopub.status.idle":"2022-09-20T19:13:10.135262Z","shell.execute_reply":"2022-09-20T19:13:10.134376Z","shell.execute_reply.started":"2022-09-20T19:13:10.128079Z"},"trusted":true},"outputs":[],"source":["# import staintools\n","# target_kidney = staintools.read_image(\"/kaggle/input/hubmap-organ-segmentation/train_images/15005.tiff\")\n","# target_largeintestine = staintools.read_image(\"/kaggle/input/hubmap-organ-segmentation/train_images/9777.tiff\")\n","# target_lung = staintools.read_image(\"/kaggle/input/hubmap-organ-segmentation/train_images/4301.tiff\")\n","# target_prostate = staintools.read_image(\"/kaggle/input/hubmap-organ-segmentation/train_images/12244.tiff\")\n","# target_spleen = staintools.read_image(\"/kaggle/input/hubmap-organ-segmentation/train_images/1123.tiff\")\n","# print('速度')"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-09-20T19:13:10.139166Z","iopub.status.busy":"2022-09-20T19:13:10.138918Z","iopub.status.idle":"2022-09-20T19:13:15.155918Z","shell.execute_reply":"2022-09-20T19:13:15.154767Z","shell.execute_reply.started":"2022-09-20T19:13:10.139143Z"},"trusted":true},"outputs":[],"source":["import os\n","import cv2\n","import matplotlib.pyplot as plt\n","import time\n","import random\n","\n","import torch\n","from torch import nn\n","import torch.cuda.amp as amp\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torch.utils.data import RandomSampler \n","from torch.utils.data import SequentialSampler\n","import torch.nn.functional as F\n","from torchmetrics.functional import dice_score\n","from torch.optim.lr_scheduler import StepLR\n","import tifffile\n","from fastai.vision.all import *\n","\n","from collections import defaultdict\n","\n","import torch\n","from torch.optim.optimizer import Optimizer\n","import itertools as it\n","\n","is_amp = True\n","import logging\n","import pandas as pd\n","from sklearn.model_selection import KFold\n","\n","import numpy as np\n","from itertools import repeat\n","import collections.abc\n","import math\n","import torch\n","from torch.optim.optimizer import Optimizer, required\n","from collections import defaultdict\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","import sys\n","sys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\n","import timm\n","\n","sys.path.append('../input/efficientnet-pytorch/EfficientNet-PyTorch/EfficientNet-PyTorch-master')\n","\n","from efficientnet_pytorch import EfficientNet\n","from efficientnet_pytorch.utils import url_map, url_map_advprop, get_model_params"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-09-20T19:13:15.159361Z","iopub.status.busy":"2022-09-20T19:13:15.158352Z","iopub.status.idle":"2022-09-20T19:13:15.201333Z","shell.execute_reply":"2022-09-20T19:13:15.200444Z","shell.execute_reply.started":"2022-09-20T19:13:15.159319Z"},"trusted":true},"outputs":[],"source":["class FPN(nn.Module):\n","    def __init__(self, input_channels:list, output_channels:list):\n","        super().__init__()\n","        self.convs = nn.ModuleList(\n","            [nn.Sequential(nn.Conv2d(in_ch, out_ch*2, kernel_size=3, padding=1),\n","             nn.ReLU(inplace=True), nn.BatchNorm2d(out_ch*2),\n","             nn.Conv2d(out_ch*2, out_ch, kernel_size=3, padding=1))\n","            for in_ch, out_ch in zip(input_channels, output_channels)])\n","        \n","    def forward(self, xs:list, last_layer):\n","        hcs = [F.interpolate(c(x),scale_factor=2**(len(self.convs)-i),mode='bilinear') \n","               for i,(c,x) in enumerate(zip(self.convs, xs))]\n","        hcs.append(last_layer)\n","        return torch.cat(hcs, dim=1)\n","\n","class UnetBlock(nn.Module):\n","    def __init__(self, up_in_c:int, x_in_c:int, nf:int=None, blur:bool=False,\n","                 self_attention:bool=False, **kwargs):\n","        super().__init__()\n","        self.shuf = PixelShuffle_ICNR(up_in_c, up_in_c//2, blur=blur, **kwargs)\n","        self.bn = nn.BatchNorm2d(x_in_c)\n","        ni = up_in_c//2 + x_in_c\n","        nf = nf if nf is not None else max(up_in_c//2,32)\n","        self.conv1 = ConvLayer(ni, nf, norm_type=None, **kwargs)\n","        self.conv2 = ConvLayer(nf, nf, norm_type=None,\n","            xtra=SelfAttention(nf) if self_attention else None, **kwargs)\n","        self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, up_in:Tensor, left_in:Tensor) -> Tensor:\n","        s = left_in\n","        up_out = self.shuf(up_in)\n","        cat_x = self.relu(torch.cat([up_out, self.bn(s)], dim=1))\n","        return self.conv2(self.conv1(cat_x))\n","        \n","class _ASPPModule(nn.Module):\n","    def __init__(self, inplanes, planes, kernel_size, padding, dilation, groups=1):\n","        super().__init__()\n","        self.atrous_conv = nn.Conv2d(inplanes, planes, kernel_size=kernel_size,\n","                stride=1, padding=padding, dilation=dilation, bias=False, groups=groups)\n","        self.bn = nn.BatchNorm2d(planes)\n","        self.relu = nn.ReLU()\n","\n","        self._init_weight()\n","\n","    def forward(self, x):\n","        x = self.atrous_conv(x)\n","        x = self.bn(x)\n","\n","        return self.relu(x)\n","\n","    def _init_weight(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                torch.nn.init.kaiming_normal_(m.weight)\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","class ASPP(nn.Module):\n","    def __init__(self, inplanes=512, mid_c=256, dilations=[6, 12, 18, 24], out_c=None):\n","        super().__init__()\n","        self.aspps = [_ASPPModule(inplanes, mid_c, 1, padding=0, dilation=1)] + \\\n","            [_ASPPModule(inplanes, mid_c, 3, padding=d, dilation=d,groups=4) for d in dilations]\n","        self.aspps = nn.ModuleList(self.aspps)\n","        self.global_pool = nn.Sequential(nn.AdaptiveMaxPool2d((1, 1)),\n","                        nn.Conv2d(inplanes, mid_c, 1, stride=1, bias=False),\n","                        nn.BatchNorm2d(mid_c), nn.ReLU())\n","        out_c = out_c if out_c is not None else mid_c\n","        self.out_conv = nn.Sequential(nn.Conv2d(mid_c*(2+len(dilations)), out_c, 1, bias=False),\n","                                    nn.BatchNorm2d(out_c), nn.ReLU(inplace=True))\n","        self.conv1 = nn.Conv2d(mid_c*(2+len(dilations)), out_c, 1, bias=False)\n","        self._init_weight()\n","\n","    def forward(self, x):\n","        x0 = self.global_pool(x)\n","        xs = [aspp(x) for aspp in self.aspps]\n","        x0 = F.interpolate(x0, size=xs[0].size()[2:], mode='bilinear', align_corners=True)\n","        x = torch.cat([x0] + xs, dim=1)\n","        return self.out_conv(x)\n","    \n","    def _init_weight(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                torch.nn.init.kaiming_normal_(m.weight)\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","\n","class config:\n","    pretrained_root = '../input/efficientnet-pytorch/'\n","    efficient_net_encoders = {\n","        \"efficientnet-b0\": {\n","            \"out_channels\": (3, 32, 24, 40, 112, 320),\n","            \"stage_idxs\": (3, 5, 9, 16),\n","            \"weight_path\": pretrained_root + \"efficientnet-b0-08094119.pth\"\n","        },\n","        \"efficientnet-b1\": {\n","            \"out_channels\": (3, 32, 24, 40, 112, 320),\n","            \"stage_idxs\": (5, 8, 16, 23),\n","            \"weight_path\": pretrained_root + \"efficientnet-b1-dbc7070a.pth\"\n","        },\n","        \"efficientnet-b2\": {\n","            \"out_channels\": (3, 32, 24, 48, 120, 352),\n","            \"stage_idxs\": (5, 8, 16, 23),\n","            \"weight_path\": pretrained_root + \"efficientnet-b2-27687264.pth\"\n","        },\n","        \"efficientnet-b3\": {\n","            \"out_channels\": (3, 40, 32, 48, 136, 384),\n","            \"stage_idxs\": (5, 8, 18, 26),\n","            \"weight_path\": pretrained_root + \"efficientnet-b3-c8376fa2.pth\"\n","        },\n","        \"efficientnet-b4\": {\n","            \"out_channels\": (3, 48, 32, 56, 160, 448),\n","            \"stage_idxs\": (6, 10, 22, 32),\n","            \"weight_path\": pretrained_root + \"efficientnet-b4-e116e8b3.pth\"\n","        },\n","        \"efficientnet-b5\": {\n","            \"out_channels\": (3, 48, 40, 64, 176, 512),\n","            \"stage_idxs\": (8, 13, 27, 39),\n","            \"weight_path\": pretrained_root + \"efficientnet-b5-586e6cc6.pth\"\n","        },\n","        \"efficientnet-b6\": {\n","            \"out_channels\": (3, 56, 40, 72, 200, 576),\n","            \"stage_idxs\": (9, 15, 31, 45),\n","            \"weight_path\": pretrained_root + \"efficientnet-b6-c76e70fd.pth\"\n","        },\n","        \"efficientnet-b7\": {\n","            \"out_channels\": (3, 64, 48, 80, 224, 640),\n","            \"stage_idxs\": (11, 18, 38, 55),\n","            \"weight_path\": pretrained_root + \"efficientnet-b7-dcc49843.pth\"\n","        }\n","    }\n","    model = 'efficientnet-b7'\n","    \n","class EfficientNetEncoder(EfficientNet):\n","    def __init__(self, stage_idxs, out_channels, model_name, depth=5):\n","\n","        blocks_args, global_params = get_model_params(model_name, override_params=None)\n","        super().__init__(blocks_args, global_params)\n","        \n","        cfg = config.efficient_net_encoders[model_name]\n","\n","        self._stage_idxs = stage_idxs\n","        self._out_channels = out_channels\n","        self._depth = depth\n","        self._in_channels = 3\n","\n","        del self._fc\n","        self.load_state_dict(torch.load(cfg['weight_path']))\n","\n","    def get_stages(self):\n","        return [\n","            nn.Identity(),\n","            nn.Sequential(self._conv_stem, self._bn0, self._swish),\n","            self._blocks[:self._stage_idxs[0]],\n","            self._blocks[self._stage_idxs[0]:self._stage_idxs[1]],\n","            self._blocks[self._stage_idxs[1]:self._stage_idxs[2]],\n","            self._blocks[self._stage_idxs[2]:],\n","        ]\n","\n","    def forward(self, x):\n","        stages = self.get_stages()\n","\n","        block_number = 0.\n","        drop_connect_rate = self._global_params.drop_connect_rate\n","\n","        features = []\n","        for i in range(self._depth + 1):\n","\n","            # Identity and Sequential stages\n","            if i < 2:\n","                x = stages[i](x)\n","\n","            # Block stages need drop_connect rate\n","            else:\n","                for module in stages[i]:\n","                    drop_connect = drop_connect_rate * block_number / len(self._blocks)\n","                    block_number += 1.\n","                    x = module(x, drop_connect)\n","\n","            features.append(x)\n","\n","        return features\n","\n","    def load_state_dict(self, state_dict, **kwargs):\n","        state_dict.pop(\"_fc.bias\")\n","        state_dict.pop(\"_fc.weight\")\n","        super().load_state_dict(state_dict, **kwargs)  \n","        \n","\n","class EffUnet(nn.Module):\n","    def __init__(self, model_name, stride=1):\n","        super().__init__()\n","        \n","        cfg = config.efficient_net_encoders[model_name]\n","        stage_idxs = cfg['stage_idxs']\n","        out_channels = cfg['out_channels']\n","        \n","        self.encoder = EfficientNetEncoder(stage_idxs, out_channels, model_name)\n","\n","        #aspp with customized dilatations\n","        self.aspp = ASPP(out_channels[-1], 256, out_c=384, \n","                         dilations=[stride*1, stride*2, stride*3, stride*4])\n","        self.drop_aspp = nn.Dropout2d(0.5)\n","        #decoder\n","        self.dec4 = UnetBlock(384, out_channels[-2], 256)\n","        self.dec3 = UnetBlock(256, out_channels[-3], 128)\n","        self.dec2 = UnetBlock(128, out_channels[-4], 64)\n","        self.dec1 = UnetBlock(64, out_channels[-5], 32)\n","        self.fpn = FPN([384, 256, 128, 64], [16]*4)\n","        self.drop = nn.Dropout2d(0.1)\n","        self.final_conv = ConvLayer(32+16*4, 1, ks=1, norm_type=None, act_cls=None)\n","        \n","        self.rgb = RGB()\n","        \n","    def forward(self, batch):\n","        x = batch['image']\n","        B, C, H, W = x.shape\n","        x = self.rgb(x)\n","        enc0, enc1, enc2, enc3, enc4 = self.encoder(x)[-5:]\n","        enc5 = self.aspp(enc4)\n","        dec3 = self.dec4(self.drop_aspp(enc5), enc3)\n","        dec2 = self.dec3(dec3,enc2)\n","        dec1 = self.dec2(dec2,enc1)\n","        dec0 = self.dec1(dec1,enc0)\n","        x = self.fpn([enc5, dec3, dec2, dec1], dec0)\n","        x = self.final_conv(self.drop(x))\n","        x = F.interpolate(x, size = 512, mode = 'bilinear')\n","        return x"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-09-20T19:13:15.205446Z","iopub.status.busy":"2022-09-20T19:13:15.204723Z","iopub.status.idle":"2022-09-20T19:13:24.086728Z","shell.execute_reply":"2022-09-20T19:13:24.085282Z","shell.execute_reply.started":"2022-09-20T19:13:15.205404Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/1 [00:07<?, ?it/s]\n"]},{"ename":"AttributeError","evalue":"'NoneType' object has no attribute 'eval'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_18/725828557.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0mPatch_model_Coat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoat_Net_0920\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mPatch_model_Coat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mListModel_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPatch_model_Coat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict_Coat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m             \u001b[0mPatch_List_Image_Coat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImage_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPatch_valid_ImageShows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mData_Pd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMy_Submitfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPatch_model_Coat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimagesizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2022\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'width'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mPatch_List_Image_Coat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m                 Patch_List_Image_Coat = cv2.resize(Patch_List_Image_Coat, dsize=(data.meta['width'], data.meta['height']),\n","\u001b[0;32m/tmp/ipykernel_18/725828557.py\u001b[0m in \u001b[0;36mPatch_valid_ImageShows\u001b[0;34m(Data_Pd, model, imagesizes)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnull_collate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     )\n\u001b[0;32m--> 287\u001b[0;31m     \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mImage_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPatch_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_18/725828557.py\u001b[0m in \u001b[0;36mPatch_validate\u001b[0;34m(net, valid_loader)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mPatch_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m     \u001b[0mimage_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img_height'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'eval'"]}],"source":["from tqdm.notebook import tqdm\n","import gc\n","import tifffile as tiff\n","import tifffile\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from rasterio.windows import Window\n","import pandas as pd\n","import rasterio\n","\n","is_amp = True\n","import warnings\n","\n","warnings.filterwarnings('ignore')\n","import sys\n","\n","sys.path.append(\"../input/deeplearnings\")\n","from dataset import *\n","sys.path.append(\"../input/deeplearnings\")\n","from imagecut import *\n","from imageplt import *\n","from model import *\n","from model_coat_daformer import *\n","from model_pvt_v2_daformer import *\n","from model_coat_daformer_2 import *\n","from My_model_daformer_coat import *\n","def rle2mask(mask_rle, shape=(1600, 256)):\n","    '''\n","    mask_rle: run-length as string formated (start length)\n","    shape: (width,height) of array to return\n","    Returns numpy array, 1 - mask, 0 - background\n","\n","    '''\n","    s = mask_rle.split()\n","    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n","    starts -= 1\n","    ends = starts + lengths\n","    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n","    for lo, hi in zip(starts, ends):\n","        img[lo:hi] = 1\n","    return img.reshape(shape).T\n","\n","\n","tensor_list = [\n","    'mask', 'image', 'organ',\n","]\n","\n","\n","def null_collate(batch):\n","    d = {}\n","    key = batch[0].keys()\n","    for k in key:\n","        v = [b[k] for b in batch]\n","        if k in tensor_list:\n","            v = torch.stack(v)\n","        d[k] = v\n","    # d['mask'] = d['mask'].unsqueeze(1)\n","    # d['organ'] = d['organ'].reshape(-1)\n","    return d\n","\n","\n","def image_to_tensor(image, mode='bgr'):  # image mode\n","    if mode == 'bgr':\n","        image = image[:, :, ::-1]\n","    x = image\n","    x = x.transpose(2, 0, 1)\n","    x = np.ascontiguousarray(x)\n","    x = torch.tensor(x, dtype=torch.float)\n","    return x\n","\n","\n","def mask_to_tensor(mask):\n","    x = mask\n","    x = torch.tensor(x, dtype=torch.float)\n","    return x\n","\n","\n","def tensor_to_mask(x):\n","    mask = x.data.cpu().numpy()\n","    mask = mask.astype(np.float32)\n","    return mask\n","imaging_measurements = {\n","    'hpa': {\n","        'pixel_size': {\n","            'kidney': 0.4,\n","            'prostate': 0.4,\n","            'largeintestine': 0.4,\n","            'spleen': 0.4,\n","            'lung': 0.4\n","        },\n","        'tissue_thickness': {\n","            'kidney': 4,\n","            'prostate': 4,\n","            'largeintestine': 4,\n","            'spleen': 4,\n","            'lung': 4\n","        }\n","    },\n","    'hubmap': {\n","        'pixel_size': {\n","            'kidney': 0.5,\n","            'prostate': 6.263,\n","            'largeintestine': 0.229,\n","            'spleen': 0.4945,\n","            'lung': 0.7562\n","        },\n","        'tissue_thickness': {\n","        'kidney': 10,\n","            'prostate': 5,\n","            'largeintestine': 8,\n","            'spleen': 4,\n","            'lung': 5\n","        }\n","    }\n","}\n","\n","\n","def pixel_size(image, domain_pixel_size, target_pixel_size):\n","    pixel_size_scale_factor = domain_pixel_size / target_pixel_size\n","\n","    image_resized = cv2.resize(\n","        image,\n","        dsize=None,\n","        fx=pixel_size_scale_factor,\n","        fy=pixel_size_scale_factor,\n","        interpolation=cv2.INTER_CUBIC\n","    )\n","\n","    image_resizedd = cv2.resize(\n","        image_resized,\n","        dsize=(\n","            image.shape[1],\n","            image.shape[0]\n","        ),\n","        interpolation=cv2.INTER_CUBIC\n","    )\n","    return image_resizedd\n","\n","\n","##数据处理\n","class HubmapDataset(Dataset):\n","    def __init__(self, df, augment=None,image_size=1024):\n","        data_path = \"../input/hubmap-organ-segmentation\"\n","        df['image_path'] = df['id'].apply(lambda x: os.path.join(data_path, 'test_images', str(x) + '.tiff'))\n","        self.df = df\n","        self.augment = augment\n","        self.length = len(self.df)\n","        self.image_size=image_size\n","\n","    def __str__(self):\n","        string = ''\n","        string += '\\tlen = %d\\n' % len(self)\n","\n","        d = self.df.organ.value_counts().to_dict()\n","        for k in ['kidney', 'prostate', 'largeintestine', 'spleen', 'lung']:\n","            string += '%24s %3d (%0.3f) \\n' % (k, d.get(k, 0), d.get(k, 0) / len(self.df))\n","        return string\n","\n","    def __len__(self):\n","        return self.length\n","\n","    def __getitem__(self, index):\n","        img_path = self.df.loc[index, 'image_path']\n","        image = tifffile.imread(img_path) \n","        \n","#         domain_pixel_size = imaging_measurements['hpa']['pixel_size'][row['organ']]\n","#         target_pixel_size = imaging_measurements['hubmap']['pixel_size'][row['organ']]\n","#         image = pixel_size(image, domain_pixel_size, target_pixel_size)\n","        \n","\n","#         target = get_target(self.df.organ.values[0])##添加-jion\n","#         image = staintools.LuminosityStandardizer.standardize(image)\n","        \n","#         # Stain normalize\n","#         normalizer = staintools.StainNormalizer(method='vahadane')\n","#         normalizer.fit(target)\n","#         transformed3 = normalizer.transform(image)\n","#         image = transformed3\n","        \n","        \n","\n","        image = image.astype(np.float32) / 255\n","\n","        #image_size = 1024\n","        if self.image_size>1024:\n","            image = cv2.resize(image, dsize=(1024, 1024), interpolation=cv2.INTER_LINEAR)\n","        else:\n","            image = cv2.resize(image, dsize=(self.image_size, self.image_size), interpolation=cv2.INTER_LINEAR)\n","        \n","        r = {}\n","        r['index'] = index\n","        r['id'] = self.df['id'][0]\n","        r['image'] = image_to_tensor(image)\n","        return r\n","\n","\n","############################################################\n","####### Validation\n","############################################################\n","def validate(net, valid_loader):\n","    net = net.eval()\n","    image_size = int(valid_loader.dataset.df['img_height'][0])\n","    for t, batch in enumerate(valid_loader):\n","        net.output_type = ['inference']\n","        with torch.no_grad():\n","            with amp.autocast(enabled=is_amp):\n","                batch['image'] = batch['image'].to(device)\n","                output = net(batch)\n","        probability = output['probability']\n","        p = tensor_to_mask(probability[0, 0])\n","        mask_Predict = cv2.resize(p, dsize=(image_size, image_size), interpolation=cv2.INTER_LINEAR)\n","\n","    return mask_Predict\n","\n","\n","def get_learning_rate(optimizer):\n","    return optimizer.param_groups[0]['lr']\n","\n","\n","##数据处理\n","def img2tensor(img, dtype: np.dtype = np.float32):\n","    if img.ndim == 2: img = np.expand_dims(img, 2)\n","    img = np.transpose(img, (2, 0, 1))\n","    return torch.from_numpy(img.astype(dtype, copy=False))\n","\n","\n","def rle_encode_less_memory(img):\n","    # the image should be transposed\n","    pixels = img.T.flatten()\n","    # This simplified method requires first and last pixel to be zero\n","    pixels[0] = 0\n","    pixels[-1] = 0\n","    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n","    runs[1::2] -= runs[::2]\n","\n","    return ' '.join(str(x) for x in runs)\n","\n","\n","def valid_augment5(image, mask, organ=None):\n","    return image, mask\n","def Patch_validate(net, valid_loader):\n","    net = net.eval()\n","    image_size = int(valid_loader.dataset.df['img_height'][0])\n","    for t, batch in enumerate(valid_loader):\n","        net.output_type = ['inference']\n","        with torch.no_grad():\n","            with amp.autocast(enabled=is_amp):\n","                # batch['image'] = batch['image'].to(device)\n","                batch_list = batch['image'].detach().numpy()\n","                batch_list = np.squeeze(batch_list)\n","                batch_Image_ = batch_list.transpose(1, 2, 0)\n","\n","                image_plt = batch_Image_\n","                image_plt = image_plt[:, :, ::-1]\n","                batch_list = do_image_cut(batch_Image_, 512)\n","\n","                for index, image in enumerate(batch_list):\n","                    image = image[:, :, ::-1]\n","                    batch_list[index] = image.transpose(2, 0, 1)\n","                    batch_list[index] = np.ascontiguousarray(batch_list[index])\n","                batch['image'] = torch.tensor(batch_list, dtype=torch.float)\n","                batch['image'] = batch['image'].to(device)\n","                output = net(batch)\n","\n","        probability = output['probability']\n","\n","        # p = tensor_to_mask(probability[0, 0])\n","        p = probability.data.cpu().numpy()\n","        p = np.squeeze(p)\n","        mask_Predict = mask_merge(p, (image_plt.shape[0], image_plt.shape[1]), 512)\n","\n","\n","    return mask_Predict,batch_Image_\n","def Patch_valid_ImageShows(Data_Pd, model, imagesizes):\n","    ##读取数据\n","    valid_dataset = HubmapDataset(Data_Pd, valid_augment5, imagesizes)\n","    valid_dataset.fnames = Data_Pd['id'][0]\n","    valid_loader = DataLoader(\n","        valid_dataset,\n","        sampler=SequentialSampler(valid_dataset),\n","        batch_size=1,\n","        drop_last=False,\n","        num_workers=0,\n","        pin_memory=False,\n","        collate_fn=null_collate,\n","    )\n","    preds,Image_ = Patch_validate(model, valid_loader)\n","    del valid_dataset, valid_loader\n","\n","    return preds,Image_\n","\n","# Stochastic Weight Averaging\n","# https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/\n","def do_swa(checkpoint):\n","    skip = ['relative_position_index', 'num_batches_tracked']\n","    \n","    K = len(checkpoint)\n","    swa = None\n","    \n","    for k in range(K):\n","        state_dict = torch.load(checkpoint[k], map_location=lambda storage, loc: storage)['state_dict']\n","        if swa is None:\n","            swa = state_dict\n","        else:\n","            for k, v in state_dict.items():\n","                #print(k)\n","                if any(s in k for s in skip): continue\n","                swa[k] += v\n","    \n","    for k, v in swa.items():\n","        if any(s in k for s in skip): continue\n","        swa[k] /= K\n","    \n","    return swa\n","\n","\n","def valid_ImageShows(Data_Pd,model,imagesizes):\n","\n","    ##读取数据\n","    valid_dataset = HubmapDataset(Data_Pd, valid_augment5,imagesizes)\n","    valid_dataset.fnames = Data_Pd['id'][0]\n","    valid_loader = DataLoader(\n","        valid_dataset,\n","        sampler=SequentialSampler(valid_dataset),\n","        batch_size=1,\n","        drop_last=False,\n","        num_workers=0,\n","        pin_memory=False,\n","        collate_fn=null_collate,\n","    )\n","    preds = validate(model, valid_loader)\n","    del valid_dataset, valid_loader\n","\n","    return preds\n","def ListModel_state(model_LD,state_dict_LD):\n","    model_LD.load_state_dict(state_dict_LD)\n","    model_LD.float()\n","    model_LD.eval()\n","    model_LD.to(device)\n","    \n","    \n","def swa_model_load(checkpoint, model_name=\"effnet\"):\n","    \n","    swa = do_swa(checkpoint)\n","    torch.save({\n","        'state_dict': swa,\n","        'swa': [c.split('/')[-1] for c in checkpoint],\n","    }, \n","        f'./{model_name}-swa.pth')\n","\n","    effnet = EffUnet(config.model)\n","    state_dict = swa\n","    effnet.load_state_dict(state_dict, strict=False)  # True\n","    effnet.float()\n","    effnet.eval()\n","    effnet.to(device)\n","    return effnet\n","\n","\n","\n","if __name__ == '__main__':\n","\n","    #TH = 0.3 # threshold for positive predictions\n","    identity = rasterio.Affine(1, 0, 0, 0, 1, 0)\n","    DATA = '../input/hubmap-organ-segmentation/test_images/'\n","    df_sample = pd.read_csv('../input/hubmap-organ-segmentation/test.csv')\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n"," #Coat2_Fold0_10080  Coat2_Fold1_12390 Coat2_Fold2_19320\n","    Coat_Tensorform = ['../input/deeplearnings/Coat_Fold2_20370.model.pth',\n","                      '../input/deeplearnings/Coat2_Fold2_19320.model.pth']\n","    Frist_Coat_Tensorform=['../input/deeplearnings/Frist_Coat_Fold0_34860.model.pth']\n","    Y_Coat_Tensorform = ['../input/deeplearnings/Y_Coat_00010600_512X512.model.pth']\n","    \n","    #Coat2_Fold2_19320  Fold3_169830  PTV2_Fold0_20790  Coat2_Fold0_10080  Coat2_Fold1_12390 Coat2_Fold2_19320\n","    MY_Swin_Transform_Tensorform=['../input/deeplearnings/Fold3_169830_762and0.114.pth',\n","                                 '../input/deeplearnings/Swin_Fold1_15198.model.pth']\n","    #Coat_Tensorform=[]\n","    #PVT2_Model_Tensorform=['../input/deeplearnings/PVT2_Fold3_20790.model.pth']\n","    Swin_Transform_Tensorform=['../input/deeplearnings/Y_Swin_Transform_Fold_2_6912.model.pth',\n","                              '../input/deeplearnings/Y_Swin_Transform_Fold_3_6237.model.pth',\n","                              '../input/deeplearnings/Y_Swin_Transform_Fold_1_6912.model.pth']\n","    PVT2_Model_Tensorform=['../input/deeplearnings/PVT2/PTV2_Fold0_20790.model.pth',#IB=0.74\n","                           '../input/deeplearnings/PVT2_Fold-1_26880.model.pth',\n","                           '../input/deeplearnings/PVT2_Fold3_20790.model.pth',\n","                          '../input/deeplearnings/PVT2/PTV2_Fold2_18270.model.pth']\n","#Coat2_Fold1_12390 Coat2_Fold2_19320  Fold3_169830 PTV2_Fold0_20790 PVT2_Fold-1_26880  PVT2_Fold2_19110  Coat2_Fold0_10080  Coat2_Fold1_12390 Coat2_Fold2_19320\n","    \n","    Patch_Coat_Tensorform = ['../input/deeplearnings/Caot_Fold4_spleen00011610.model.pth',\n","                             '../input/deeplearnings/Caot_Fold4_largeintestine00009024.model.pth',\n","                             '../input/deeplearnings/Caot_Fold4_prostate00008550.model.pth',\n","                             '../input/deeplearnings/Caot_Fold4_kidney00009600.model.pth',\n","                            '../input/deeplearnings/Coat_Fold2_20370.model.pth']#'Coat_spleen00033024.model.pth\n","    \n","    valid = ['../input/hubmap-lungs-effnet-512-fold3-weights/00010600.model.pth',\n","            '../input/hubmap-lungs-effnet-512-fold3-weights/00010000.model.pth',\n","            '../input/hubmap-lungs-effnet-512-fold3-weights/00010700.model.pth']\n","    \n","    \n","    organ_Dict = {'spleen': 0, 'largeintestine': 1, 'prostate': 2,'kidney': 3,'lung': 4 }\n","    organs_lists = ['kidney', 'largeintestine', 'lung', 'prostate', 'spleen']\n","    names, preds = [], []\n","    for idx, row in tqdm(df_sample.iterrows(), total=len(df_sample)):\n","        Data_Type=str(row['data_source'])\n","        idx = str(row['id'])\n","        organ_=str(row['organ'])\n","        data = rasterio.open(os.path.join(DATA, idx + '.tiff'),\n","                             transform=identity, num_threads='all_cpus')\n","        ##生成需要文件\n","        colname_ = ['id','organ', 'img_height', 'img_width']\n","        values = [idx,organ_,\n","                  data.meta['width'],\n","                  data.meta['height']]\n","        My_Submitfile = pd.DataFrame(np.array(values), index=np.array(colname_)).T\n","        mask = np.zeros((data.meta['width'], data.meta['width']), dtype=np.float32)\n","        \n","        \n","        ###Patch_Coat_Model\n","        #Patch_Coat_mask=np.zeros((data.meta['width'], data.meta['width']), dtype=np.float32)\n","        if organ_Dict[organ_]<=3:\n","            \n","            if Data_Type=='HPA':\n","                TH=0.3\n","            else:\n","                TH=0.3\n","            \n","            \n","            \n","            state_dict_Coat = torch.load(Patch_Coat_Tensorform[organ_Dict[organ_]], map_location=None)['state_dict']\n","            Patch_model_Coat = Coat_Net_0920()\n","            Patch_model_Coat = ListModel_state(Patch_model_Coat, state_dict_Coat)\n","            Patch_List_Image_Coat, Image_ = Patch_valid_ImageShows(Data_Pd=My_Submitfile, model=Patch_model_Coat,imagesizes=2022)\n","            if (data.meta['width'] != Patch_List_Image_Coat.shape[0]):\n","                Patch_List_Image_Coat = cv2.resize(Patch_List_Image_Coat, dsize=(data.meta['width'], data.meta['height']),\n","                                               interpolation=cv2.INTER_LINEAR)\n","            Patch_Coat_mask=Patch_List_Image_Coat\n","            \n","            Frist_Coat_mask = np.zeros((data.meta['width'], data.meta['width']), dtype=np.float32)\n","            for Coat_Transform_model in Frist_Coat_Tensorform:\n","                state_dict_Coat = torch.load(Coat_Transform_model, map_location=None)['state_dict']\n","                model_Coat = Net_Coat()\n","                model_Coat=ListModel_state(model_Coat,state_dict_Coat)\n","                List_Image_Coat=valid_ImageShows(Data_Pd=My_Submitfile, model=model_Coat,imagesizes=1024)\n","                Frist_Coat_mask+=List_Image_Coat\n","            Frist_Coat_mask=Frist_Coat_mask/len(Frist_Coat_Tensorform)\n","            \n","            \n","            \n","            PVT2_mask = np.zeros((data.meta['width'], data.meta['width']), dtype=np.float32)\n","            for pvt_model in PVT2_Model_Tensorform:\n","                state_dict_PVT2 = torch.load(pvt_model, map_location=None)['state_dict']\n","                model_PVT2 = Net_PVT2()\n","                model_PVT2 =ListModel_state(model_PVT2,state_dict_PVT2)\n","                List_Image_PVT2 = valid_ImageShows(Data_Pd=My_Submitfile, model=model_PVT2,imagesizes=768)\n","                PVT2_mask+=List_Image_PVT2\n","            PVT2_mask=PVT2_mask/len(PVT2_Model_Tensorform)\n","            \n","            \n","            My_Swin_mask=np.zeros((data.meta['width'], data.meta['width']), dtype=np.float32)\n","            for Swin_Transform_model in MY_Swin_Transform_Tensorform:\n","                state_dict_Swin_Transform = torch.load(Swin_Transform_model, map_location=None)['state_dict']\n","                model_Swin_Transform = Net_Swin_Transform()\n","                model_Swin_Transform =ListModel_state(model_Swin_Transform,state_dict_Swin_Transform)\n","                List_Image_Swin_Transform = valid_ImageShows(Data_Pd=My_Submitfile, model=model_Swin_Transform,imagesizes=768)\n","                My_Swin_mask+=List_Image_Swin_Transform\n","            My_Swin_mask=My_Swin_mask/len(MY_Swin_Transform_Tensorform)\n","#             PVT2_mask=(PVT2_mask+My_Swin_mask)/2\n","            \n","            \n","            Swin_mask=np.zeros((data.meta['width'], data.meta['width']), dtype=np.float32)\n","            for Swin_Transform_model in Swin_Transform_Tensorform:\n","                state_dict_Swin_Transform = torch.load(Swin_Transform_model, map_location=None)['state_dict']\n","                model_Swin_Transform = Net_Swin_Transform()\n","                model_Swin_Transform =ListModel_state(model_Swin_Transform,state_dict_Swin_Transform)\n","                List_Image_Swin_Transform = valid_ImageShows(Data_Pd=My_Submitfile, model=model_Swin_Transform,imagesizes=768)\n","                Swin_mask+=List_Image_Swin_Transform\n","            Swin_mask=Swin_mask/len(Swin_Transform_Tensorform)\n","            \n","            Coat_mask=np.zeros((data.meta['width'], data.meta['width']), dtype=np.float32)\n","            ###Coat_Model\n","            for Coat_Transform_model in Coat_Tensorform:\n","                state_dict_Coat = torch.load(Coat_Transform_model, map_location=None)['state_dict']\n","                model_Coat = Coat_Net2()\n","                model_Coat=ListModel_state(model_Coat,state_dict_Coat)\n","                List_Image_Coat=valid_ImageShows(Data_Pd=My_Submitfile, model=model_Coat,imagesizes=1024)\n","                Coat_mask+=List_Image_Coat\n","            Coat_mask=Coat_mask/len(Coat_Tensorform)\n","            \n","            Patch_Coat_mask=(PVT2_mask+Patch_Coat_mask+Swin_mask+Coat_mask+My_Swin_mask+Frist_Coat_mask)/6\n","\n","            #TH=0.3\n","            mask=Patch_Coat_mask\n","            mask[mask > TH] = 1\n","            mask[mask < 1] = 0\n","            preds.append(rle_encode_less_memory(mask))\n","            names.append(int(idx))\n","            \n","        else:\n","\n","            \n","            Coat_mask=np.zeros((data.meta['width'], data.meta['width']), dtype=np.float32)\n","            ###Coat_Model\n","            for Coat_Transform_model in Coat_Tensorform:\n","                state_dict_Coat = torch.load(Coat_Transform_model, map_location=None)['state_dict']\n","                model_Coat = Coat_Net2()\n","                model_Coat=ListModel_state(model_Coat,state_dict_Coat)\n","                List_Image_Coat=valid_ImageShows(Data_Pd=My_Submitfile, model=model_Coat,imagesizes=1024)\n","                Coat_mask+=List_Image_Coat\n","            Coat_mask=Coat_mask/len(Coat_Tensorform)\n","            My_Swin_mask=np.zeros((data.meta['width'], data.meta['width']), dtype=np.float32)\n","            for Swin_Transform_model in MY_Swin_Transform_Tensorform:\n","                state_dict_Swin_Transform = torch.load(Swin_Transform_model, map_location=None)['state_dict']\n","                model_Swin_Transform = Net_Swin_Transform()\n","                model_Swin_Transform =ListModel_state(model_Swin_Transform,state_dict_Swin_Transform)\n","                List_Image_Swin_Transform = valid_ImageShows(Data_Pd=My_Submitfile, model=model_Swin_Transform,imagesizes=768)\n","                My_Swin_mask+=List_Image_Swin_Transform\n","            My_Swin_mask=My_Swin_mask/len(MY_Swin_Transform_Tensorform)\n","            Frist_Coat_mask = np.zeros((data.meta['width'], data.meta['width']), dtype=np.float32)\n","            for Coat_Transform_model in Frist_Coat_Tensorform:\n","                state_dict_Coat = torch.load(Coat_Transform_model, map_location=None)['state_dict']\n","                model_Coat = Net_Coat()\n","                model_Coat=ListModel_state(model_Coat,state_dict_Coat)\n","                List_Image_Coat=valid_ImageShows(Data_Pd=My_Submitfile, model=model_Coat,imagesizes=1024)\n","                Frist_Coat_mask+=List_Image_Coat\n","            Frist_Coat_mask=Frist_Coat_mask/len(Frist_Coat_Tensorform)\n","            \n","            PVT2_mask = np.zeros((data.meta['width'], data.meta['width']), dtype=np.float32)\n","            for pvt_model in PVT2_Model_Tensorform:\n","                state_dict_PVT2 = torch.load(pvt_model, map_location=None)['state_dict']\n","                model_PVT2 = Net_PVT2()\n","                model_PVT2 =ListModel_state(model_PVT2,state_dict_PVT2)\n","                List_Image_PVT2 = valid_ImageShows(Data_Pd=My_Submitfile, model=model_PVT2,imagesizes=768)\n","                PVT2_mask+=List_Image_PVT2\n","            PVT2_mask=PVT2_mask/len(PVT2_Model_Tensorform)\n","            \n","            SWA_Model = swa_model_load(valid, model_name=\"effnet\")\n","            swa_lung_model_pred = valid_ImageShows(Data_Pd=My_Submitfile, model=SWA_Model,imagesizes=512)\n","            \n","            \n","            Swin_mask=np.zeros((data.meta['width'], data.meta['width']), dtype=np.float32)\n","            for Swin_Transform_model in Swin_Transform_Tensorform:\n","                state_dict_Swin_Transform = torch.load(Swin_Transform_model, map_location=None)['state_dict']\n","                model_Swin_Transform = Net_Swin_Transform()\n","                model_Swin_Transform =ListModel_state(model_Swin_Transform,state_dict_Swin_Transform)\n","                List_Image_Swin_Transform = valid_ImageShows(Data_Pd=My_Submitfile, model=model_Swin_Transform,imagesizes=768)\n","                Swin_mask+=List_Image_Swin_Transform\n","            Swin_mask=Swin_mask/len(Swin_Transform_Tensorform)\n","            Patch_Coat_mask=(Swin_mask+PVT2_mask+Coat_mask+My_Swin_mask+Frist_Coat_mask+swa_lung_model_pred)/6\n","            TH=0.1\n","            mask=Patch_Coat_mask\n","            mask[mask > TH] = 1\n","            mask[mask < 1] = 0\n","            preds.append(rle_encode_less_memory(mask))\n","            names.append(int(idx))\n","       \n","\n","\n","        del data, My_Submitfile, mask,Patch_Coat_mask#,Coat_mask,Swin_mask,PVT2_mask\n","        gc.collect()\n","    df = pd.DataFrame({'id': names, 'rle': preds})\n","    df.to_csv('submission.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3.7.0 ('.venv': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"vscode":{"interpreter":{"hash":"21a2a557654fc1676068684031cf9bb9dfda94e124d3623f4e9c9ed764d794ac"}}},"nbformat":4,"nbformat_minor":4}
